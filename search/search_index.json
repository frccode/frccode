{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FRCCode","text":"<p># Welcome to FRCCode.org The Best FRC Software Tutorial</p> <p>Note</p> <p>Still tycho's version, needs a lot of work</p>"},{"location":"#about-us","title":"About Us","text":"<p>Currently FRCCode.org is powered by volunteer students who are passionate about programming and hope to share their knowledge with others. The website is constantly in a state of improvement and development, and is a work-in-progress. Please report any mistakes you see to the GitHub.</p> <p>Ready to level up? Join our Discord community for real-time help, code reviews, and collaboration.</p> <p>Support &amp; Help</p>"},{"location":"support_and_help/","title":"Support &amp; Help","text":""},{"location":"Learn/Chapter_0/java_intro/","title":"Introduction to Java in FRC Programming","text":"<p>Note</p> <p>wrote: sean | edited: tycho | still needs proofreading - may be too long</p>"},{"location":"Learn/Chapter_0/java_intro/#choosing-a-programming-language","title":"Choosing a Programming Language","text":"<p>When choosing a programming language for your team, you should choose the language that your team has the most experience and is the most comfortable working in. This experience will make learning programming that much easier\u2014there is much in FRC programming regardless of your language. </p> <p>While possible to use Python and C++ to program competition robots, most teams choose to use Java (89.2% in 2025). Python is more beginner-friendly, and C++ is more powerful, but Java provides a good middle ground between performance, capability, and ease of use. </p> <p>Furthermore, due to its popularity as a first programming language in high-school curricula (in part through AP CSA), and its widespread online support, Java is a strong choice for most teams and the main language for this guide.</p> <p>However, for the above reasons, this guide will focus on Java as the primary programming language. In the future, we may look to expand support to C++ and Python, but the guide offers no support for these languages at this time. </p> <p>Thus, to begin programming, you must first have a solid background in the Java programming language.</p>"},{"location":"Learn/Chapter_0/java_intro/#learning-java","title":"Learning Java","text":"<p>Cultivating a strong command of Java can be accomplished through a range of publicly available, high-quality learning channels. Whether you favor interactive exercises, comprehensive written guides, or instructor-led video lessons, choose resources that align with your personal learning style. Below is a curated selection to help you begin:</p>"},{"location":"Learn/Chapter_0/java_intro/#recommended-resources","title":"Recommended Resources","text":"<ul> <li> <p>Codecademy: Learn Java   An interactive, hands-on course that introduces core language constructs through guided challenges.</p> </li> <li> <p>LearnJavaOnline.org   A browser-based tutorial offering concise lessons accompanied by in-browser coding exercises.</p> </li> <li> <p>Oracle Java Tutorials   The official documentation and tutorials maintained by Oracle, providing authoritative coverage of the language.</p> </li> <li> <p>Bro Code (YouTube Channel)   A YouTube series offering concise Java tutorials and hands-on projects, ideal for visual learners.</p> </li> <li> <p>freeCodeCamp Java Tutorial for Beginners   A comprehensive, no-cost guide (written + video) covering Java fundamentals in depth.</p> </li> <li> <p>JetBrains Academy: Java Developer Track   A project-based learning environment that weaves coding exercises into real-world Java applications. (IntelliJ Only)</p> </li> </ul>"},{"location":"Learn/Chapter_0/java_intro/#core-concepts","title":"Core Concepts","text":"<p>Tip</p> <p>It isn't necessary to learn all of Java, particularly when it comes to Object-Oriented Programming. Because FRC programming uses of a subset of the language to accomplish most tasks, it is possible to start this course with a very surface level understanding of basic syntax. However, for more advanced behavior and programs, a deeper understanding of Java will become more important. </p> <p>To be able to start learning FRC Programming, a strong foundation in programming is necessary. Below is a list of topics that any FRC programmer should know:</p> <ul> <li>Variables and Data Types (int, double, boolean, String)</li> <li>Control Flow (if/else, switch, for loops/while loops)</li> <li>Methods and Functions (declaration, parameters, return values)</li> <li>Object-Oriented Programming (classes, objects, inheritance, polymorphism, encapsulation)</li> <li>Collections and Data Structures (arrays, Lists, Maps)</li> <li>Functional Interfaces (lambda, suppliers)</li> </ul> <p>Links have been embeded into each concept to help make navigation to each topic easier.</p>"},{"location":"Learn/Chapter_0/java_intro/#next-steps","title":"Next Steps","text":"<p>If you've finished learning/or reviewing the Java Concepts listed in  the Core Concepts section above, please proced onto   Chapter 1: Prerequisites Introduction</p>"},{"location":"Learn/Chapter_1/1_prerequisites_intro/","title":"Prerequisites of FRC Programming","text":"<p>FRC programming requires multiple applications to be downloaded onto your computer. In this section, you will learn how to setup all of the required applications.</p> <p>We will also provide a oversimplified usage guide. However it is recommended that users see the original documentation linked below each section. </p>"},{"location":"Learn/Chapter_1/1_prerequisites_intro/#applications","title":"Applications","text":"<ul> <li>WPILib</li> <li>National Instruments Game Tools</li> <li>Github &amp; Github Desktop</li> </ul>"},{"location":"Learn/Chapter_1/2_wpilib/","title":"WPIlib and Associated Tools","text":"<p>This section covers a brief overview of the WPILIB program and tools you will use as part of the WPIlib instalation.</p>"},{"location":"Learn/Chapter_1/2_wpilib/#instalation","title":"Instalation","text":"<ol> <li>Go to the latest releases for WPILIB linked here Download Latest WPILib Release and download the version of WPILIB that is applicable to your system setup: </li> <li>Follow the instructions provided to install the latest version of WPILib here: Official WPILib installation guide </li> </ol>"},{"location":"Learn/Chapter_1/2_wpilib/#tips-for-quality-of-life-during-instalation","title":"Tips for quality of life during Instalation","text":"<ul> <li>You should be aware of the version year you are downloading. WPILIB releases alpha and beta versions for upcoming years that can be confused as the latest version. Download the latest stable release of WPILIB for your year.</li> <li> <p>Because the WPILib installer is a large file, if you need to install it on multiple computers, consider downloading it once and transferring it via a USB drive. This can save time and bandwidth during the installation process.</p> </li> <li> <p>\"WPILib VSCode vs VSCode\": WPILib uses its own version of VSCode that contains plugins specific for FRC, enabling you to build and use WPILib templates. Do not confuse this with a preexisting installation of VSCode.</p> </li> </ul>"},{"location":"Learn/Chapter_1/2_wpilib/#vs-code-exploration","title":"VS Code Exploration","text":"<p> Wpilib vscode is where you will spend the majority of your coding time as an FRC programmer. Hre are some key layout features to take note of</p> <ul> <li>Project Explorer - Blue: Shows how your robot code project is organized. In order for your code to be deployable to the robot.</li> </ul>"},{"location":"Learn/Chapter_1/2_wpilib/#tips-for-programming-in-wpilib-vscode-enviroment","title":"Tips for programming in WPILIB Vscode enviroment","text":""},{"location":"Learn/Chapter_1/2_wpilib/#wpilib-tools","title":"WPILIB tools","text":"<p>WPILib includes many other tools that are helpful in robot programming. The most commonly used tools will be mentioned here:</p>"},{"location":"Learn/Chapter_1/2_wpilib/#generally-used-wpilib-programs","title":"Generally Used WPILIB programs","text":"<ul> <li>AdvantageScope: Visualizes and analyzes robot log data for debugging and performance review.</li> <li>Elastic: A tool for managing and visualizing log data, often used with AdvantageScope.</li> <li>roboRIO Team Number Setter: Sets the team number on the roboRIO controller.</li> <li>SmartDashboard: Another dashboard tool for displaying and controlling robot data.</li> <li>SysId: System identification tool for characterizing robot mechanisms.</li> </ul>"},{"location":"Learn/Chapter_1/2_wpilib/#less-commonly-used-programs","title":"Less Commonly Used Programs","text":"<p>The WPILib suite also includes several programs that are less frequently used in typical FRC workflows. While these tools may be helpful for specific tasks or advanced troubleshooting, most teams will not need them for typical robot programming applicaitons.</p> <ul> <li>Data Log Tool: Records and manages data logs from your robot for later analysis.</li> <li>Glass: A real-time dashboard for monitoring robot variables and network tables.</li> <li>OutlineViewer: Views and edits NetworkTables entries in real time.</li> <li>PathWeaver: Designs and exports autonomous robot paths for use in code.</li> <li>RobotBuilder: Generates robot code structure based on a graphical subsystem and command layout.</li> <li>Shuffleboard: Customizable dashboard for displaying robot data and controls. (Replaced by Elastic)</li> <li>WPIcal: Utility for generating CAN device configuration files.</li> <li>Java Development Kit: Required for developing and running Java-based robot code.</li> </ul>"},{"location":"Learn/Chapter_1/4_Version_Control_intro/","title":"Version Control Guide for FRC Teams","text":""},{"location":"Learn/Chapter_1/4_Version_Control_intro/#preface","title":"Preface","text":"<p>After setting up your FRC programming environment as described earlier in this chapter, the next step is to establish a structured location to store and manage your robot code. Platforms like GitHub and GitLab offer cloud-based solutions that enable multiple users to collaborate and edit code efficiently.</p> <p>This is a simple guide for getting started on Version Control for FRC teams with Git and Github.</p> <p>Note While there are other solutions for </p> <p>If your team already has an Organization setup, please skip steps </p>"},{"location":"Learn/Chapter_1/4_Version_Control_intro/#github-and-github-desktop","title":"Github and Github Desktop","text":""},{"location":"Learn/Chapter_1/4_Version_Control_intro/#github","title":"Github","text":"<p>Sign in here: https://github.com/</p>"},{"location":"Learn/Chapter_1/4_Version_Control_intro/#github-desktop","title":"Github Desktop","text":"<p>Download and Sign in: https://desktop.github.com/download/</p> <p>test</p>"},{"location":"Learn/Chapter_2/simple_robot_programming/","title":"An Intro Through Timed Robot","text":"<p>At this point in the guide, you should have your VSCode and WPILib installed, as well as a fundamental understanding of the Java programming language. </p> <p>Important</p> <p>TS NOT DONE</p> <p>This module will introduce you to many beginner concepts that will be used throughout all of FRC programming using <code>TimedRobot</code> as an example. While other robot architectures are more popular and often more powerful, <code>TimedRobot</code> serves as an excellent stepping stone to introduce the fundamentals. </p> <p>This training will overview/simplify explantations from WPILib's official Zero-to-Robot guide. </p>"},{"location":"Learn/Chapter_2/simple_robot_programming/#introduction-to-the-timedrobot-template","title":"Introduction to the TimedRobot Template","text":"<p>WPILib provides the <code>TimedRobot</code> base class to simplify handling robot periodic actions. A TimedRobot automatically calls key methods at regular intervals (default every 20 milliseconds), allowing you to focus on writing logic rather than managing timing.</p>"},{"location":"Learn/Chapter_2/simple_robot_programming/#the-timedrobot-class","title":"The <code>TimedRobot</code> Class","text":"<p>When you create a new robot project in VSCode with WPILib, the generated <code>Robot.java</code> will extend <code>TimedRobot</code>. This class includes several lifecycle methods that you can implement based on a time loop:</p> <pre><code>\n</code></pre>"},{"location":"Learn/Chapter_2/simple_robot_programming/#basic-project-structure","title":"Basic Project Structure","text":"<p>Here\u2019s an example skeleton of a <code>Robot.java</code> file:</p> <pre><code>package frc.robot;\n\nimport edu.wpi.first.wpilibj.TimedRobot;\n\npublic class Robot extends TimedRobot {\n\n    @Override\n    public void robotInit() {\n        // Initialize hardware and state\n    }\n\n    @Override\n    public void autonomousInit() {\n        // Reset sensors or timers before autonomous\n    }\n\n    @Override\n    public void autonomousPeriodic() {\n        // Autonomous control logic\n    }\n\n    @Override\n    public void teleopInit() {\n        // Any setup before teleop\n    }\n\n    @Override\n    public void teleopPeriodic() {\n        // Driver control code\n    }\n\n    @Override\n    public void disabledInit() {\n        // Actions on disable\n    }\n\n    @Override\n    public void disabledPeriodic() {\n        // Periodic actions when disabled\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_3/1_command_based_programming/","title":"Command-Based Robot Programming Guide","text":"<p>Note: Much of command-based programming has been extensively documented on the WPILib Docs. This guide is meant to simplify explanations and provide a step-by-step approach for users creating their first command-based program. For every topic, we will link the relevant WPILib reference documentation.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#1-understanding-command-based-programming","title":"1. Understanding Command-Based Programming","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#what-is-command-based-programming","title":"What is Command-Based Programming?","text":"<p>Command-based programming is a declarative programming paradigm that helps you organize your WPILib project and execute robot code logic in an organized fashion. </p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#why-use-command-based-programming","title":"Why Use Command-Based Programming?","text":"<ul> <li>Modularity: Breaks down robot actions into small, reusable commands and subsystems, making code easier to maintain and continously develop over the course of a season.</li> <li>Independent Development: Allows each subsystem and command to be developed and tested separately, helping teams manage complexity as the robot grows.</li> <li>Supports Parallel and Sequential Actions: Enables more sophisticated robot behaviors by naturally supporting concurrent and ordered operations.</li> <li>Cleaner Code and Collaboration: Syphoned abstracted logic between hardware, high level multi mechanism logic and lower level, single mechanism logic leads to \"less boilerplate\" code.</li> </ul> <p>The command-based pattern is based around two core concepts: commands and subsystems.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#key-concepts","title":"Key Concepts","text":"<p>Commands: Commands represent actions the robot can take. Commands run when scheduled, until they are interrupted or their end condition is met. Commands are very recursively composable: commands can be formatted together to accomplish more-complicated tasks.</p> <p>Subsystems: Subsystems represent independently-controlled collections of robot hardware (such as motor controllers, sensors, pneumatic actuators, etc.) that operate together. Subsystems back the resource-management system of command-based: only one command can use a given subsystem at the same time.</p> <p>Reference Material:</p> <p>WPILIB Command based explanation</p> <p>Timed vs Command-Based Discussion</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#2-setting-up-your-project-structure","title":"2. Setting Up Your Project Structure","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#creating-a-command-based-project","title":"Creating a Command-Based Project","text":"<ol> <li>In WPILib Visual Studio Code:<ul> <li>Execute the command \"Create a new project\"</li> <li>Select \"Template\" \u2192 \"java\" \u2192 \"Command Robot\"</li> <li>Enter your project information</li> <li>Create the project</li> </ul> </li> </ol> <p>Note: There are two different command-based robot project templates available in WPILib: \"Command Robot\" and \"Command Robot Skeleton\".  </p> <p>It is recommended to use the \"Command Robot Skeleton\" template. The skeleton template does not include example files, which can sometimes confuse new users. Starting with the skeleton template gives you a cleaner foundation for building your own robot code.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#project-structure-overview","title":"Project Structure Overview","text":"<p>A standard template for a command-based robot project is included in the WPILib examples repository. The root package/directory generally will contain four classes:</p> <pre><code>src/main/java/frc/robot/\n\u251c\u2500\u2500 Main.java              // Main robot application (don't modify)\n\u251c\u2500\u2500 Robot.java             // Main control flow\n\u251c\u2500\u2500 RobotContainer.java    // Robot setup and bindings\n\u251c\u2500\u2500 Constants.java         // Global constants\n\u251c\u2500\u2500 subsystems/            // All subsystem classes\n\u2502   \u2514\u2500\u2500 IntakeSubsystem.java\n\u2514\u2500\u2500 Class Commands/              // All command classes\n      \u2514\u2500\u2500 RunIntakeCommand.java\n</code></pre>"},{"location":"Learn/Chapter_3/1_command_based_programming/#key-files-explained","title":"Key Files Explained","text":"<ul> <li>Main.java: The entry point for your robot program (Java only - don't modify)</li> <li>Robot.java: Handles the main control flow and periodic updates</li> <li>RobotContainer.java: Where you define subsystems, commands, and button bindings</li> <li>Constants.java: Store all your robot's constants in one place</li> </ul>"},{"location":"Learn/Chapter_3/1_command_based_programming/#reference-material","title":"Reference material","text":"<p>Structuring a Command-Based Robot Project</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#3-setting-up-a-subsystem","title":"3. Setting up a Subsystem","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#what-are-subsystems","title":"What Are Subsystems?","text":"<p>Subsystems are the basic unit of robot organization in the command-based paradigm. A subsystem is an abstraction for a collection of robot hardware that operates together as a unit.</p> <p>Tip: For the purposes of this tutorial, we will begin with a base Intake Roller Subsystem and build our command-based system from there.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#create-a-subsystem","title":"Create a Subsystem","text":"<p>The recommended method to create a subsystem for most users is to subclass the abstract SubsystemBase class. Simply type \"extend SubsystemBase\" after your class name and add the specified periodic method as shown below.</p> <p>Basic Subsystem Structure:</p> <pre><code>public class IntakeSubsystem extends SubsystemBase {\n      // Hardware components\n      private final PWMSparkMax intakeMotor = new PWMSparkMax(Constants.IntakeConstants.INTAKE_MOTOR_PORT);\n\n      /** Creates a new IntakeSubsystem. */\n      public IntakeSubsystem() {\n            // Initialize hardware here\n      }\n\n      @Override\n      public void periodic() {\n            // This method is called once per scheduler run\n            // Use for continuous monitoring/updates\n      }\n\n      // Subsystem methods that commands will call\n      public void setIntakeSpeed(double speed) {\n            intakeMotor.set(speed);\n      }\n\n      public double getIntakeSpeed() {\n            return intakeMotor.get();\n      }\n\n      public void stopIntake() {\n            intakeMotor.set(0);\n      }\n\n      // Command factory methods\n      public Command runIntakeCommand(double speed) {\n            return runOnce(() -&gt; setIntakeSpeed(speed));\n      }\n\n      public Command stopIntakeCommand() {\n            return runOnce(this::stopIntake);\n      }\n}\n</code></pre>"},{"location":"Learn/Chapter_3/1_command_based_programming/#subsystem-best-practices","title":"Subsystem Best Practices","text":"<ol> <li>Encapsulation: Hide hardware details inside the subsystem</li> <li>Single Responsibility: Each subsystem should control one functional area (e.g., the intake should only be responsible for intake roller movement)</li> <li>Provide Clean Interface: Offer methods that commands can easily use</li> <li>Use periodic(): For continuous monitoring and updates</li> </ol>"},{"location":"Learn/Chapter_3/1_command_based_programming/#reference-material_1","title":"Reference material","text":"<p>Command Based Subsystems</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#4-setting-up-commands","title":"4. Setting up Commands","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#what-are-commands","title":"What Are Commands?","text":"<p>Commands represent actions the robot can take. Commands run when scheduled, until they are interrupted or their end condition is met.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#command-lifecycle","title":"Command Lifecycle","text":"<p>Every command, including Built-In, factory, and class-types, has four key methods:</p> <ol> <li> <p>initialize():    Called once when the command starts. Use this method to set up any initial state, reset sensors, or perform setup actions before the command begins executing.</p> </li> <li> <p>execute():    Called repeatedly while the command runs. This is where you put the main logic of your command. It runs in a loop until <code>isFinished()</code> returns true. If you have a multi subsystem auto aim, state machine, or trajectory that requries constant refreshing of variables, put that logic in execute()</p> </li> <li> <p>isFinished():    Returns true when the command should end. This method checks if the command\u2019s goal has been met (for example, a timer has expired or a sensor has reached a value). When it returns true, the command will stop executing.</p> </li> <li> <p>end():    Called once when the command finishes. Use this to clean up or reset anything after the command ends, such as stopping motors or releasing resources. This runs whether the command ends normally or is interrupted.</p> </li> </ol>"},{"location":"Learn/Chapter_3/1_command_based_programming/#using-built-in-command-types","title":"Using Built-in Command Types","text":"<p>The command-based library includes many pre-written command types. Through the use of lambdas, these commands can cover almost all use cases and teams should rarely need to write custom command classes.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#common-command-types-and-when-to-use-them","title":"Common Command Types and When to Use Them","text":"<p>Below are the most common command types in WPILib, each with a description and guidance on when to use them:</p> <ol> <li> <p>InstantCommand</p> <ul> <li>Use when: You want to perform a quick, one-time action that completes immediately (e.g., stop the intake roller).</li> <li>Example: <pre><code>new InstantCommand(() -&gt; intakeSubsystem.stopIntake(), intakeSubsystem);\n</code></pre></li> </ul> </li> <li> <p>RunCommand</p> <ul> <li>Use when: You want to run code continuously while the command is scheduled, and stop when it ends (e.g., run the intake while a button is held).</li> <li>Example: <pre><code>new RunCommand(() -&gt; intakeSubsystem.setIntakeSpeed(0.75), intakeSubsystem);\n</code></pre></li> </ul> </li> <li> <p>StartEndCommand</p> <ul> <li>Use when: You need to run code while the command is active, and a different action when it ends (e.g., run the intake on press, stop on release).</li> <li>Example: <pre><code>new StartEndCommand(\n          () -&gt; intakeSubsystem.setIntakeSpeed(0.75),\n          () -&gt; intakeSubsystem.stopIntake(),\n          intakeSubsystem\n);\n</code></pre></li> </ul> </li> <li> <p>FunctionalCommand</p> <ul> <li>Use when: You want full control over initialization, execution, ending, and completion conditions, but don\u2019t want to write a full class (e.g., run the intake for a certain time).</li> <li>Example: <pre><code>new FunctionalCommand(\n          () -&gt; {}, // initialize\n          () -&gt; intakeSubsystem.setIntakeSpeed(0.75),  // execute\n          interrupted -&gt; intakeSubsystem.stopIntake(), // end\n          () -&gt; Timer.getFPGATimestamp() &gt; 2.0, // isFinished (example)\n          intakeSubsystem\n);\n</code></pre></li> </ul> </li> <li> <p>Command Factories (Factory Methods)</p> <ul> <li>Use when: You want to quickly create simple commands using static factory methods for common patterns.  </li> <li>Note: These factory methods (<code>runOnce</code>, <code>run</code>, <code>waitSeconds</code>, etc.) are only available within a <code>SubsystemBase</code> subclass (i.e., inside your subsystem class).</li> <li>Examples (inside IntakeSubsystem): <pre><code>// Inside your IntakeSubsystem class\n\n// Run the intake once and finish immediately\npublic Command startIntakeCommand() {\n      return runOnce(() -&gt; setIntakeSpeed(0.75));\n}\n\n// Run the intake repeatedly until interrupted\npublic Command holdIntakeCommand() {\n      return run(() -&gt; setIntakeSpeed(0.75));\n}\n\n// Run until a condition is met (e.g., sensor detects object)\npublic Command runIntakeUntilObjectDetectedCommand(Supplier&lt;Boolean&gt; objectDetected) {\n      return run(() -&gt; setIntakeSpeed(0.75))\n            .until(objectDetected);\n}\n\n// Wait for a specified time\npublic Command waitTwoSecondsCommand() {\n      return waitSeconds(2.0);\n}\n</code></pre></li> </ul> </li> <li> <p>Command Classes (Extending CommandBase)</p> <ul> <li>Use when: The command is complex, reused, or needs its own state/fields (e.g., run the intake for a specific duration).</li> <li>Example: <pre><code>public class RunIntakeForTime extends CommandBase {\n          private final IntakeSubsystem intake;\n          private final double duration;\n          private double startTime;\n\n          public RunIntakeForTime(IntakeSubsystem intake, double duration) {\n                 this.intake = intake;\n                 this.duration = duration;\n                 addRequirements(intake);\n          }\n\n          @Override\n          public void initialize() {\n                 startTime = Timer.getFPGATimestamp();\n          }\n\n          @Override\n          public void execute() {\n                 intake.setIntakeSpeed(0.75);\n          }\n\n          @Override\n          public boolean isFinished() {\n                 return Timer.getFPGATimestamp() - startTime &gt;= duration;\n          }\n\n          @Override\n          public void end(boolean interrupted) {\n                 intake.stopIntake();\n          }\n}\n</code></pre></li> </ul> <p>Note: The <code>wait</code> command pauses the execution of the command sequence for a specified duration before proceeding to the next command. This is useful for introducing delays between actions or synchronizing with asynchronous events. For example, <code>waitSeconds(2.0)</code> will delay the next command by 2 seconds.</p> <p>Example usage:</p> <pre><code>// Wait for 2 seconds before running the next command\nCommand waitThenRunIntake = new SequentialCommandGroup(\n     new WaitCommand(2.0),\n     new InstantCommand(() -&gt; intakeSubsystem.setIntakeSpeed(0.75), intakeSubsystem)\n);\n</code></pre> <p>You can also use the <code>waitSeconds</code> factory method (inside a <code>SubsystemBase</code> subclass or with the <code>Commands</code> utility):</p> <pre><code>// Using the factory method for a 1.5 second wait\nCommand waitCommand = waitSeconds(1.5);\n</code></pre> <p>Note: <code>WaitCommand</code> is especially useful in autonomous routines where you need to pause between actions, such as waiting for a mechanism to finish moving before starting the next step.</p> <p>For more details, see the WPILib WaitCommand documentation.</p> </li> </ol>"},{"location":"Learn/Chapter_3/1_command_based_programming/#summary-table","title":"Summary Table","text":"Command Type Use For InstantCommand One-shot actions RunCommand Continuous actions StartEndCommand Start/stop paired actions FunctionalCommand Custom lifecycle logic Command Class Complex or reusable cmds <p>Tip: Start with the simplest command type that fits your need, such as a subsystem factory command that starts or stops the intake. Then, branch off into command compositions for multi-subsystem logic (Described below). Use full command classes only for complex or reusable behaviors. See WPILib Command Types for more details.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#reference-material_2","title":"Reference Material","text":"<ul> <li>Commands</li> </ul>"},{"location":"Learn/Chapter_3/1_command_based_programming/#5-chaining-commands-in-command-compositions","title":"5. Chaining Commands in Command Compositions","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#building-complex-commands","title":"Building Complex Commands","text":"<p>Individual commands are capable of accomplishing a large variety of robot tasks, but the format can quickly become cumbersome when more advanced functionality requiring extended sequences of robot tasks or coordination of multiple robot subsystems is required. Below are a list of the most useful command chain types that you can implement in your project:</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#sequential-commands","title":"Sequential Commands","text":"<p>Run commands one after another:</p> <pre><code>// Using the sequence factory\nCommand autoSequence = Commands.sequence(\n      runOnce(() -&gt; intakeSubsystem.setIntakeSpeed(0.75)),\n      waitSeconds(0.5),\n      run(() -&gt; intakeSubsystem.setIntakeSpeed(0.3)).withTimeout(2.0),\n      runOnce(() -&gt; intakeSubsystem.stopIntake())\n);\n</code></pre>"},{"location":"Learn/Chapter_3/1_command_based_programming/#parallel-commands","title":"Parallel Commands","text":"<p>There are three types of parallel compositions, differing based on when the composition finishes:</p> <pre><code>// All commands must finish\nCommand parallelAll = Commands.parallel(\n      runOnce(() -&gt; intakeSubsystem.setIntakeSpeed(0.75)),\n      runOnce(() -&gt; driveSubsystem.arcadeDrive(0.5, 0))\n);\n\n// Finish when any command finishes\nCommand raceGroup = Commands.race(\n      run(() -&gt; intakeSubsystem.setIntakeSpeed(0.75)),\n      waitSeconds(3.0)\n);\n\n// Run commands alongside each other\nCommand driveAndIntake = Commands.run(() -&gt; driveSubsystem.arcadeDrive(0.5, 0))\n      .alongWith(run(() -&gt; intakeSubsystem.setIntakeSpeed(0.3)));\n</code></pre> <p>Note: The <code>Commands.sequence</code>, <code>Commands.parallel</code>, and similar factory methods are convenience wrappers. You can also create command groups directly using classes like <code>SequentialCommandGroup</code> or <code>ParallelCommandGroup</code>, passing your commands as parameters.</p> <p>Example using <code>ParallelCommandGroup</code>:</p> <pre><code>Command parallelGroup = new ParallelCommandGroup(\n      new InstantCommand(() -&gt; intakeSubsystem.setIntakeSpeed(0.75), intakeSubsystem),\n      new InstantCommand(() -&gt; driveSubsystem.arcadeDrive(0.5, 0), driveSubsystem)\n);\n</code></pre>"},{"location":"Learn/Chapter_3/1_command_based_programming/#other-command-compositions","title":"Other Command Compositions","text":"<p>In addition to <code>Commands.sequence</code> and <code>Commands.parallel</code>, WPILib provides several other command composition patterns for building complex robot behaviors:</p> <ul> <li> <p>race: Runs multiple commands in parallel, but the group ends as soon as any command finishes. Useful for timeouts or interrupting actions.      <pre><code>Commands.race(\n          run(() -&gt; intakeSubsystem.setIntakeSpeed(0.75)),\n          waitSeconds(3.0)\n);\n</code></pre></p> </li> <li> <p>deadline: Runs multiple commands in parallel, but the group ends when a designated \"deadline\" command finishes. All other commands are interrupted at that point.      <pre><code>Commands.deadline(\n          waitSeconds(2.0), // deadline command\n          run(() -&gt; intakeSubsystem.setIntakeSpeed(0.75))\n);\n</code></pre></p> </li> <li> <p>alongWith: Runs two commands in parallel, and both must finish for the group to end. This is a convenient way to combine two commands without creating a full parallel group.      <pre><code>Command driveAndIntake = run(() -&gt; driveSubsystem.arcadeDrive(0.5, 0))\n          .alongWith(run(() -&gt; intakeSubsystem.setIntakeSpeed(0.3)));\n</code></pre></p> </li> <li> <p>repeatedly: Repeats a command indefinitely until interrupted. Useful for continuous actions that should only stop when explicitly canceled.      <pre><code>Command repeatIntake = run(() -&gt; intakeSubsystem.setIntakeSpeed(0.75)).repeatedly();\n</code></pre></p> </li> <li> <p>perpetually: Similar to <code>repeatedly</code>, but the command never finishes on its own and must be interrupted.      <pre><code>Command perpetualIntake = run(() -&gt; intakeSubsystem.setIntakeSpeed(0.75)).perpetually();\n</code></pre></p> </li> </ul> <p>For more details and advanced usage, see the WPILib Command Compositions documentation.</p> <p>Tip: For most use cases, you should primarily use parallel and sequential command compositions to organize your robot's actions. Branch out into other specialized command compositions (such as <code>race</code>, <code>deadline</code>, <code>alongWith</code>, etc.) only when you encounter scenarios that require their unique behaviors. This approach keeps your code simple and maintainable. For example, for the context of your intake subsystem, you can have a sequential command group that turns</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#6-setting-up-robotcontainer","title":"6. Setting Up RobotContainer","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#the-robotcontainer-class","title":"The RobotContainer Class","text":"<p>RobotContainer is where most of the setup for your command-based robot will take place. In this class, you will define your robot's subsystems and commands, bind those commands to triggering events (such as buttons), and specify which command you will run in your autonomous routine.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#basic-robotcontainer-structure","title":"Basic RobotContainer Structure","text":"<pre><code>public class RobotContainer {\n      // Subsystems\n      private final IntakeSubsystem intakeSubsystem = new IntakeSubsystem();\n      private final DriveSubsystem driveSubsystem = new DriveSubsystem();\n\n      // Controllers\n      private final CommandXboxController driverController = \n            new CommandXboxController(Constants.OIConstants.DRIVER_CONTROLLER_PORT);\n\n      public RobotContainer() {\n            // Configure default commands\n            configureDefaultCommands();\n\n            // Configure button bindings\n            configureBindings();\n      }\n\n      private void configureDefaultCommands() {\n            // Set default command for drivetrain\n            driveSubsystem.setDefaultCommand(\n                    run(() -&gt; driveSubsystem.arcadeDrive(\n                          -driverController.getLeftY(),\n                          -driverController.getRightX()\n                    )).withName(\"Default Drive\")\n            );\n      }\n\n      private void configureBindings() {\n            // Button bindings go here (See button binding section for more info)\n\n          // Bind A button to start intake at 75% speed while held\n          driverController.a().whileTrue(\n               run(() -&gt; intakeSubsystem.setIntakeSpeed(0.75))\n          );\n\n          // Bind B button to reverse intake at 50% speed while held\n          driverController.b().whileTrue(\n               run(() -&gt; intakeSubsystem.setIntakeSpeed(-0.5))\n          );\n\n          // Bind X button to stop intake when pressed\n          driverController.x().onTrue(\n               runOnce(() -&gt; intakeSubsystem.stopIntake())\n          );\n\n      }\n\n      public Command getAutonomousCommand() {\n            // Return autonomous command\n            return null;\n      }\n}\n</code></pre> <p>Tip: When writing your RobotContainer class, declare your subsystems at the top of the file, place your button bindings configuration in the middle, and define your autonomous routines at the bottom. This structure keeps your code organized and easy to maintain.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#reference-material_3","title":"Reference material","text":"<ul> <li>Organizing Robot Code</li> </ul>"},{"location":"Learn/Chapter_3/1_command_based_programming/#7-binding-commands-to-triggers","title":"7. Binding Commands to Triggers","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#understanding-triggers","title":"Understanding Triggers","text":"<p>Apart from autonomous commands, which are scheduled at the start of the autonomous period, and default commands, which are automatically scheduled whenever their subsystem is not currently in-use, the most common way to run a command is by binding it to a triggering event, such as a button being pressed by a human operator.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#button-bindings","title":"Button Bindings","text":"<p>Command binding is done through the Trigger class. The command-based HID classes contain factory methods returning a Trigger for a given button.</p> <pre><code>private void configureBindings() {\n      // Button press bindings\n      driverController.a().onTrue(\n            runOnce(() -&gt; intakeSubsystem.setIntakeSpeed(0.75))\n      );\n\n      // While button is held\n      driverController.b().whileTrue(\n            run(() -&gt; intakeSubsystem.setIntakeSpeed(0.75))\n      );\n\n      // When button is released\n      driverController.x().onFalse(\n            runOnce(() -&gt; intakeSubsystem.stopIntake())\n      );\n\n      // Toggle command\n      driverController.y().toggleOnTrue(\n            run(() -&gt; intakeSubsystem.setIntakeSpeed(-0.75))\n      );\n}\n</code></pre>"},{"location":"Learn/Chapter_3/1_command_based_programming/#trigger-conditions","title":"Trigger Conditions","text":"<p>You can also bind commands to custom conditions:</p> <pre><code>// Bind to sensor readings (e.g., object detected)\nnew Trigger(() -&gt; intakeSubsystem.getIntakeSpeed() &gt; 0.5)\n      .onTrue(runOnce(() -&gt; intakeSubsystem.stopIntake()));\n\n// Bind to subsystem conditions\nnew Trigger(() -&gt; /* some intake condition */ false)\n      .onTrue(runOnce(() -&gt; System.out.println(\"Intake condition met!\")));\n</code></pre>"},{"location":"Learn/Chapter_3/1_command_based_programming/#reference-material_4","title":"Reference material","text":"<ul> <li>Binding Commands to Triggers</li> </ul>"},{"location":"Learn/Chapter_3/1_command_based_programming/#8-the-command-scheduler","title":"8. The Command Scheduler","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#how-the-scheduler-works","title":"How the Scheduler Works","text":"<p>The CommandScheduler is the class responsible for actually running commands. Each iteration (ordinarily once per 20ms), the scheduler polls all registered buttons, schedules commands for execution, and runs all scheduled commands.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#scheduler-process","title":"Scheduler Process","text":"<p>The scheduler follows this process every 20ms:</p> <ol> <li>Poll Triggers: Check all registered triggers for state changes</li> <li>Schedule Commands: Start new commands based on trigger conditions</li> <li>Run Commands: Call execute() on all active commands</li> <li>Check Completion: Call isFinished() and end() as needed</li> <li>Resource Management: Ensure subsystem requirements are respected</li> </ol>"},{"location":"Learn/Chapter_3/1_command_based_programming/#resource-management","title":"Resource Management","text":"<p>Multiple commands can run concurrently, as long as they do not require the same resources on the robot. Resource management is handled on a per-subsystem basis: commands specify which subsystems they interact with, and the scheduler will ensure that no more more than one command requiring a given subsystem is scheduled at a time.</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#reference-material_5","title":"Reference material","text":"<ul> <li>Command Scheduler</li> </ul>"},{"location":"Learn/Chapter_3/1_command_based_programming/#9-constants-and-organization","title":"9. Constants and Organization","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#constants-class","title":"Constants Class","text":"<p>Organize all your robot's constants in one place in a separate class from your subsystem. This declutters your subsystem and command program.</p> <p>Note: Some teams choose to create a separate <code>Constants</code> class for each subsystem (e.g., <code>DriveConstants</code>, <code>IntakeConstants</code>) instead of grouping all constants in a single file. This can help keep constants closely associated with their respective subsystems and improve maintainability in larger projects.</p> <pre><code>public final class Constants {\n      public static final class DriveConstants {\n            public static final int LEFT_MOTOR_PORT = 0;\n            public static final int RIGHT_MOTOR_PORT = 1;\n            public static final double DRIVE_SPEED = 0.8;\n            public static final double TURN_SPEED = 0.6;\n      }\n\n      public static final class IntakeConstants {\n            public static final int INTAKE_MOTOR_PORT = 3;\n            public static final double INTAKE_SPEED = 0.75;\n      }\n\n      public static final class OIConstants {\n            public static final int DRIVER_CONTROLLER_PORT = 0;\n            public static final int OPERATOR_CONTROLLER_PORT = 1;\n      }\n}\n</code></pre>"},{"location":"Learn/Chapter_3/1_command_based_programming/#project-organization-tips","title":"Project Organization Tips","text":"<ol> <li>Group Related Commands: Create subfolders in the commands directory</li> <li>Use Descriptive Names: Make your intent clear in class and method names</li> <li>Keep Methods Small: Break complex operations into smaller, testable pieces</li> <li>Comment Your Code: Explain the \"why\" behind your decisions</li> </ol>"},{"location":"Learn/Chapter_3/1_command_based_programming/#10-testing-and-debugging","title":"10. Testing and Debugging","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#testing-individual-components","title":"Testing Individual Components","text":"<ol> <li>Test Subsystems: Use the subsystem's periodic() method for monitoring with Smartdashboard or Advantagekit Logging.</li> <li>Test Commands: Run commands individually to verify behavior. Use the printline technique shown below for verifying command functionality</li> </ol>"},{"location":"Learn/Chapter_3/1_command_based_programming/#common-debugging-techniques","title":"Common Debugging Techniques","text":"<pre><code>// Add logging to commands\n@Override\npublic void initialize() {\n      System.out.println(\"Command started: \" + getName());\n}\n\n// Monitor subsystem state\n@Override\npublic void periodic() {\n      SmartDashboard.putNumber(\"Intake Speed\", intakeMotor.get());\n}\n</code></pre>"},{"location":"Learn/Chapter_3/1_command_based_programming/#11-best-practices-summary","title":"11. Best Practices Summary","text":""},{"location":"Learn/Chapter_3/1_command_based_programming/#design-principles","title":"Design Principles","text":"<ol> <li>Single Responsibility: Each class should have one clear purpose</li> <li>Encapsulation: Hide implementation details within subsystems</li> <li>Dependency Injection: Pass subsystems to commands through constructors</li> <li>Declarative Programming: Set up behavior once, let the framework handle execution</li> </ol>"},{"location":"Learn/Chapter_3/1_command_based_programming/#code-organization","title":"Code Organization","text":"<ol> <li>Use the Standard Structure: Follow the WPILib template organization</li> <li>Keep Constants Organized: Group related constants together</li> <li>Write Factory Methods: Create reusable command factories in subsystems</li> <li>Use Descriptive Names: Make your code self-documenting</li> </ol>"},{"location":"Learn/Chapter_3/1_command_based_programming/#example-real-world-project-structure-phantomcatz-2025","title":"Example: Real-World Project Structure (PhantomCatz 2025)","text":"<p>Below is an outline of the <code>frc/robot</code> directory from the referenced project:</p> <pre><code>frc/robot/\n\u251c\u2500\u2500 Constants.java\n\u251c\u2500\u2500 Main.java\n\u251c\u2500\u2500 Robot.java\n\u251c\u2500\u2500 RobotContainer.java\n\u251c\u2500\u2500 commands/\n\u2502   \u251c\u2500\u2500 auto/\n\u2502   \u251c\u2500\u2500 drive/\n\u2502   \u251c\u2500\u2500 intake/\n\u2502   \u2514\u2500\u2500 ... (other command groups)\n\u251c\u2500\u2500 subsystems/\n\u2502   \u251c\u2500\u2500 DriveSubsystem.java\n\u2502   \u251c\u2500\u2500 IntakeSubsystem.java\n\u2502   \u2514\u2500\u2500 ... (other subsystems)\n\u2514\u2500\u2500 util/\n      \u2514\u2500\u2500 ... (utility classes)\n</code></pre> <p>Key Points: - Commands are grouped by function (e.g., <code>auto</code>, <code>drive</code>, <code>intake</code>). - Subsystems are in their own folder. - Utility code is separated into <code>lib/util</code> and <code>util</code> folders. - Core files (<code>Robot.java</code>, <code>RobotContainer.java</code>, <code>Constants.java</code>) are at the root.</p> <p>This structure helps keep code organized, scalable, and easy to navigate as the robot project grows.</p> <p>Reference: You can view a real-world example of a command-based robot project on GitHub: PhantomCatz 2025 Robot Code</p>"},{"location":"Learn/Chapter_3/1_command_based_programming/#testing-strategy","title":"Testing Strategy","text":"<ol> <li>Test Incrementally: Build and test one component at a time</li> <li>Use Default Commands: Set up safe default behaviors</li> <li>Handle Edge Cases: Consider what happens when things go wrong</li> <li>Log Important Events: Use console output or SmartDashboard for debugging</li> </ol>"},{"location":"Learn/Chapter_3/1_command_based_programming/#12-next-steps","title":"12. Next Steps","text":"<p>Once you have a basic command-based robot working, consider exploring:</p> <ul> <li>Advanced Command Compositions: More complex parallel and sequential operations</li> <li>PID Control: Closed-loop control for precise movements</li> <li>Path Following: Autonomous navigation using trajectory generation</li> <li>State Machines: Managing complex robot behaviors</li> <li>Custom Triggers: Creating sophisticated activation conditions</li> </ul> <p>The command-based framework is powerful and flexible. Start with simple commands and subsystems, then gradually add complexity as you become more comfortable with the paradigm.</p>"},{"location":"Learn/Chapter_4/PID/","title":"PID Control","text":"<p>Warning</p> <p>tycho - need some short paragraph explaining what \"control theory\" is and some basic terms before jumping straight into PID - this page is about PID and its implementation</p> <p>Control theory is the study of how to make systems behave in a desired way by automatically adjusting their inputs based on feedback. As described in the CERN lecture notes, \"Control theory deals with the problem of manipulating the inputs to a system to obtain the desired effect on the output of the system.\" In robotics, this means using sensors to measure what the robot is doing and then adjusting motors or actuators to reach a goal. Key terms include \"system\" (the thing being controlled), \"input\" (what you can change), \"output\" (what you want to achieve), and \"feedback\" (information about the current state). Control theory provides the mathematical and practical foundation for techniques like PID control, which is widely used in robotics and automation.</p> <p>Reference material: CERN lecture notes on control theory (PDF)</p>"},{"location":"Learn/Chapter_4/PID/#overview","title":"Overview","text":"<p>This guide teaches you the basics to get your position/velocity based elevator/pivot/flywheel/extending tube mechanisms moving accurately and covers why these systems work and are so wildly used.</p> <p>What you'll build: A position-controlled mechanism (arm, elevator, etc.) that smoothly moves to target positions.</p>"},{"location":"Learn/Chapter_4/PID/#1-why-pid","title":"1. Why PID?","text":"<p>Let us consider an example. We have a robot, and we want to drive ten feet. We have encoders that tell us how many feet our robot has driven, and motors that we can send a percentage power to. How do we achieve our goal?</p> <p>Warning</p> <p>tycho -  manual control needs to be differentiated from bangbang- its open loop, and shouldn't look at encoders</p>"},{"location":"Learn/Chapter_4/PID/#manual-control","title":"Manual Control","text":"<p>One way to approach this problem is through a predetermined manual effort. Through trial and error, we may determine that we need to give the motors 50% power for one and a half seconds to reach our goal of 10 feet. </p> <p>Warning</p> <p>tycho - i feel like an interactive demo would do really well here to help build intuition with these different models. it's programming.. we should be able to do better. </p> <p><pre><code>while(time &lt; 1.5) {\n    motor.set(0.5); // Full speed regardless of position\n}\n</code></pre> That's great! However, this method isn't adaptable to any variation. If we put a cone in front of the robot, it will be slowed down and go nine feet. If we remove a mechanism from the robot, it becomes lighter, and thus moves faster, and now goes 11 feet. </p> <p>If we want the robot to go 20 feet, we can't just double the time, because acceleration and velocity vary. It would require a whole new set of tests.</p>"},{"location":"Learn/Chapter_4/PID/#bang-bang-control","title":"Bang-Bang Control","text":"<p>Bang-bang tries to incorporate some sensor feedback by switching our control effort to on or off depending on which side of the target we're on.</p> <p>Imagine you're trying to boil a pot of water. You'd turn the stove on high heat to raise the temperature quickly, and then once it's boiling, you turn it off. In this scenario, Bang-Bang is optimal!</p> <pre><code>if (currentPositionFt &lt; 10) {\n    motor.set(1.0); // full speed\n} else {\n    motor.set(0); // stop\n}\n</code></pre> <p>In our robot scenario, robot will keep pushing even if we try to slow it down until it reaches the target. But there is a fatal flaw with this design. Unlike in our boiling scenario, the robot carries momentum. This means that the robot will continue to move, even after we stop applying control effort, leading to what is known as overshoot.</p> <p>Still, Bang-Bang is applicable in many scenarios where the mechanism variable doesn't have momentum (like a motor powering a flywheel), or if the mechanism has a tendency to return to its original state (like an elevator that has tendency to fall due to gravity). Mostly, Bang-Bang is implemented because its simple. In FRC however, we can do much better.</p>  \u25b6 What else is wrong with Bang-Bang?      While bang-bang control can be good for some mechanisms like flywheels, it leads to jerky, abrupt movements that make it difficult to stop precisely at the target, which can increase wear on mechanical parts over time."},{"location":"Learn/Chapter_4/PID/#pids-smart-solution","title":"PID's Smart Solution","text":"<p>We can be a lot smarter with our control. Let's say for the sake of understanding that you're driving a car, and you want this car to move exactly 100 feet. How would you achieve that?</p>      FRC Distance Control Simulator - Navigate to Target (50m away)    Reset \u25c0 Backward \u23f8 Pause \u25b6 Forward 0m 10m 20m 30m 40m 50m \ud83e\udd16 \ud83c\udfaf          Position: 0.0m         Distance to Target: 50.0m         Speed: 0.0         Status: Stopped                 \ud83c\udf89 TARGET REACHED! \ud83c\udf89         Well done!        <p>Warning</p> <p>it would be much more engaging if we had some kind of javascript demo where users could \"drive\" a car 100 ft and we plot the control effort</p> <p>To reach your target quickly for our Car Senario, you\u2019d start by applying full power, then ease off as you get closer, and finally brake to stop exactly at the target. For a motor, this means starting at max power, gradually reducing it as you approach, and stopping when you reach the goal.</p> <p>So what does PID do? PID control is the foundation of precise robot movement. Instead of manually setting motor speeds, PID automatically adjusts power based on where you are versus where you want to be. This is also known as error.</p> <p>Warning</p> <p>tycho - i think its better to frame this not as \"move fast\" or \"move slow\" but rather a push- since we dont have control over how fast the motor moves directly, particularly in a velocity-control setting. (however, this is nuanced, as we don't really have direct control over how hard it pushes either... ka.. kv.. )</p> <p>edited until here</p> <ul> <li>Far from target: Push hard</li> <li>Close to target: Slow down</li> <li>At target: Don't push</li> <li>Overshoot: Push back</li> </ul> <p>Real Benefits - Consistent, repeatable positioning - Automatically adapts to load changes - Smooth, professional-looking movement</p>"},{"location":"Learn/Chapter_4/PID/#2-how-to-setup-a-simple-pid-controller","title":"2. How to setup a simple PID controller","text":""},{"location":"Learn/Chapter_4/PID/#kp-proportional-how-far-off-am-i","title":"kP (Proportional) - \"How far off am I?\"","text":"<p><pre><code>error = target - currentPosition;\noutput = kP * error;\n</code></pre> </p> <p>What it does: Provides power proportional to distance from target.  </p> <p>The graph above shows how the output of a P controller increases linearly with the error. The farther you are from the target, the stronger the correction; as you approach the target, the output decreases, resulting in smoother stopping.</p> <ul> <li>Large error: High power</li> <li>Small error: Low power  </li> <li>Zero error: No power</li> </ul>"},{"location":"Learn/Chapter_4/PID/#kd-derivative-how-fast-am-i-approaching","title":"kD (Derivative) - \"How fast am I approaching?\"","text":"<p><pre><code>rateOfChange = (currentError - previousError) / deltaTime;\noutput = kD * rateOfChange;\n</code></pre> </p> <p>What it does: Slows down as you approach the target.  When the error changes rapidly (steep slope), the D output is high and acts to slow the system down, preventing overshoot. When the error changes slowly, the D output is small, allowing the system to approach the target smoothly. - Approaching fast: Reduces power (acts like brakes) - Approaching slow: Minimal effect - Moving away: Adds power</p>"},{"location":"Learn/Chapter_4/PID/#ki-integral-how-long-have-i-been-off","title":"kI (Integral) - \"How long have I been off?\"","text":"<pre><code>errorSum += error * deltaTime;\noutput = kI * errorSum;\n</code></pre> <p>What it does: Eliminates steady-state error over time. </p> <p>Simplfying calculus terms, if we model the \"area\" of error that has occured, that increasing error over time can be multiplied by a constant to \"push\" our control system to reach it's position. </p> <ul> <li>Persistent small error: Gradually increases power</li> <li>At target: Resets to zero</li> </ul>"},{"location":"Learn/Chapter_4/PID/#3-tutorial-implementation","title":"3. Tutorial Implementation","text":"<p>This tutorial covers writing introduction code that will get you started on writing/testing FRC code using commonly available hardware.</p>"},{"location":"Learn/Chapter_4/PID/#basic-position-control-setup","title":"Basic Position Control Setup","text":"<pre><code>public class ArmSubsystem extends SubsystemBase {\n    private final CANSparkMax motor;\n    private final PIDController pidController;\n    private final RelativeEncoder encoder;\n\n    public ArmSubsystem() {\n        motor = new CANSparkMax(1, MotorType.kBrushless);\n        encoder = motor.getEncoder();\n\n        // Start with these values, tune from here\n        pidController = new PIDController(0.1, 0.0, 0.01);\n        pidController.setTolerance(0.05); // Within 0.05 units = \"at target\"\n    }\n\n    public void setTargetPosition(double target) {\n        double output = pidController.calculate(encoder.getPosition(), target);\n        motor.set(output);\n    }\n\n    public boolean atTarget() {\n        return pidController.atSetpoint();\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_4/PID/#4-step-by-step-tuning-process","title":"4. Step-by-Step Tuning Process","text":""},{"location":"Learn/Chapter_4/PID/#step-1-start-with-p-only","title":"Step 1: Start with P Only","text":"<pre><code>pidController = new PIDController(0.0, 0.0, 0.0);\n</code></pre> <ol> <li>Set kI and kD to 0</li> <li>Gradually increase kP until the system moves toward the target</li> <li>Keep increasing until it oscillates around the target</li> <li>Reduce kP by 25-50% or until the system stops occilating</li> </ol> <p>Good kP signs: - Moves quickly to target - Small overshoot or oscillation - Settles near (but not exactly at) target</p> <p>If your your control system settles within an error range that is acceptable for your mechanism, congrats! This is all you need to do.</p> <p>If you need even greater precision and a smaller error range, continue to the next steps.</p>"},{"location":"Learn/Chapter_4/PID/#step-2-add-d-for-smoothness","title":"Step 2: Add D for Smoothness","text":"<p><pre><code>pidController = new PIDController(yourKp, 0.0, 0.01);\n</code></pre> Tuning tip: Add this if kP alone causes overshoot. Bigger values = gentler approach.</p> <ol> <li>Start with small kD (try 10-100x smaller than kP)</li> <li>Increase in smalle amounts until overshoot is eliminated</li> </ol> <p>Warning: Too much kD makes response sluggish</p> <p>Good kD signs: - Smooth approach to target - Little to no overshoot - Still reaches target quickly</p>"},{"location":"Learn/Chapter_4/PID/#step-3-add-i-for-precision-optional","title":"Step 3: Add I for Precision (Optional)","text":"<pre><code>pidController = new PIDController(yourKp, 0.001, yourKd);\n</code></pre> <ol> <li>Only add if system doesn't reach exact target or if it feels that the system needs a small \"push\" to make it to target position</li> <li>Start very small (1000x smaller than kP)</li> <li>Increase slowly until steady-state error disappears</li> </ol> <p>Warning signs (kI too high): - Oscillation that gets worse over time - Overshoot that takes long to settle</p>"},{"location":"Learn/Chapter_4/PID/#5-real-world-example-values","title":"5. Real-World Example Values","text":""},{"location":"Learn/Chapter_4/PID/#arm-mechanism-encoder-ticks","title":"Arm Mechanism (Encoder ticks)","text":"<pre><code>// Example for arm rotating ~180 degrees (8192 ticks)\npidController = new PIDController(0.0005, 0.000001, 0.0001);\npidController.setTolerance(50); // 50 encoder ticks tolerance\n</code></pre>"},{"location":"Learn/Chapter_4/PID/#elevator-inches","title":"Elevator (Inches)","text":"<pre><code>// Example for 24-inch travel elevator\npidController = new PIDController(0.2, 0.0, 0.05);\npidController.setTolerance(0.1); // 0.1 inch tolerance\n</code></pre>"},{"location":"Learn/Chapter_4/PID/#remember-these-are-starting-points-every-mechanism-is-different","title":"Remember: These are starting points! Every mechanism is different.","text":""},{"location":"Learn/Chapter_4/PID/#6-essential-debugging-tips","title":"6. Essential Debugging Tips","text":""},{"location":"Learn/Chapter_4/PID/#monitor-your-values-smart-dashboard","title":"Monitor Your Values (Smart Dashboard)","text":"<pre><code>@Override\npublic void periodic() {\n    SmartDashboard.putNumber(\"Arm Position\", encoder.getPosition());\n    SmartDashboard.putNumber(\"Arm Target\", pidController.getSetpoint());\n    SmartDashboard.putNumber(\"Arm Error\", pidController.getPositionError());\n    SmartDashboard.putNumber(\"Arm Output\", lastPIDOutput);\n}\n</code></pre>"},{"location":"Learn/Chapter_4/PID/#using-advantagekit-for-telemetry","title":"Using AdvantageKit for Telemetry","text":"<p><pre><code>import org.littletonrobotics.advantagekit.AKLog;\n\n@Override\npublic void periodic() {\n    Logger.recordOutput(\"Arm/Position\", encoder.getPosition());\n    Logger.recordOutput(\"Arm/Target\", pidController.getSetpoint());\n    Logger.recordOutput(\"Arm/Error\", pidController.getPositionError());\n    Logger.recordOutput(\"Arm/Output\", lastPIDOutput);\n}\n</code></pre> Using Advantagekit or Epilogue Annotation enables powerful real-time and post-match debugging with tools like AdvantageScope.</p>"},{"location":"Learn/Chapter_4/PID/#common-problems-solutions","title":"Common Problems &amp; Solutions","text":"<p>Problem: System doesn't move at all - Solution: Increase kP significantly</p> <p>Problem: Oscillates wildly around target - Solution: Reduce kP, add kD</p> <p>Problem: Moves toward target but stops short - Solution: Add small kI, or increase kP slightly</p> <p>Problem: Overshoots and takes forever to settle - Solution: Increase kD</p> <p>Problem: Motor saturates (always at 100% power) - Solution: Much smaller kP, check if target is reachable</p>"},{"location":"Learn/Chapter_4/PID/#7-advanced-tips","title":"7. Advanced Tips","text":""},{"location":"Learn/Chapter_4/PID/#handling-gravity-feedforward","title":"Handling Gravity (Feedforward)","text":"<pre><code>// For arms that fight gravity\ndouble gravityCompensation = Math.cos(Math.toRadians(armAngle)) * 0.1;\ndouble pidOutput = pidController.calculate(current, target);\nmotor.set(pidOutput + gravityCompensation);\n</code></pre>"},{"location":"Learn/Chapter_4/PID/#output-limiting","title":"Output Limiting","text":"<pre><code>// Prevent violent movements\ndouble output = pidController.calculate(current, target);\noutput = MathUtil.clamp(output, -0.5, 0.5); // Limit to \u00b150% power\nmotor.set(output);\n</code></pre>"},{"location":"Learn/Chapter_4/PID/#multiple-pid-slots","title":"Multiple PID Slots","text":"<pre><code>// Different gains for different situations\nif (holdingGamePiece) {\n    pidController.setPID(0.1, 0.001, 0.02); // More aggressive\n} else {\n    pidController.setPID(0.05, 0.0, 0.01);  // Gentler\n}\n</code></pre>"},{"location":"Learn/Chapter_4/PID/#8-practice-project","title":"8. Practice Project","text":"<p>Build this step-by-step:</p> <ol> <li>Basic P Control - Get mechanism moving to approximate position</li> <li>Add D Control - Smooth out the movement</li> <li>Add I Control - Fine-tune for precision (if needed)</li> <li>Add Feedforward - Compensate for gravity/friction</li> <li>Integrate Commands - Create reusable position commands</li> </ol> <p>Success criteria: - Reaches target within tolerance consistently - Movement looks smooth and controlled - Responds well to different target positions</p>"},{"location":"Learn/Chapter_4/PID/#where-to-go-next","title":"Where to Go Next","text":"<p>With the basics of pid in mind, These links will take you to other resources that </p> <p>Explore these:</p> <p>\ud83c\udfaf Advanced PID Features - WPILib PID Controller - Complete API reference - Profiled PID - Velocity and acceleration limits</p> <p>\ud83d\udd27 System Identification - SysId Tool - Mathematically derive optimal gains - Feedforward Control - Physics-based control</p> <p>\ud83d\udcca State-Space Control - Linear Quadratic Regulator - Optimal control theory - Kalman Filters - Sensor fusion</p> <p>\u26a1 Motor Controller PID - Phoenix 6 PID - On-board PID control - REV PID - SparkMax closed-loop control</p> <p>\ud83d\ude80 Ready to start tuning? Work through the practice project with a simple mechanism, then apply these concepts to arms, elevators, shooters, and more complex systems!</p> <p>Next steps: Advanced control theory and system identification using WPILib's PID documentation.</p>"},{"location":"Learn/Chapter_4/simple_profiling/","title":"Motion Profiling: Getting Started","text":""},{"location":"Learn/Chapter_4/simple_profiling/#overview","title":"Overview","text":"<p>Motion profiling creates smooth, predictable movement by planning the entire motion before it happens. Instead of jerky start-stop movement, your mechanisms will accelerate smoothly, cruise at optimal speed, then decelerate perfectly to the target.</p> <p>What you'll build: Mechanisms that move with professional smoothness - no more jarring stops or overshooting.</p>"},{"location":"Learn/Chapter_4/simple_profiling/#1-why-motion-profiling","title":"1. Why Motion Profiling?","text":""},{"location":"Learn/Chapter_4/simple_profiling/#the-problem-with-basic-pid","title":"The Problem with Basic PID","text":"<pre><code>// Basic PID: Jerky, unpredictable movement\nsetPosition(currentPos + 24); // Jump command - mechanism lurches forward\n</code></pre> <p>Issues: - Sudden acceleration stresses mechanisms - Inconsistent timing makes coordination difficult - Hard to predict when movement will finish - Can cause oscillation and overshoot</p>"},{"location":"Learn/Chapter_4/simple_profiling/#motion-profiling-solution","title":"Motion Profiling Solution","text":"<pre><code>// Motion profiling: Smooth, planned movement\nTrapezoidProfile.State goal = new TrapezoidProfile.State(targetPos, 0);\n// Automatically generates smooth acceleration \u2192 cruise \u2192 deceleration\n</code></pre> <p>Benefits: - Predictable timing - Know exactly when movement finishes - Smooth acceleration - Gentle on mechanisms and game pieces - Coordinated motion - Multiple mechanisms move in sync - Reduced wear - Less mechanical stress</p>"},{"location":"Learn/Chapter_4/simple_profiling/#2-understanding-motion-profiles","title":"2. Understanding Motion Profiles","text":""},{"location":"Learn/Chapter_4/simple_profiling/#the-trapezoidal-profile","title":"The Trapezoidal Profile","text":"<p>A motion profile defines position, velocity, and acceleration over time:</p> <pre><code>Velocity\n   ^\n   |     /\u203e\u203e\u203e\u203e\u203e\u203e\u203e\\     \u2190 Cruise phase (constant velocity)\n   |    /         \\\n   |   /           \\   \u2190 Acceleration/deceleration phases  \n   |  /             \\\n   |_/_______________\\____\u2192 Time\n     \u2191               \u2191\n   Start            End\n</code></pre> <p>Three Phases: 1. Acceleration - Ramp up to max velocity 2. Cruise - Maintain constant velocity 3. Deceleration - Ramp down to stop at target</p>"},{"location":"Learn/Chapter_4/simple_profiling/#key-parameters","title":"Key Parameters","text":"<ul> <li>Max Velocity - Top speed during cruise phase</li> <li>Max Acceleration - How quickly we ramp up/down</li> <li>Start State - Current position and velocity</li> <li>End State - Target position and velocity (usually 0)</li> </ul>"},{"location":"Learn/Chapter_4/simple_profiling/#analogy-driving-a-car-with-and-without-motion-profiling","title":"Analogy: Driving a Car with and without Motion Profiling","text":"<p>Imagine you're driving a car to a stop sign:</p> <ul> <li> <p>Without Motion Profiling (Basic PID Only):     You slam the gas pedal to reach the stop sign as quickly as possible, then stomp on the brakes at the last moment. The car lurches forward, passengers get tossed around, and you might overshoot or stop short. This is how basic PID works\u2014reacting to errors without planning the whole journey.</p> </li> <li> <p>With Motion Profiling + PID:     Instead, you plan your drive: gently accelerate, cruise at a comfortable speed, then smoothly slow down so you stop exactly at the sign. Motion profiling creates this plan\u2014defining how fast to go and when to speed up or slow down. PID then acts like your cruise control, making small adjustments to keep you on the planned path.</p> </li> </ul> <p>Summary: Motion profiling is like planning your route and speed ahead of time for a smooth, predictable trip. PID is your assistant, making sure you follow that plan precisely, correcting for hills or bumps along the way.</p>"},{"location":"Learn/Chapter_4/simple_profiling/#3-basic-motion-profile-implementation","title":"3. Basic Motion Profile Implementation","text":""},{"location":"Learn/Chapter_4/simple_profiling/#step-1-create-the-profile","title":"Step 1: Create the Profile","text":"<p>These are the necessary components to setup a motion profile subsystem using available hardware. <pre><code>public class ElevatorSubsystem extends SubsystemBase {\n    private final CANSparkMax motor;\n    private final RelativeEncoder encoder;\n    private final PIDController pidController;\n\n    // Motion profile constraints\n    private final TrapezoidProfile.Constraints constraints = \n        new TrapezoidProfile.Constraints(\n            2.0,  // Max velocity (inches/sec)\n            4.0   // Max acceleration (inches/sec\u00b2)\n        );\n\n    private TrapezoidProfile profile;\n    private Timer timer = new Timer();\n    private TrapezoidProfile.State goal;\n\n    public ElevatorSubsystem() {\n        motor = new CANSparkMax(1, MotorType.kBrushless);\n        encoder = motor.getEncoder();\n        pidController = new PIDController(0.1, 0.0, 0.01);\n    }\n}\n</code></pre></p>"},{"location":"Learn/Chapter_4/simple_profiling/#step-2-start-a-profiled-movement","title":"Step 2: Start a Profiled Movement","text":"<p>Write these methods within your subsystems so existing commands or periodics can set position for your mechanism. <pre><code>public void setTargetPosition(double targetPosition) {\n    // Current state (where we are now)\n    TrapezoidProfile.State current = new TrapezoidProfile.State(\n        encoder.getPosition(),\n        getVelocity()\n    );\n\n    // Goal state (where we want to be)\n    goal = new TrapezoidProfile.State(targetPosition, 0);\n\n    // Create the profile\n    profile = new TrapezoidProfile(constraints, goal, current);\n\n    // Start timing\n    timer.restart();\n}\n\nprivate double getVelocity() {\n    return encoder.getVelocity() / 60.0; // Convert RPM to inches/sec\n}\n</code></pre></p>"},{"location":"Learn/Chapter_4/simple_profiling/#step-3-follow-the-profile","title":"Step 3: Follow the Profile","text":"<p>In your subsystem periodic, this logic will run your looped motion profile. <pre><code>@Override\npublic void periodic() {\n    if (profile != null) {\n        // Get the current setpoint from the profile\n        TrapezoidProfile.State setpoint = profile.calculate(timer.get());\n\n        // Use PID to follow the setpoint\n        double pidOutput = pidController.calculate(\n            encoder.getPosition(), \n            setpoint.position\n        );\n\n        // Add feedforward for smooth tracking\n        double feedforward = setpoint.velocity * 0.1; // kV gain\n\n        motor.set(pidOutput + feedforward);\n\n        // Check if profile is complete\n        if (profile.isFinished(timer.get())) {\n            profile = null;\n            timer.stop();\n        }\n    }\n}\n\npublic boolean isAtTarget() {\n    return profile == null || profile.isFinished(timer.get());\n}\n</code></pre></p>"},{"location":"Learn/Chapter_4/simple_profiling/#4-standard-implementation-wpilibs-profiledpidcontroller","title":"4. Standard Implementation: WPILib's ProfiledPIDController","text":""},{"location":"Learn/Chapter_4/simple_profiling/#simplified-implementation","title":"Simplified Implementation","text":"<p>WPILib combines profiling + PID into one easy class:</p> <pre><code>public class ArmSubsystem extends SubsystemBase {\n    private final CANSparkMax motor;\n    private final RelativeEncoder encoder;\n\n    // All-in-one profiled PID controller\n    private final ProfiledPIDController controller;\n\n    public ArmSubsystem() {\n        motor = new CANSparkMax(1, MotorType.kBrushless);\n        encoder = motor.getEncoder();\n\n        // Create constraints\n        TrapezoidProfile.Constraints constraints = \n            new TrapezoidProfile.Constraints(\n                Math.PI,     // Max velocity (rad/s)\n                2 * Math.PI  // Max acceleration (rad/s\u00b2)\n            );\n\n        // Create profiled PID controller\n        controller = new ProfiledPIDController(\n            1.0, 0.0, 0.1,    // PID gains\n            constraints        // Motion constraints\n        );\n\n        controller.setTolerance(0.05); // Position tolerance\n    }\n\n    public void setTargetAngle(double targetRadians) {\n        controller.setGoal(targetRadians);\n    }\n\n    @Override\n    public void periodic() {\n        // Calculate output (automatically handles profiling + PID)\n        double output = controller.calculate(encoder.getPosition());\n        motor.set(output);\n    }\n\n    public boolean atTarget() {\n        return controller.atGoal();\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#5-choosing-profile-parameters","title":"5. Choosing Profile Parameters","text":""},{"location":"Learn/Chapter_4/simple_profiling/#max-velocity-guidelines","title":"Max Velocity Guidelines","text":"<p>Too Fast: - Mechanism can't keep up - Control becomes unstable - Overshooting at end</p> <p>Too Slow: - Wastes time - Looks sluggish</p> <p>Good Starting Point: 75% of theoretical max velocity</p> <pre><code>// Example: Elevator can physically move 4 inches/sec\ndouble maxVelocity = 3.0; // Use 75% for safety margin\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#max-acceleration-guidelines","title":"Max Acceleration Guidelines","text":"<p>Too High: - Jerky motion defeats the purpose - Mechanisms stress and wear - Game pieces might fall</p> <p>Too Low: - Takes forever to reach max speed - Profile becomes mostly acceleration/deceleration</p> <p>Good Starting Point: Time to max velocity = 0.5-1.0 seconds</p> <pre><code>// Example: Reach 3.0 in/s in 0.75 seconds\ndouble maxAcceleration = 3.0 / 0.75; // = 4.0 in/s\u00b2\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#6-real-world-examples","title":"6. Real-World Examples","text":""},{"location":"Learn/Chapter_4/simple_profiling/#elevator-system","title":"Elevator System","text":"<pre><code>// 36-inch travel elevator\nTrapezoidProfile.Constraints elevatorConstraints = \n    new TrapezoidProfile.Constraints(\n        24.0,  // Max velocity: 24 in/s (2 feet/sec)\n        48.0   // Max acceleration: 48 in/s\u00b2 (0.5 sec to max speed)\n    );\n\nProfiledPIDController elevatorController = new ProfiledPIDController(\n    0.2, 0.0, 0.05,     // PID gains\n    elevatorConstraints\n);\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#arm-system","title":"Arm System","text":"<pre><code>// 180-degree arm rotation\nTrapezoidProfile.Constraints armConstraints = \n    new TrapezoidProfile.Constraints(\n        Math.PI,      // Max velocity: \u03c0 rad/s (180\u00b0/sec)  \n        2 * Math.PI   // Max acceleration: 2\u03c0 rad/s\u00b2 (0.5 sec to max)\n    );\n\nProfiledPIDController armController = new ProfiledPIDController(\n    2.0, 0.0, 0.1,    // PID gains (higher for rotational)\n    armConstraints\n);\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#shooter-hood","title":"Shooter Hood","text":"<pre><code>// Small, precise adjustments\nTrapezoidProfile.Constraints hoodConstraints = \n    new TrapezoidProfile.Constraints(\n        0.5,  // Max velocity: 0.5 rad/s (slow and precise)\n        1.0   // Max acceleration: 1.0 rad/s\u00b2\n    );\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#7-advanced-features","title":"7. Advanced Features","text":""},{"location":"Learn/Chapter_4/simple_profiling/#custom-end-velocity","title":"Custom End Velocity","text":"<pre><code>// Don't stop at target - useful for handoffs\nTrapezoidProfile.State goal = new TrapezoidProfile.State(\n    targetPosition, \n    1.0  // End with 1.0 velocity instead of stopping\n);\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#coordinated-multi-axis-movement","title":"Coordinated Multi-Axis Movement","text":"<pre><code>public class IntakeSequence extends CommandBase {\n    public IntakeSequence(ArmSubsystem arm, WristSubsystem wrist) {\n        // Both movements finish at the same time\n        addCommands(\n            new MoveArmCommand(arm, Math.PI/4).withTimeout(2.0),\n            new MoveWristCommand(wrist, Math.PI/6).withTimeout(2.0)\n        );\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#dynamic-constraint-adjustment","title":"Dynamic Constraint Adjustment","text":"<pre><code>public void setSpeed(boolean fastMode) {\n    TrapezoidProfile.Constraints newConstraints;\n\n    if (fastMode) {\n        newConstraints = new TrapezoidProfile.Constraints(4.0, 8.0);\n    } else {\n        newConstraints = new TrapezoidProfile.Constraints(1.0, 2.0);\n    }\n\n    controller.setConstraints(newConstraints);\n}\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#8-debugging-motion-profiles","title":"8. Debugging Motion Profiles","text":""},{"location":"Learn/Chapter_4/simple_profiling/#essential-telemetry","title":"Essential Telemetry","text":"<pre><code>@Override\npublic void periodic() {\n    // Current state\n    SmartDashboard.putNumber(\"Position\", encoder.getPosition());\n    SmartDashboard.putNumber(\"Velocity\", getVelocity());\n\n    // Profile state\n    TrapezoidProfile.State setpoint = controller.getSetpoint();\n    SmartDashboard.putNumber(\"Profile Position\", setpoint.position);\n    SmartDashboard.putNumber(\"Profile Velocity\", setpoint.velocity);\n\n    // Control output\n    double output = controller.calculate(encoder.getPosition());\n    SmartDashboard.putNumber(\"Motor Output\", output);\n    SmartDashboard.putBoolean(\"At Goal\", controller.atGoal());\n\n    motor.set(output);\n}\n</code></pre>"},{"location":"Learn/Chapter_4/simple_profiling/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"<p>Problem: Motion is still jerky - Solution: Lower max acceleration, check PID tuning</p> <p>Problem: Never reaches target precisely - Solution: Add feedforward term, tune PID gains</p> <p>Problem: Takes too long to complete - Solution: Increase max velocity/acceleration within safe limits</p> <p>Problem: Overshoots target - Solution: Lower max velocity, increase D gain, add velocity feedforward</p> <p>Problem: Oscillates around target - Solution: Decrease P gain, increase D gain</p>"},{"location":"Learn/Chapter_4/simple_profiling/#9-practice-project","title":"9. Practice Project","text":"<p>Build this step-by-step:</p> <ol> <li>Basic Profile - Create simple position-to-position movement</li> <li>Tune Parameters - Adjust velocity/acceleration for smooth motion  </li> <li>Add Feedforward - Improve tracking accuracy</li> <li>Multiple Positions - Create commands for common positions</li> <li>Coordinated Motion - Combine multiple mechanisms</li> <li>Dynamic Constraints - Adjust speed based on game state</li> </ol> <p>Success Criteria: - Smooth acceleration and deceleration - Consistent timing for same distance moves - Accurate positioning at target - No mechanical stress or game piece dropping</p>"},{"location":"Learn/Chapter_4/simple_profiling/#where-to-go-next","title":"Where to Go Next","text":"<p>More resources:</p> <p>\ud83c\udfaf Advanced Profiling - WPILib Trajectory Generation - Multi-dimensional paths - Custom Profile Shapes - S-curves and other profiles</p> <p>\ud83d\udd27 Feedforward Control - Feedforward Characterization - Physics-based control - Elevator Feedforward - Gravity compensation</p> <p>\ud83d\udcca Multi-DOF Systems - Differential Drive Trajectories - Robot path following - Swerve Trajectories - Holonomic motion</p> <p>\u26a1 Motor Controller Integration - Phoenix Motion Magic - Hardware-based profiling - REV Smart Motion - SparkMax motion control</p> <p>Next steps: Advanced trajectory optimization and feedforward control using WPILib's trajectory documentation.</p>"},{"location":"Learn/Chapter_5/differential_control/","title":"Differential (Tank) Drive Programming Guide","text":""},{"location":"Learn/Chapter_5/differential_control/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Core Concepts</li> <li>Mathematical Foundation</li> <li>Additional Control Concepts</li> <li>Coordinate Systems</li> <li>Implementation Guide</li> <li>Troubleshooting</li> <li>Additional Resources</li> </ol>"},{"location":"Learn/Chapter_5/differential_control/#1-introduction","title":"1. Introduction","text":"<p>This guide covers the programming concepts needed to implement a differential (tank) drive system using WPILIB. Understanding how joystick inputs are translated into motor commands will help you debug common programming issues.</p> <p>Note: While WPILIB provides high-level abstractions for tank drive, learning the underlying principles will give you more control and flexibility in your robot code.</p> <p>What You'll Learn: - Fundamental tank drive concepts - Mathematical principles behind tank drive kinematics - WPILIB implementation details - Best practices for control systems - Common pitfalls and solutions</p>"},{"location":"Learn/Chapter_5/differential_control/#2-core-concepts","title":"2. Core Concepts","text":"<p>Below are some common terms associated with differential (tank) drive:</p>"},{"location":"Learn/Chapter_5/differential_control/#control-definitions","title":"Control Definitions","text":"<ul> <li> <p>Differential (Tank) Drive</p> <ul> <li>Two sets of wheels (left and right) powered independently</li> <li>Robot turns by varying speed between sides</li> <li>Simple, robust, and widely used in FRC</li> </ul> </li> <li> <p>Teleop Control</p> <ul> <li>Direct driver control via joysticks or tank controls</li> <li>Real-time response to input</li> </ul> </li> </ul>"},{"location":"Learn/Chapter_5/differential_control/#key-components","title":"Key Components","text":"<ul> <li>Left Motors: Drive the left side</li> <li>Right Motors: Drive the right side</li> <li>Encoders: Measure wheel rotation for distance/speed</li> <li>Gyro: Measures robot heading (optional but recommended)</li> </ul>"},{"location":"Learn/Chapter_5/differential_control/#key-classes-in-wpilib","title":"Key Classes in WPILIB","text":""},{"location":"Learn/Chapter_5/differential_control/#differentialdrive","title":"DifferentialDrive","text":"<p>Controls left and right motors for tank drive.</p> <pre><code>DifferentialDrive drive = new DifferentialDrive(leftMotor, rightMotor);\n</code></pre>"},{"location":"Learn/Chapter_5/differential_control/#chassisspeeds","title":"ChassisSpeeds","text":"<p>Represents the desired movement of the robot as a whole.</p> <pre><code>ChassisSpeeds chassisSpeeds = new ChassisSpeeds(\n    xVelocity,    // Forward/backward speed (m/s)\n    0.0,          // No strafe for tank drive\n    omegaVelocity // Rotational speed (rad/s)\n);\n</code></pre>"},{"location":"Learn/Chapter_5/differential_control/#3-mathematical-foundation","title":"3. Mathematical Foundation","text":""},{"location":"Learn/Chapter_5/differential_control/#kinematics","title":"Kinematics","text":"<p>In the context of FRC drives, kinematics refers to the mathematical relationships that describe how the robot's wheel speeds translate to its overall movement (position, velocity, and rotation) on the field, and vice versa. For differential (tank) drive systems, kinematics allows you to:</p> <ul> <li>Determine the robot's forward and rotational velocities based on the speeds of the left and right wheels (forward kinematics).</li> <li>Calculate the required wheel speeds to achieve a desired robot motion (inverse kinematics).</li> </ul> <p>Understanding kinematics is essential for tasks such as autonomous path following, odometry, and precise control of the robot's movement.</p> <p>Tank drive kinematics relate left/right wheel speeds to robot motion.</p> <p>Forward Kinematics: - Calculate robot velocity from left/right wheel speeds</p> <p>Inverse Kinematics: - Calculate left/right wheel speeds from desired robot velocity</p> <p>Formulas: <pre><code>v_left = v - \u03c9 * (trackwidth / 2)\nv_right = v + \u03c9 * (trackwidth / 2)\n</code></pre> Where: - <code>v</code> = Forward velocity (m/s) - <code>\u03c9</code> = Angular velocity (rad/s) - <code>trackwidth</code> = Distance between left and right wheels (meters)</p>"},{"location":"Learn/Chapter_5/differential_control/#4-control-systems","title":"4. Control Systems","text":""},{"location":"Learn/Chapter_5/differential_control/#speed-control","title":"Speed Control","text":"<p>Use PID control for precise velocity control of each side. Place this code within your drive subsystem periodic loop: <pre><code>double leftOutput = leftPID.calculate(currentLeftSpeed, targetLeftSpeed);\ndouble rightOutput = rightPID.calculate(currentRightSpeed, targetRightSpeed);\nleftMotor.setVoltage(leftOutput);\nrightMotor.setVoltage(rightOutput);\n</code></pre></p>"},{"location":"Learn/Chapter_5/differential_control/#arcade-vs-tank-drive","title":"Arcade vs. Tank Drive","text":"<ul> <li>Tank Drive: Separate joystick for each side</li> <li>Arcade Drive: One joystick for forward/backward, one for turning</li> </ul> <p>Arcade Drive Example: <pre><code>drive.arcadeDrive(forward, rotation);\n</code></pre></p> <p>Tank Drive Example: <pre><code>drive.tankDrive(leftSpeed, rightSpeed);\n</code></pre></p>"},{"location":"Learn/Chapter_5/differential_control/#5-coordinate-systems","title":"5. Coordinate Systems","text":"<p>WPILIB uses the same coordinate system as swerve drive. Consistency is important for autonomous routines and sensor integration.</p>"},{"location":"Learn/Chapter_5/differential_control/#6-implementation-guide","title":"6. Implementation Guide","text":"<p>Below is a step-by-step guide for implementing a basic differential (tank) drive subsystem and teleop command.</p>"},{"location":"Learn/Chapter_5/differential_control/#step-1-create-your-tank-drive-subsystem","title":"Step 1: Create Your Tank Drive Subsystem","text":"<p>Start by building a subsystem class to encapsulate all hardware and drive logic. This class should manage the motors, encoders, and gyro, and provide a method to drive the robot.</p> <p>Key Features: - Holds references to motors, encoders, and gyro - Provides a <code>drive</code> method for controlling movement - Initializes all hardware in the constructor</p> <pre><code>public class TankDrive extends SubsystemBase {\n    private final DifferentialDrive drive;\n    private final MotorController leftMotor;\n    private final MotorController rightMotor;\n    private final Encoder leftEncoder;\n    private final Encoder rightEncoder;\n    private final Gyro gyro;\n\n    public TankDrive() {\n        // Initialize motors, encoders, and gyro\n        drive = new DifferentialDrive(leftMotor, rightMotor);\n    }\n\n    public void drive(double forward, double rotation) {\n        drive.arcadeDrive(forward, rotation);\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/differential_control/#step-2-implement-a-teleop-command","title":"Step 2: Implement a Teleop Command","text":"<p>Create a command class to handle teleoperated driving. This command reads joystick values using suppliers, applies a deadband for smoother control, and calls the drive method.</p> <p>Key Steps: - Accepts suppliers for forward and rotation input - Applies deadband to filter out small joystick noise - Calls the subsystem's drive method each cycle</p> <pre><code>public class TeleopTankCommand extends CommandBase {\n    private final TankDrive tankDrive;\n    private final Supplier&lt;Double&gt; forwardSupplier;\n    private final Supplier&lt;Double&gt; rotationSupplier;\n\n    public TeleopTankCommand(\n        TankDrive tankDrive,\n        Supplier&lt;Double&gt; forwardSupplier,\n        Supplier&lt;Double&gt; rotationSupplier\n    ) {\n        this.tankDrive = tankDrive;\n        this.forwardSupplier = forwardSupplier;\n        this.rotationSupplier = rotationSupplier;\n        addRequirements(tankDrive);\n    }\n\n    @Override\n    public void execute() {\n        double forward = MathUtil.applyDeadband(forwardSupplier.get(), 0.1);\n        double rotation = MathUtil.applyDeadband(rotationSupplier.get(), 0.1);\n        tankDrive.drive(forward, rotation);\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/differential_control/#step-3-integrate-into","title":"Step 3: Integrate into","text":"<p>Set up your subsystem and command in <code>RobotContainer</code> to enable teleop driving. Use suppliers to connect joystick axes to your command.</p> <p>Key Steps: - Instantiate the subsystem and controller - Set the teleop command as the default for the subsystem - Use lambda expressions to supply joystick values</p> <pre><code>public class RobotContainer {\n    private final TankDrive tankDrive = new TankDrive();\n    private final CommandXboxController driverController = new CommandXboxController(0);\n\n    public RobotContainer() {\n        // Set the default command for teleop driving\n        tankDrive.setDefaultCommand(\n            new TeleopTankCommand(\n                tankDrive,\n                () -&gt; -driverController.getLeftY(),\n                () -&gt; -driverController.getRightX()\n            )\n        );\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/differential_control/#7-extra-features","title":"7. Extra Features","text":""},{"location":"Learn/Chapter_5/differential_control/#encoder-distance-calculation","title":"Encoder Distance Calculation","text":"<pre><code>// Returns distance in meters\npublic double getLeftDistance() {\n    return leftEncoder.getDistance();\n}\npublic double getRightDistance() {\n    return rightEncoder.getDistance();\n}\n</code></pre>"},{"location":"Learn/Chapter_5/differential_control/#8-troubleshooting-and-possible-fixes","title":"8. Troubleshooting and Possible Fixes","text":"<p>Robot Drifts or Turns Unexpectedly - Verify encoder and gyro readings</p> <p>Inaccurate Distance or Heading - Ensure encoder conversion factors/instantiations are correct</p> <p>Jerky or Unstable Movement - Add input deadbands - Add 0.5 or 0.2 scale factors to decrease \"sensitivity\"</p>"},{"location":"Learn/Chapter_5/differential_control/#9-additional-resources","title":"9. Additional Resources","text":"<ul> <li>WPILIB Differential Drive Documentation</li> <li>WPILIB Odometry</li> <li>FRC Tank Drive Examples</li> </ul>"},{"location":"Learn/Chapter_5/drive_bases/","title":"Drive Base in FRC Programming","text":""},{"location":"Learn/Chapter_5/drive_bases/#types-of-drive-bases","title":"Types of Drive Bases","text":"<ul> <li> <p>Differential(aka West Coast Drive / Tank Drive) enables simple maneuverability using two mirrored drivetrains(sets of 2-3 wheels powered by a set of motors and gearboxes) that independently power each side of the drive base. This allows for simple yet effective manuvering with speed differences, strong pushing power, and a ease of control system implementation.</p> </li> <li> <p>Swerve drive enables ominidirectional drivebase control, allowing robots drive in ways that typical drivebases cannot do (ie. spin in place, sideways, diagonally, drive while spinning). It has become the dominant drivebase in FRC since 2022\u20132023, giving teams a significant mobility advantage.</p> </li> <li> <p>Mecanum drive is four wheel drive system in which every wheel is a \"mecanum wheel\"(pictured below). This special wheel enables ominidrectional movement that includes strafing directly sideways or diagonally.</p> </li> <li> <p>H Drive is a differential drive with an added fith wheel(usually an omniwheel) at the center of the drivebase. This extra wheel enables sideways strafe motion.</p> </li> <li> <p>Other Swerve Drives While the most common FRC drive bases are covered above, there are several other unique or experimental designs used by teams for specific strategic advantages. Butterfly Drive and Octanum Drive are hybrid systems that allow switching between traction and omnidirectional movement\u2014Butterfly uses omni and traction wheels, while Octanum uses mecanum and traction wheels. Kiwi Drive uses three omni wheels in a triangular layout for holonomic movement. Crab Drive mounts all wheels on steering modules so the robot can move in any direction without changing orientation, with all wheels turning together. Ackermann Steering mimics car-style steering with only the front wheels turning, and is rare in FRC. Teams also sometimes create custom or hybrid drives to meet unique game challenges, combining features from multiple systems or using nonstandard wheel arrangements. These drive bases are less common but demonstrate the creativity and engineering diversity found in FRC.</p> </li> </ul>"},{"location":"Learn/Chapter_5/drive_bases/#focus-of-this-guide","title":"Focus of This Guide","text":"<p>This guide will primarily focus on programming swerve drives and tank drives (also known as differential or West Coast drive). These systems are most commonly used in competition due to their versatility, performance, and prevalence in modern FRC games. The following chapters will provide detailed explanations, code examples, and best practices for implementing both swerve and tank drive systems in your robot code.</p>"},{"location":"Learn/Chapter_5/drive_bases/#key-concepts-across-all-drivebases","title":"Key Concepts Across All Drivebases","text":""},{"location":"Learn/Chapter_5/drive_bases/#java-suppliers-and-frc-usage","title":"Java Suppliers and FRC Usage","text":"<p>A <code>Supplier&lt;T&gt;</code> in Java is a functional interface that represents a function with no arguments that returns a value of type <code>T</code>. Suppliers are commonly used for deferred or dynamic value retrieval, such as reading sensor data or getting the latest state of a subsystem.</p> <p>FRC Example: In FRC robot code, Suppliers are often used to pass joystick or sensor values into commands or subsystems. For example, when creating a command to drive a robot, you might use <code>DoubleSupplier</code> (a primitive specialization of <code>Supplier&lt;Double&gt;</code>) to provide the latest joystick input each time the command runs:</p> <pre><code>// Example: Passing joystick values as suppliers to a drive command\nDoubleSupplier forward = () -&gt; driverController.getLeftY();\nDoubleSupplier turn = () -&gt; driverController.getRightX();\n\nDriveCommand driveCommand = new DriveCommand(driveSubsystem, forward, turn);\n</code></pre> <p>This approach ensures the command always uses the most recent joystick values, making the robot responsive to operator input.</p>"},{"location":"Learn/Chapter_5/drive_bases/#organization-of-drive-base-subsystems-commands-and-command-calls-in-frc","title":"Organization of Drive Base Subsystems, Commands, and Command Calls in FRC","text":"<p>In the FRC Command-Based framework, robot code is organized into subsystems and commands to promote modularity and clarity:</p>"},{"location":"Learn/Chapter_5/drive_bases/#1-subsystems","title":"1. Subsystems","text":"<p>A subsystem represents a physical part of the robot (e.g., the drive base). It contains methods to control hardware (motors, sensors) and maintains the state of that mechanism.</p> <pre><code>public class DriveSubsystem extends SubsystemBase {\n    // Motor controllers and sensors declared here\n\n    public void drive(double forward, double turn) {\n        // Code to set motor outputs\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/drive_bases/#2-commands","title":"2. Commands","text":"<p>A command defines a specific robot action or behavior, often using one or more subsystems. For a drive base, a command might continuously read joystick values and call the drive method.</p> <pre><code>public class DriveCommand extends CommandBase {\n    private final DriveSubsystem driveSubsystem;\n    private final DoubleSupplier forward, turn;\n\n    public DriveCommand(DriveSubsystem subsystem, DoubleSupplier forward, DoubleSupplier turn) {\n        this.driveSubsystem = subsystem;\n        this.forward = forward;\n        this.turn = turn;\n        addRequirements(subsystem);\n    }\n\n    @Override\n    public void execute() {\n        driveSubsystem.drive(forward.getAsDouble(), turn.getAsDouble());\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/drive_bases/#3-command-calls-binding","title":"3. Command Calls (Binding)","text":"<p>Commands are scheduled or bound to triggers (like joystick buttons or default behaviors) in the <code>RobotContainer</code> class.</p> <pre><code>driveSubsystem.setDefaultCommand(\n    new DriveCommand(driveSubsystem, \n                     () -&gt; driverController.getLeftY(), \n                     () -&gt; driverController.getRightX())\n);\n</code></pre>"},{"location":"Learn/Chapter_5/drive_bases/#diagram-command-based-structure-for-drive-base","title":"Diagram: Command-Based Structure for Drive Base","text":"<pre><code>graph TD\n    A[Driver Controller] --&gt;|Supplies values| B[DriveCommand]\n    B --&gt;|Calls| C[DriveSubsystem]\n    C --&gt;|Controls| D[Motors/Sensors]</code></pre> <ul> <li>Driver Controller: Provides input (joystick values).</li> <li>DriveCommand: Reads input, calls drive methods.</li> <li>DriveSubsystem: Implements hardware control.</li> <li>Motors/Sensors: Physical hardware on the robot.</li> </ul> <p>This structure separates hardware logic from robot behavior, making code easier to maintain and extend.</p>"},{"location":"Learn/Chapter_5/drive_bases/#simulator","title":"Simulator","text":"<p>Below is an interactive simulator showing the difference between drivebases.</p> FRC Drive Base Simulator \ud83e\udd16 FRC Drive Base Simulator Drive Type Selection Tank Drive Swerve Drive Drive Inputs Forward/Backward Left/Right Turn Swerve Inputs Strafe (Left/Right) Rotation \ud83d\udd04 Reset Position X: 0.0m Y: 0.0m Angle: 0.0\u00b0 Speed: 0.0m/s \ud83c\udfae Controls <p>Movement:</p> WASD - WASD movement              \u2191\u2190\u2193\u2192 - Arrow keys              <p>Swerve-specific:</p> QE - Rotate left/right              ZC - Strafe left/right              <p>Tank Drive: Uses differential steering - forward/back + left/right turn</p> <p>Swerve Drive: Full omnidirectional movement with independent translation and rotation</p>"},{"location":"Learn/Chapter_5/drive_bases/#where-to-go-next","title":"Where to go Next?","text":"<p>Choose the following section based off the drivebase you plan to program:</p> <p>Swerve Control</p> <p>Tank Drive Control </p>"},{"location":"Learn/Chapter_5/drive_bases/#article","title":"Article","text":"<p>For more information on drive bases. Please check out:</p> <p>https://docs.revrobotics.com/frc-kickoff-concepts/2023/drivetrains</p>"},{"location":"Learn/Chapter_5/simple_trajectories/","title":"Swerve Trajectory: Getting Started","text":""},{"location":"Learn/Chapter_5/simple_trajectories/#overview","title":"Overview","text":"<p>Swerve trajectory combines the power of motion profiling with holonomic movement, letting your robot follow smooth, planned paths while rotating independently. Instead of jerky point-to-point movement, your robot will glide along curves with perfect timing and coordination.</p> <p>Jump to Basic Implementation for a quick start. However it is highly recommended to read the sections 1-2 for a lower level understanding of trajectories.</p> <p>What you'll build: A Swerve robot that smoothly follows pre-planned paths with independent rotation control. This will cover the underlying explanation behind trajectory planning and a simple implementation of trajectory planning and following from wpilib. </p>"},{"location":"Learn/Chapter_5/simple_trajectories/#1-why-swerve-trajectories","title":"1. Why Swerve Trajectories?","text":""},{"location":"Learn/Chapter_5/simple_trajectories/#the-problem-with-basic-movement","title":"The Problem with Basic Movement","text":"<pre><code>// Basic swerve: Get there somehow\nnew SequentialCommandGroup(\n    new InstantCommand(() -&gt; swerve.drive(new ChassisSpeeds(2.0, 0, 0))), // Move forward\n    new WaitCommand(1.0), // Wait for 1 second (adjust as needed)\n    new InstantCommand(() -&gt; swerve.drive(new ChassisSpeeds(0, 2.0, 0))) // Move sideways\n)\n</code></pre> <p>Issues: - Did not need to rely on vision -&gt; Teams primarily chose full wheel odometry to measure how far they travelled, which could potentially lead to measurment errors. - Can't avoid obstacles smoothly</p> <p>However, with tank drives being the primary choice of drive base, this method of striaght-line movements was the common autonomous method in games pre-2022.</p>"},{"location":"Learn/Chapter_5/simple_trajectories/#swerve-trajectory-solution","title":"Swerve Trajectory Solution","text":"<pre><code>public Command trajectory() {\n    PathPlannerPath path = getStraightLinePath(currentPose, goal, DriveConstants.PATHFINDING_CONSTRAINTS); \n    return new TrajectoryDriveCmd(path, true, true).\n}\n// Robot follows exact path with precise timing\n</code></pre> <p>Benefits: - Predictable paths - Know exactly where robot will be - Smooth motion - Curved paths instead of sharp corners - Independent rotation - Face any direction while moving - Obstacle avoidance - Plan around field elements</p> <p>With the introduction of swerve drives and outside libraries such as Pathplanner and Choreo, the bar to acchiving trajectory has been lowered. More and more teams can take advantage of these tools to boost compeittion performance. </p> <p>However, it is crucial to understand what happens at a lower level with trajectory planning. Without understanding some lower level components, it's easy for teams to solve their trajectory issues with the wrong methods due to a lack of understanding. </p> <p>Holonomic Advantage:</p> <p>If you recall from Swerve Control: Control Definitions, Holomonic Controllers, which include swerves operating on trajectory, will not require the robot to face in a specific direction to follow the path, easing time constraints and enabling more game specific actions. <pre><code>Traditional Robot:    Swerve Robot:\n     \u2191                   \u2191 \u2190 Robot can face any direction\n     |                   |   while following any path\n  [ROBOT]             [ROBOT]\n     |                   \u2193\n     Path = Facing    Path \u2260 Facing\n</code></pre></p>"},{"location":"Learn/Chapter_5/simple_trajectories/#2-understanding-swerve-trajectories","title":"2. Understanding Swerve Trajectories","text":""},{"location":"Learn/Chapter_5/simple_trajectories/#key-distinctions","title":"Key Distinctions","text":"<p>There is a key distinction to make between Trajectories and Paths - Path - The shape/route (like a road)     - Used by Pathplanner and Choreo vendors display on gui. - Trajectory - Path + timing (like a GPS route with arrival times)</p> <p>Further Reading and Reference: Path Planning and Trajectory Planning Algorithms: a General Overview. </p>"},{"location":"Learn/Chapter_5/simple_trajectories/#components-of-a-swerve-trajectory","title":"Components of a Swerve Trajectory","text":"<p>Due to the slight differences in how Pathplanner and Choreo define a trajectory, we will be explaining the WPILIB definition of a trajectory.</p> <p>Translation Trajectory: - X and Y velocity - X and Y acceleration - Goal Pose</p> <p>Rotation Trajectory: - Angular velocity  - Angular Accel - Heading</p> <p>Combined Result: At any time <code>t</code>, the trajectory provides a full robot state at that singular moment. This is usually encapsulated within its own object called a state or sample. This can be then used as target information for our robot to follow in order to stay on the path the user provided. <pre><code>// At any time t, the trajectory provides:\nTrajectoryState state = trajectory.sample(t);\n// state.poseMeters     - Where robot should be (x, y, rotation)\n// state.velocityMPS    - How fast robot should move\n// state.curvatureRPM   - Path curvature at this point\n</code></pre></p>"},{"location":"Learn/Chapter_5/simple_trajectories/#path-constraints","title":"Path Constraints","text":"<p>Path constraints are rules that limit the robot's speed, acceleration, or other properties along specific sections or entire sections of a trajectory. They help give more control over what the drive base should do at a specific area. Similarly to before, there are slight implementation differences across trajectory libraries. WPILIB uses a code based approach shown below while Pathplanner and Chreo prefer to have them defined on their path GUI.</p> <p>Common constraint types: - Max Velocity Constraint: Limits top speed in a region. - Max Acceleration Constraint: Restricts acceleration. - Centripetal Acceleration Constraint: Prevents excessive speed. - Region Constraint: Applies constraints only within a defined area. - Custom Constraints: WPILIB trajectory implementation specifcally allows users to implement their own logics that determines when velocity, acceleration, or region constraints should be applied.</p> <p>Path constraints can have 1 or more of these constraints combined into one. Here is a WPILIB example:</p> <pre><code>TrajectoryConfig config = new TrajectoryConfig(2.0, 1.0);\n\n// Slow down in the scoring zone (rectangle from (2,2) to (4,4))\nconfig.addConstraint(new RectangularRegionConstraint(\n    new Translation2d(2, 2),           // Bottom-left corner\n    new Translation2d(4, 4),           // Top-right corner\n    new MaxVelocityConstraint(0.5)     // Limit speed to 0.5 m/s in this region\n));\n\n// Generate trajectory with constraints\nTrajectory trajectory = TrajectoryGenerator.generateTrajectory(\n    new Pose2d(0, 0, new Rotation2d(0)),\n    List.of(new Translation2d(3, 3)),\n    new Pose2d(5, 5, new Rotation2d(0)),\n    config\n);\n</code></pre>"},{"location":"Learn/Chapter_5/simple_trajectories/#holomonic-controller","title":"Holomonic Controller","text":"<p>The robot's current state (position and velocity) is used by a holonomic controller object to calculate the desired chassis speeds (vx, vy, omega) needed to follow the trajectory. These speeds are then sent to the robot's drive subsystem, enabling the trajectory.</p> <p>Key Steps: - The robot's current pose and velocity are passed to the holonomic controller. - The controller computes the required chassis speeds (vx, vy, omega) to achieve the desired motion. - These speeds are then sent to the robot's drive subsystem for execution.</p> <pre><code>// Example: Using a holonomic controller to follow a trajectory\nHolonomicDriveController controller = new HolonomicDriveController(\n    new PIDController(1.0, 0, 0),    // X controller\n    new PIDController(1.0, 0, 0),    // Y controller\n    new ProfiledPIDController(2.0, 0, 0, new TrapezoidProfile.Constraints(3.0, 2.0)) // Rotation\n);\n\n@Override\npublic void periodic() {\n    if (isFollowingTrajectory) {\n        Trajectory.State desiredState = trajectory.sample(timer.get());\n        Pose2d currentPose = swerve.getPose();\n\n        ChassisSpeeds speeds = hocontroller.calculate(\n            currentPose,\n            desiredState,\n            desiredState.poseMeters.getRotation()\n        );\n    }\n}\n</code></pre> <p>Reference and Further Reading - WPILib Trajectory Class Reference - WPILib Trajectory Generation Documentation - WPILib Trajectory Constraints Documentation</p>"},{"location":"Learn/Chapter_5/simple_trajectories/#3-basic-implementation","title":"3. Basic Implementation","text":"<p>Most teams will choose from vendors such as Pathplanner or Choreo to run their trajectory functions due to their add-on functionality. However it is recommended to first implement WPILIB trajectory. This will enable you to have access to every part of how the trajectory runs and familiarlize yourself with all the parameters available to you to adjust your trajectory.  After running this iteration, it will be easier to integrate in Choreo or Pathlanner as your team chooses.</p> <p>Below is a simple implementation of WPILIB Trajectory. Follow this guide to get it working. </p>"},{"location":"Learn/Chapter_5/simple_trajectories/#create-your-first-trajectory","title":"Create Your First Trajectory","text":"<ol> <li>Create a custom class command that will house your trajectory following code.</li> <li>Make sure to have the following features<ul> <li>A trajectory object taken in as a parameter</li> <li>A swerve subsystem object taken in as a parameter</li> <li>Timer for tracking state of a trajectory</li> </ul> </li> <li>Constructor that completes any remaining object declarations.</li> </ol>"},{"location":"Learn/Chapter_5/simple_trajectories/#define-your-holomonic-controller","title":"Define your Holomonic Controller","text":"<ol> <li>Create a holomonic controller object that is defined in the Command constructor. Use PIDController objects as parameters.</li> <li>Compartively to other PID control systems, start with int type numbers such as 1.0, 2.0, etc. </li> </ol>"},{"location":"Learn/Chapter_5/simple_trajectories/#populate-execute-initialize-isfinished","title":"Populate Execute, Initialize, isFinished","text":"<ol> <li><code>Intitalize</code> should contain all your reset methods for:<ul> <li>reseting odometry if it's the first path</li> <li>restarting the internal timer to match with the trajectory time</li> </ul> </li> <li> <p><code>Execute</code> should contain:</p> <ul> <li>all your trajectory state/swerve measurment readings.</li> <li>The final outputs should be a chassisspeed object that can be fed into the swerve drive.</li> </ul> </li> <li> <p><code>isFinished</code> should terminate when:</p> </li> <li>The recorded swerve measurments are in a certain deadband of the goal Pose</li> <li>When our timer exceeds the estimated time for our trajectory.</li> </ol> <pre><code>public class TrajectoryCommand extends CommandBase {\n    private final SwerveDrive swerve;\n    private final Timer timer = new Timer();\n    private final Trajectory trajectory;\n    private final HolonomicDriveController holonomicController;\n    private static final double POSITION_TOLERANCE = 0.10; // meters\n    private static final double ROTATION_TOLERANCE = 5.0; // degrees\n\n    public TrajectoryCommand(SwerveDrive swerve, Trajectory trajectory) {\n        this.swerve = swerve;\n        this.trajectory = trajectory;\n\n        // Holonomic controller for trajectory following\n        holonomicController = new HolonomicDriveController(\n            new PIDController(1.0, 0, 0),    // X controller\n            new PIDController(1.0, 0, 0),    // Y controller\n            new ProfiledPIDController(2.0, 0, 0, new TrapezoidProfile.Constraints(3.0, 2.0)) // Rotation\n        );\n\n        addRequirements(swerve);\n    }\n\n    @Override\n    public void initialize() {\n        timer.restart();\n        swerve.resetOdometry(trajectory.getInitialPose());\n    }\n\n    @Override\n    public void execute() {\n        // Get current trajectory state\n        Trajectory.State desiredState = trajectory.sample(timer.get());\n\n        // Get current robot pose\n        Pose2d currentPose = swerve.getPose();\n\n        // Use holonomic controller to calculate chassis speeds\n        ChassisSpeeds chassisSpeeds = holonomicController.calculate(\n            currentPose,\n            desiredState,\n            desiredState.poseMeters.getRotation()\n        );\n\n        swerve.drive(chassisSpeeds);\n\n        // SmartDashboard telemetry for debugging\n        SmartDashboard.putNumber(\"Trajectory Time\", timer.get());\n        SmartDashboard.putNumber(\"Desired X\", desiredState.poseMeters.getX());\n        SmartDashboard.putNumber(\"Desired Y\", desiredState.poseMeters.getY());\n        SmartDashboard.putNumber(\"Desired Rotation\", desiredState.poseMeters.getRotation().getDegrees());\n        SmartDashboard.putNumber(\"Robot X\", currentPose.getX());\n        SmartDashboard.putNumber(\"Robot Y\", currentPose.getY());\n        SmartDashboard.putNumber(\"Robot Rotation\", currentPose.getRotation().getDegrees());\n        SmartDashboard.putNumber(\"X Error\", currentPose.getX() - desiredState.poseMeters.getX());\n        SmartDashboard.putNumber(\"Y Error\", currentPose.getY() - desiredState.poseMeters.getY());\n    }\n\n    @Override\n    public boolean isFinished() {\n        Pose2d currentPose = swerve.getPose();\n        Pose2d goalPose = trajectory.getStates().get(trajectory.getStates().size() - 1).poseMeters;\n\n        boolean positionClose = currentPose.getTranslation().getDistance(goalPose.getTranslation()) &lt; POSITION_TOLERANCE;\n        boolean rotationClose = Math.abs(currentPose.getRotation().getDegrees() - goalPose.getRotation().getDegrees()) &lt; ROTATION_TOLERANCE;\n\n        return timer.get() &gt; trajectory.getTotalTimeSeconds() || (positionClose &amp;&amp; rotationClose);\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/simple_trajectories/#5-creating-autos","title":"5. Creating Autos","text":"<p>Now that we've created our first intial trajectory commands, let's fit it into an autonomous sequence.</p>"},{"location":"Learn/Chapter_5/simple_trajectories/#creating-trajectories","title":"Creating trajectories","text":"<ol> <li>Create a new trajectory config with a max speed and max acceleration</li> <li>Create a trjaectory with:<ul> <li>Start and Goal Poses</li> <li>Translation2d type waypoints (Units in Meters) <pre><code>//   maxSpeed: The maximum speed for the trajectory (in meters per second).\n//   maxAcceleration: The maximum acceleration for the trajectory (in meters per second squared).\nTrajectoryConfig config = new TrajectoryConfig(2.0, 1.0); \n\n// Curved path through multiple points\nTrajectory complexPath = TrajectoryGenerator.generateTrajectory(\n    new Pose2d(0, 0, new Rotation2d(0)),           // Start\n    List.of(\n        new Translation2d(1, 1),                    // Waypoint 1\n        new Translation2d(2, -1),                   // Waypoint 2\n        new Translation2d(3, 0)                     // Waypoint 3\n    ),\n    new Pose2d(4, 1, new Rotation2d(Math.PI)),     // End facing backward\n    config\n);\n</code></pre></li> </ul> </li> </ol>"},{"location":"Learn/Chapter_5/simple_trajectories/#optionalconstraints-for-different-sections","title":"(Optional)Constraints for Different Sections","text":"<pre><code>// Slower through tight areas\nTrajectoryConfig slowConfig = new TrajectoryConfig(1.0, 0.5);\nslowConfig.addConstraint(new RectangularRegionConstraint(\n    new Translation2d(1, -1),  // Bottom-left corner\n    new Translation2d(3, 1),   // Top-right corner  \n    new MaxVelocityConstraint(0.5) // Slow down in this region\n));\n</code></pre>"},{"location":"Learn/Chapter_5/simple_trajectories/#apply-it-to-a-drive-trajectory-command","title":"Apply it to a Drive Trajectory Command","text":"<p>Here is a full example usage as a full Command. This can be used in a chain of trajectory drive commands as shown in Real-World example below.</p> <pre><code>// Single-command autonomous routine using TrajectoryCommand\npublic Command testPath1(SwerveDrive swerve) {\n    TrajectoryConfig config = new TrajectoryConfig(2.0, 1.0);\n\n    Trajectory trajectory = TrajectoryGenerator.generateTrajectory(\n        new Pose2d(0, 0, new Rotation2d(0)),\n        List.of(new Translation2d(2, 1), new Translation2d(3, -1)),\n        new Pose2d(4, 0, new Rotation2d(Math.PI)),\n        config\n    );\n\n    return new TrajectoryCommand(swerve, trajectory);\n}\n</code></pre>"},{"location":"Learn/Chapter_5/simple_trajectories/#6-real-world-examples","title":"6. Real-World Examples","text":""},{"location":"Learn/Chapter_5/simple_trajectories/#simple-autonomous-routine","title":"Simple Autonomous Routine","text":"<pre><code>public class SimpleAuto extends SequentialCommandGroup {\n    public SimpleAuto(SwerveDrive swerve) {\n        // Trajectory config: max speed 2 m/s, max accel 1 m/s^2\n        TrajectoryConfig config = new TrajectoryConfig(2.0, 1.0);\n\n        // First trajectory: leave starting position\n        Trajectory leaveTarmac = TrajectoryGenerator.generateTrajectory(\n            new Pose2d(0, 0, new Rotation2d(0)),\n            List.of(new Translation2d(1, 0)),\n            new Pose2d(2, 0, new Rotation2d(0)),\n            config\n        );\n\n        // Second trajectory: return and face backward\n        Trajectory returnToShoot = TrajectoryGenerator.generateTrajectory(\n            new Pose2d(2, 0, new Rotation2d(0)),\n            List.of(new Translation2d(1, 0)),\n            new Pose2d(0, 0, new Rotation2d(Math.PI)),\n            config\n        );\n\n        addCommands(\n            new TrajectoryCommand(swerve, leaveTarmac),\n            new TrajectoryCommand(swerve, returnToShoot)\n        );\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/simple_trajectories/#7-tuning-and-debugging","title":"7. Tuning and Debugging","text":""},{"location":"Learn/Chapter_5/simple_trajectories/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":""},{"location":"Learn/Chapter_5/simple_trajectories/#pid-tuning-guidelines","title":"PID Tuning Guidelines","text":"<p>Important Make sure your swerve drive is tuned properly first before tuning your holonomic controller. See Chapter 4: Simple Profiling for tunning a velocity controller. Tuning the holonomic controller first will lead to issues in following your trajectory properly.</p> <p>It's recommended to start big incrementing by ints or in 10s decimals places when tunning.</p> <pre><code>// Start with these values, adjust based on robot performance\nPIDController xController = new PIDController(\n    1.0,  // P: Start here, increase if robot is slow to correct\n    0.0,  // I: Usually not needed for trajectory following  \n    0.1   // D: Add if oscillating around path\n);\n\n// Rotation typically needs higher gains\nPIDController thetaController = new PIDController(\n    2.0,  // P: Higher for rotation\n    0.0,  // I: Avoid unless persistent error\n    0.0   // D: Avoid\n);\n</code></pre> <p>Problem: Robot doesn't follow path accurately - Possible Fix: Tune PID controllers, check odometry calibration. Are the drive velocity controllers tuned properly?</p> <p>Problem: Robot is not accurate to real world. - Possible Fix: Double check field measurements, calibrate all sensors, and confirm the coordinate system matches your field setup. Even small measurement errors can accumulate and lead to noticeable trajectory inaccuracies.</p> <p>Problem: Path is too aggressive/jerky - Possible Fix: Lower max velocity/acceleration in TrajectoryConfig.</p> <p>Problem: Robot overshoots waypoints - Possible Fix: Increase derivative gains, add velocity constraints. Use SmartDashboard to check if your current pose is lagging behind your goal pose.</p> <p>Problem: Rotation lags behind translation - Possible Fix: Tune theta controller separately, check angular constraints.</p> <p>Problem: Robot drifts off path over time - Possible Fix: Improve odometry (better wheel characterization, vision correction).</p>"},{"location":"Learn/Chapter_5/simple_trajectories/#success-criteria","title":"Success Criteria:","text":"<ul> <li>Robot follows planned path within 10cm accuracy</li> <li>Smooth acceleration and deceleration</li> <li>Consistent timing for autonomous coordination</li> <li>Independent rotation works smoothly</li> </ul>"},{"location":"Learn/Chapter_5/simple_trajectories/#8-advanced-techniques","title":"8. Advanced Techniques","text":""},{"location":"Learn/Chapter_5/simple_trajectories/#dynamic-trajectory-generation","title":"Dynamic Trajectory Generation","text":"<p>Below is a simple Drive to Pose that can be useful for games where the player needs to autonomously align with scoring or loading positions.</p>"},{"location":"Learn/Chapter_5/simple_trajectories/#public-command-autotargetposeswervedrive-swerve-pose2d-target-create-new-trajectory-from-current-position-trajectory-dynamicpath-trajectorygeneratorgeneratetrajectory-swervegetpose-start-from-where-we-are-now-listof-target-go-to-new-target-config-start-following-immediately-return-new-trajectorycommandswerve-dynamicpath","title":"<pre><code>public Command autoTargetPose(SwerveDrive swerve, Pose2d target) {\n    // Create new trajectory from current position\n    Trajectory dynamicPath = TrajectoryGenerator.generateTrajectory(\n        swerve.getPose(),      // Start from where we are now\n        List.of(),\n        target,                // Go to new target\n        config\n    );\n\n    // Start following immediately\n    return new TrajectoryCommand(swerve, dynamicPath)\n}\n</code></pre>","text":""},{"location":"Learn/Chapter_5/simple_trajectories/#links-for-further-reference","title":"Links for further reference","text":"<p>Visual Path Planning - PathPlanner - Visual trajectory designer - Choreo - Advanced trajectory optimization</p>"},{"location":"Learn/Chapter_5/swerve_control/","title":"Swerve Drive Programming Guide","text":""},{"location":"Learn/Chapter_5/swerve_control/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Core Concepts</li> <li>Mathematical Foundation</li> <li>Additional Control Concepts</li> <li>Coordinate Systems</li> <li>Implementation Guide</li> <li>Troubleshooting</li> <li>Additional Resources</li> </ol>"},{"location":"Learn/Chapter_5/swerve_control/#1-introduction","title":"1. Introduction","text":"<p>This guide covers the programming concepts needed to implement a swerve drive system using the WPILIB framework. Understanding how the your joystick inputs are turned into module commands will greatly help in debugging common programming issues associated with swerve.</p> <p>Note: While there are many Swerve Drive solutions that abstract much of the coding you need to do to create working swerve software such as (YAGSL, CTRE), it is recommended to learn the WPILIB way of creating swerve software. This will enable you to have more control over every part of your swerve code from joystick to motor.</p> <p>Note: Before starting this section, review the Command-Based Programming Guide. Many concepts here rely on command-based programming principles discussed in that guide.</p> <p>For a quick start, you can skip to the Implementation Guide to begin coding your swerve drive. However it's recommended to review sections 2\u20135 first for a deeper understanding.</p> <p>What You'll Learn: - Fundamental swerve drive concepts - Mathematical principles behind swerve kinematics - WPILIB implementation details - Best practices for control systems - Common pitfalls and solutions</p>"},{"location":"Learn/Chapter_5/swerve_control/#2-core-concepts","title":"2. Core Concepts","text":"<p>Below are some common terms associated with swerve drive:</p>"},{"location":"Learn/Chapter_5/swerve_control/#control-definitions","title":"Control Definitions","text":"<ul> <li> <p>Holonomic Drive Defininition (Swerve)</p> <ul> <li>Holonomic drive systems allow robots to move in any direction without being in a specific orientation, combining forward, lateral, and rotational motion simultaneously.  </li> <li>Each wheel can rotate and steer independently</li> <li>Reference</li> </ul> </li> <li> <p>Trajectory Control</p> <ul> <li>Pre-planned or dynamically generated paths</li> <li>Automated movement along specific routes</li> <li>Used for autonomous navigation</li> </ul> </li> <li> <p>Teleop Control</p> <ul> <li>Direct driver control via joysticks</li> <li>Real-time response to input</li> <li>Used during driver-controlled periods</li> </ul> </li> </ul>"},{"location":"Learn/Chapter_5/swerve_control/#swerve-module-components","title":"Swerve Module Components","text":"<ul> <li> <p>Each swerve module consists of:</p> <ul> <li>Drive Motor: Controls wheel speed</li> <li>Steer Motor: Controls wheel direction</li> <li>Encoder: Provides position feedback</li> <li>Wheel: The contact point with the ground</li> </ul> </li> <li> <p>The drive motor, steer motor, and encoder are usually team specific and require their own unique instantiation. </p> <ul> <li>Drive Motor</li> <li>Steer Motor</li> <li>Absolute Encoder</li> </ul> </li> </ul>"},{"location":"Learn/Chapter_5/swerve_control/#key-classes-in-wpilib","title":"Key Classes in WPILIB","text":""},{"location":"Learn/Chapter_5/swerve_control/#chassisspeeds","title":"ChassisSpeeds","text":"<p>The <code>ChassisSpeeds</code> class in relation to the swerve drive represents the desired movement of the robot as a whole. It encapsulates three components: - xVelocity: Forward/backward speed (meters per second) - yVelocity: Left/right speed (meters per second) - omegaVelocity: Rotational speed (radians per second)</p> <p>This class is commonly used as an input to swerve drive kinematics, which then calculates the necessary wheel speeds and angles for each module to achieve the desired chassis motion. By adjusting these values, you can command the robot to move in any direction and rotate simultaneously.</p> <pre><code>// Stores target velocities for the entire robot\nChassisSpeeds chassisSpeeds = new ChassisSpeeds(\n    xVelocity,    // Forward/backward speed (m/s)\n    yVelocity,    // Left/right speed (m/s)\n    omegaVelocity // Rotational speed (rad/s)\n);\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#swervemodulestate","title":"SwerveModuleState","text":"<p>Represents the state of an individual swerve drive module, encapsulating both its current angle (direction) and speed (velocity). By doing so, it enables coordinated control of all modules to achieve complex robot maneuvers such as strafing, rotating, and omnidirectional movement.</p> <pre><code>// Stores angle and speed for individual modules\nSwerveModuleState moduleState = new SwerveModuleState(\n    speed,  // Speed in m/s\n    angle   // Angle as Rotation2d\n);\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#swervemodulestate_1","title":"SwerveModuleState","text":"<p>Represents the state of an individual swerve drive module, encapsulating both its current angle (direction) and speed (velocity). By doing so, it enables coordinated control of all modules to achieve complex robot maneuvers such as strafing, rotating, and omnidirectional movement.</p> <pre><code>// Stores angle and speed for individual modules\nSwerveModuleState moduleState = new SwerveModuleState(\n    speed,  // Speed in m/s\n    angle   // Angle as Rotation2d\n);\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#swervedrivekinematics-object","title":"SwerveDriveKinematics Object","text":"<p>The <code>SwerveDriveKinematics</code> class in WPILIB is responsible for converting desired robot motion (chassis speeds) into individual swerve module states (speed and angle for each wheel). It uses the physical positions of each module relative to the robot center to perform these calculations.</p> <ul> <li>How to create a kinematics object: You must specify the position of each swerve module as a <code>Translation2d</code> from the robot's center in meters.</li> </ul> <pre><code>// Example: Four-module swerve drive\nSwerveDriveKinematics kinematics = new SwerveDriveKinematics(\n    new Translation2d(0.3, 0.3),   // Front Left\n    new Translation2d(0.3, -0.3),  // Front Right\n    new Translation2d(-0.3, 0.3),  // Back Left\n    new Translation2d(-0.3, -0.3)  // Back Right\n);\n</code></pre> <ul> <li>Below is a simple example of converting desired robot motion into individual swerve module states using the <code>SwerveDriveKinematics</code> object:</li> </ul> <pre><code>// Desired robot motion (forward, sideways, rotation)\nChassisSpeeds speeds = new ChassisSpeeds(2.0, 0.0, 1.0); // 2 m/s forward, no strafe, 1 rad/s rotation\n\n// Convert to module states\nSwerveModuleState[] states = Constants.kinematics.toSwerveModuleStates(speeds);\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#joystick-input-to-motor-command","title":"Joystick Input to Motor Command","text":"<p>Joystick inputs flow down to the motors on the drivetrain using a sequence shown below. The inputs go through two \"code layers\" before ending up as a motor command.</p> <p>Command Layer Flowchart</p> <pre><code>    A[Joystick Inputs] --&gt; B[TeleopSwerveCommand.execute()]\n    B --&gt; C[Apply Red alliance flipping if applicable, scaling, deadbands]\n    C --&gt; D[ChassisSpeeds.fromFieldRelativeSpeeds()]\n    D --&gt; E[SwerveDrive.drive()]\n</code></pre> <p>Drive Subsystem Flowchart</p> <pre><code>    F[SwerveDrive.drive(chassisSpeeds)] --&gt; \n    F --&gt; G[SwerveDriveKinematics.toSwerveModuleStates()]\n    G --&gt; H[SwerveDriveKinematics.desaturateWheelSpeeds()]\n    H --&gt; I[SwerveModule.setDesiredState()]\n    I --&gt; J[SwerveModule: steerPID.calculate() &amp; driveFeedforward.calculate() &amp; drivePID.calculate()]\n    J --&gt; K[driveMotor.set() &amp; steerMotor.set()]\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#3-mathematical-foundation","title":"3. Mathematical Foundation","text":""},{"location":"Learn/Chapter_5/swerve_control/#coordinate-transformation","title":"Coordinate Transformation","text":"<p>Coordinate transformation enables your FRC robot to interpret movement commands relative to the field, regardless of its current orientation. For example, during teleop, if the driver pushes the joystick forward, the robot should always move toward the opponent\u2019s alliance wall, even if the robot is rotated or facing sideways. </p> <p>This is achieved by converting field-relative commands (from the driver\u2019s perspective) into robot-relative wheel motions using trigonometric functions. The robot uses its gyro heading and the desired field-relative velocities to calculate the correct wheel speeds and angles, ensuring consistent and intuitive control for the driver throughout the match.</p> <p>Field-Oriented Control: Convert field coordinates to robot coordinates: <pre><code>x' = x*cos(\u03b8) + y*sin(\u03b8)\ny' = y*cos(\u03b8) - x*sin(\u03b8)\n</code></pre></p> <p>Where: - <code>(x, y)</code> = Field-relative coordinates - <code>(x', y')</code> = Robot-relative coordinates - <code>\u03b8</code> = Robot orientation</p>"},{"location":"Learn/Chapter_5/swerve_control/#inverse-kinematics","title":"Inverse Kinematics","text":"<p>Swerve drive uses inverse kinematics to calculate individual module states from desired robot motion as shown below:</p> <p></p> <p>Simple Explanation</p> <p>Imagine you have two arrows: one for how you want the robot to move (forward, sideways, or diagonally), and one for how much you want it to spin. You point each arrow with your joysticks. The longer the arrow, the stronger that movement or spin. The robot adds these arrows together to figure out which way and how fast each wheel should go. If you push the spin joystick more, the robot will turn more; if you push the move joystick more, it will drive more in that direction as reflected by the vectors.</p> <p>Vector Addition Principle: Each swerve module takes advantage of simple vector addition. For each module, the final velocity vector is the sum of: 1. Translation vector (desired robot velocity) 2. Rotation vector (tangent to rotation at module position) 3. Final vector (Summation of two vectors)</p> <p>Formula: <pre><code>V_module = V_translation + \u03c9 \u00d7 r_module\n</code></pre></p> <p>Variable defining: - <code>V_module</code> = Final module velocity vector - <code>V_translation</code> = Desired translation velocity - <code>\u03c9</code> = Angular velocity - <code>r_module</code> = Position vector from robot center to module</p>"},{"location":"Learn/Chapter_5/swerve_control/#4-additional-control-concepts","title":"4. Additional Control Concepts","text":"<p>Below are some additional concepts that will help optimize your swerve drive so the modules are operating in a fashion that is kinematically feasible and taking the shortest mechanical movements to reach it's goal.</p>"},{"location":"Learn/Chapter_5/swerve_control/#speed-desaturation","title":"Speed Desaturation","text":"<p>Speed desaturation ensures that no swerve module is commanded to exceed its physical maximum speed. When combined translation and rotation commands result in wheel speeds above this limit, all module speeds are scaled down proportionally. This maintains the intended motion direction while keeping commands achievable for the hardware.</p> <p>How it works: When calculated module speeds exceed maximum velocity, all modules must be scaled proportionally. Below is the code that scales down all module speeds proportionally:</p> <pre><code>public static void desaturateWheelSpeeds(SwerveModuleState[] moduleStates, double attainableMaxSpeedMetersPerSecond) {\n    double recordedSpeed = 0;\n    for (SwerveModuleState moduleState : moduleStates) {\n        recordedSpeed = Math.max(recordedSpeed, Math.abs(moduleState.speedMetersPerSecond));\n    }\n    if (recordedSpeed &gt; attainableMaxSpeedMetersPerSecond) {\n        for (SwerveModuleState moduleState : moduleStates) {\n            moduleState.speedMetersPerSecond =\n                moduleState.speedMetersPerSecond / recordedSpeed * attainableMaxSpeedMetersPerSecond;\n        }\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#wheel-optimization","title":"Wheel Optimization","text":"<p>Wheel optimization minimizes the amount each module needs to rotate to reach its target angle. By choosing the shortest path, the module can reverse its wheel direction instead of rotating a full 180\u00b0, resulting in faster and more efficient movement.</p> <p>How it works: If the angle difference between the current and target position is greater than 90\u00b0, the module reverses its speed and adds 180\u00b0 to the target angle. This reduces unnecessary rotation.</p> <pre><code>// Calculate angle difference\ndouble angleDifference = targetAngle - currentAngle;\n\n// Normalize to [-180, 180] degrees\nwhile (angleDifference &gt; 180) angleDifference -= 360;\nwhile (angleDifference &lt; -180) angleDifference += 360;\n\n// If &gt; 90 degrees, flip direction and reverse speed\nif (Math.abs(angleDifference) &gt; 90) {\n    targetAngle += 180;\n    speed *= -1;\n    angleDifference += (angleDifference &gt; 0) ? -180 : 180;\n}\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#motor-control","title":"Motor Control","text":"<p>Motor control ensures each swerve module accurately reaches its desired speed and angle using feedback and feedforward algorithms.</p> <p>Steering Control: PID control is used to precisely position the steering motor to the target angle, correcting for any error between the current and desired position.</p> <pre><code>double output = steerPID.calculate(currentAngle, targetAngle);\nsteerMotor.setVoltage(output);\n</code></pre> <p>Drive Control: Combines feedforward (predictive) and feedback (corrective) control to achieve the desired wheel velocity. Feedforward estimates the required output, while PID corrects for any deviation.</p> <pre><code>double feedforward = driveFeedforward.calculate(targetVelocity);\ndouble feedback = drivePID.calculate(currentVelocity, targetVelocity);\ndriveMotor.setVoltage(feedforward + feedback);\n</code></pre> <p>Drive Control: Combines feedforward (predictive) and feedback (corrective) control to achieve the desired wheel velocity. Feedforward estimates the required output, while PID corrects for any deviation.</p> <pre><code>double feedforward = driveFeedforward.calculate(targetVelocity);\ndouble feedback = drivePID.calculate(currentVelocity, targetVelocity);\ndriveMotor.setVoltage(feedforward + feedback);\n</code></pre> <p>Please reference Simple Profiling for Swerve Drive for an in depth explanation on how to tune a velocity control system.</p>"},{"location":"Learn/Chapter_5/swerve_control/#5-coordinate-systems","title":"5. Coordinate Systems","text":"<p>Understanding and adhering to the WPILIB coordinate system is crucial because it ensures consistent movement commands, sensor readings, and autonomous routines across the swerve drive. Teams have frequently run into switching alliance issues/direction confusion/unreliable feild control etc. due to misunderstanding the WPILIB coordinate system.</p>"},{"location":"Learn/Chapter_5/swerve_control/#wpilib-coordinate-system-nwu","title":"WPILIB Coordinate System (NWU)","text":"<p>Field Coordinates: - X-axis: Points toward opponent alliance (positive = forward) - Y-axis: Points to the left when facing forward - Z-axis: Points up - Rotation: Counterclockwise positive, starting from +X axis</p> <p>Important Notes: - All components must use the same coordinate system - Angles typically range from -180\u00b0 to +180\u00b0 - Field-oriented driving maintains consistent directions regardless of robot orientation - All units are in Meters</p>"},{"location":"Learn/Chapter_5/swerve_control/#alliance-considerations","title":"Alliance Considerations","text":"<p>Everything is in a Blue Alliance Perspective: \ud83d\udd35 - Coordinate system origin at blue alliance corner - Positive X = toward red alliance - Positive Y = toward left side of field</p>"},{"location":"Learn/Chapter_5/swerve_control/#key-swerve-coordinate-system-adjustments-to-keep-track-of","title":"Key Swerve Coordinate System Adjustments to keep track of","text":"<p>Red Alliance Considerations: </p> <p>Actions(Driver inputs, autonomous paths) taken on the Red Alliance side must be mirrored </p> <ul> <li>Note: Do not mirror omega turns: Omega turns are specialized maneuvers and should not be mirrored, as this may lead to unintended behavior or incorrect movement patterns.</li> </ul> <p>Below is an example of an action that must be mirrored. <pre><code>// Flip coordinates for red alliance in mirrored fields\nif (DriverStation.getAlliance().get() == DriverStation.Alliance.Red) {\n    xVelocity = -xVelocity;\n    yVelocity = -yVelocity;\n}\n</code></pre></p>"},{"location":"Learn/Chapter_5/swerve_control/#kinematics-considerations","title":"Kinematics Considerations","text":"<p>When setting up your Kinematics object as shown below, please be aware that it is very easy to mess up assigning kinematics to swerve modules. Please ensure the order your modules are in and the location of the modules for your kinematics match the coordinate system pictured here.</p> <p>Teams often do a Front Left, Front Right, Back Left, Back Right setup. </p> <p></p>"},{"location":"Learn/Chapter_5/swerve_control/#steering-inversion-considerations","title":"Steering Inversion Considerations","text":"<p>Depending on your mechanical setup and wiring, the direction that increases the encoder value may not match the direction that increases the motor output. If the module turns in the opposite direction of the command, you may need to add a negative sign to the output (e.g., <code>steerMotor.setVoltage(-output)</code>). This ensures the feedback loop drives the module toward the correct angle. Always verify module movement matches software commands and adjust sign as needed for your robot.</p>"},{"location":"Learn/Chapter_5/swerve_control/#6-implementation-guide","title":"6. Implementation Guide","text":"<p>Now that you've covered what's under the hood when it comes to swerve drive. Here's a step by step implmentation guide.</p> <p>We will use the CTRE implementation of motors/sensors due to its commonality. However, this setup works the same with different vendored motors/sensors. We will cover setup with different motors/sensors more in Complete Swerve Module Class.</p>"},{"location":"Learn/Chapter_5/swerve_control/#drive-constants-setup","title":"Drive Constants Setup","text":"<p>To keep your swerve code organized and maintainable, it's best practice to centralize all drive-related constants and commonly reused objects in a dedicated file. This approach reduces the chances of duplicatate objects and makes your swerve code easier to navigate.</p> <p>Follow these steps:</p> <ol> <li>Copy the following kinematics setup into a seperate class labeled \"DriveConstants.\"</li> <li>Adjust the \"WHEELBASE\", \"TRACKWIDTH\" parameters as according to your swerve drive in \"meters.\"</li> <li>Define the swerve module order (Please reference Kinematics Considerations).</li> </ol> <p>Kinematics explaination: SwerveDriveKinematics Object. </p> <pre><code>public class DriveConstants {\n    // Robot dimensions (meters)\n    public static final double WHEELBASE = 0.6;  // Distance between front/back wheels\n    public static final double TRACKWIDTH = 0.6; // Distance between left/right wheels\n\n    // Module positions relative to robot center\n    public static final Translation2d FRONT_LEFT_LOCATION = new Translation2d(WHEELBASE / 2, TRACKWIDTH / 2);\n    public static final Translation2d FRONT_RIGHT_LOCATION = new Translation2d(WHEELBASE / 2, -TRACKWIDTH / 2);\n    public static final Translation2d BACK_LEFT_LOCATION = new Translation2d(-WHEELBASE / 2, TRACKWIDTH / 2);\n    public static final Translation2d BACK_RIGHT_LOCATION = new Translation2d(-WHEELBASE / 2, -TRACKWIDTH / 2);\n\n    public static final SwerveDriveKinematics KINEMATICS = \n        new SwerveDriveKinematics(\n            FRONT_LEFT_LOCATION,\n            FRONT_RIGHT_LOCATION,\n            BACK_LEFT_LOCATION,\n            BACK_RIGHT_LOCATION\n        );\n}\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#complete-swerve-module-class","title":"Complete Swerve Module Class","text":"<p>Implementing a swerve module centralizes all motor, encoder, and control logic in a dedicated class. This keeps your code modular and makes it easier to tune, debug, and reuse across different robots.</p> <p>Follow these steps:</p> <ol> <li>Copy the following swerve module class into your robot project.</li> <li>Adjust CAN IDs, gear ratios, and wheel dimensions for your hardware</li> <li>Setup class constructor to take in all CAN IDs needed for your swerve module (drive, steer, absolute encoder)</li> <li>Tune the PID and feedforward constants for your drivetrain. See   Simple profiling for more in depth on tuning velocity control systems.</li> <li>Ensure encoder readings and motor outputs match your mechanical setup (see Steering Inversion Considerations).</li> </ol> <p>Key features: - Encapsulates drive and steer motor control - Uses PID and feedforward for precise movement - Optimizes wheel rotation for shortest path - Converts encoder readings to physical units</p> <p>Reference: Motor Control, Steering Control, Drive Control, Wheel Optimization</p> <pre><code>public class SwerveModule {\n    private final TalonFX driveMotor;\n    private final TalonFX steerMotor;\n    private final CANcoder steerEncoder;\n\n    private final PIDController steerPID;\n    private final SimpleMotorFeedforward driveFeedforward;\n    private final PIDController drivePID;\n\n    // Constants (replace with your actual values)\n    private static final double DRIVE_GEAR_RATIO = 6.75;\n    private static final double WHEEL_CIRCUMFERENCE = 0.1016 * Math.PI; // 4\" wheel in meters\n\n    public SwerveModule(int driveID, int steerID, int encoderID) {\n        driveMotor = new TalonFX(driveID);\n        steerMotor = new TalonFX(steerID);\n        steerEncoder = new CANcoder(encoderID);\n\n        driveMotor.setNeutralMode(NeutralModeValue.Brake);\n        steerMotor.setNeutralMode(NeutralModeValue.Brake);\n\n        steerPID = new PIDController(1.0, 0.0, 0.0);\n        steerPID.enableContinuousInput(-Math.PI, Math.PI);\n\n        drivePID = new PIDController(0.1, 0.0, 0.0);\n        driveFeedforward = new SimpleMotorFeedforward(0.1, 2.0);\n    }\n\n    public void setDesiredState(SwerveModuleState desiredState) {\n        // Optimize the reference state to avoid spinning &gt; 90 degrees\n        SwerveModuleState state = SwerveModuleState.optimize(\n            desiredState, new Rotation2d(getSteerAngle()));\n\n        // Calculate steering output\n        double steerOutput = steerPID.calculate(\n            getSteerAngle(), state.angle.getRadians());\n        steerMotor.set(steerOutput / 12.0); // TalonFX expects [-1, 1] for percent output\n\n        // Calculate drive output\n        double driveFF = driveFeedforward.calculate(state.speedMetersPerSecond);\n        double driveFB = drivePID.calculate(\n            getDriveVelocity(), state.speedMetersPerSecond);\n        driveMotor.set((driveFF + driveFB) / 12.0); // Convert voltage to percent output\n    }\n\n    public SwerveModuleState getState() {\n        return new SwerveModuleState(\n            getDriveVelocity(),\n            new Rotation2d(getSteerAngle())\n        );\n    }\n\n    private double getSteerAngle() {\n        // CANcoder returns [0,1) rotations, convert to radians\n        return steerEncoder.getAbsolutePosition().getValue() * 2 * Math.PI;\n    }\n\n    private double getDriveVelocity() {\n        // TalonFX velocity is in rotations per second, convert to m/s\n        double rotationsPerSecond = driveMotor.getVelocity().getValue();\n        return rotationsPerSecond * WHEEL_CIRCUMFERENCE / DRIVE_GEAR_RATIO;\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#basic-swerve-drive-subsystem-structure","title":"Basic Swerve Drive Subsystem Structure","text":"<p>This subsystem serves as the central hub for swerve drive control. It receives commands from the drive controller, manages the coordination of all swerve modules, and ensures each module moves as needed to execute the requested driving actions. Within this subsystem, you instantiate your gyro and swerve modules, then handle all command and feedback operations for swerve movement.</p> <p>Follow these steps: 1. Copy the following swerve drive subsystem into your robot project. 2. Ensure the module array order matches your kinematics setup (see Kinematics Setup). 3. Adjust CAN IDs and constants as needed for your hardware. Ensure the parameters align with what you defined in your swerve module class.</p> <p>Topics covered: SwerveDriveKinematics Object, SwerveModuleState, ChassisSpeeds, Speed Desaturation, Wheel Optimization</p> <pre><code>public class SwerveDrive extends SubsystemBase {\n    private final SwerveModule[] modules;\n    private final SwerveDriveKinematics kinematics;\n    private final Gyro gyro;\n\n    public SwerveDrive() {\n        // Initialize modules in the same order as kinematics\n        modules = new SwerveModule[] {\n            new SwerveModule(1, 2, 11), // Front Left\n            new SwerveModule(3, 4, 12), // Front Right\n            new SwerveModule(5, 6, 13), // Back Left\n            new SwerveModule(7, 8, 14)  // Back Right\n        };\n\n        // Reference kinematics from DriveConstants\n        kinematics = DriveConstants.KINEMATICS;\n\n        // Instantiate gyro (CTRE Pigeon2 example)\n        gyro = new Pigeon2(0); // CAN ID 0, change as needed\n    }\n\n    public void drive(ChassisSpeeds chassisSpeeds) {\n        // Convert chassis speeds to module states\n        SwerveModuleState[] moduleStates =\n            kinematics.toSwerveModuleStates(chassisSpeeds);\n\n        // Desaturate wheel speeds to max allowed\n        SwerveDriveKinematics.desaturateWheelSpeeds(\n            moduleStates, DriveConstants.MAX_SPEED_MPS);\n\n        // Command each module to its desired state\n        for (int i = 0; i &lt; modules.length; i++) {\n            modules[i].setDesiredState(moduleStates[i]);\n        }\n    }\n\n\n    public SwerveModuleState[] getModuleStates() {\n        return Arrays.stream(modules)\n            .map(SwerveModule::getState)\n            .toArray(SwerveModuleState[]::new);\n    }\n\n    public Rotation2d getRotation2d() {\n        // Get robot heading from gyro\n        return gyro.getRotation2d();\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#teleop-command-implementation","title":"Teleop Command Implementation","text":"<p>This command handles real-time driver input and translates joystick values into swerve drive commands. It applies alliance flipping, deadbands, and scaling, then generates field-relative chassis speeds for the drive subsystem.</p> <p>Follow these steps: 1. Copy the following teleop command class into your robot project. 2. Connect joystick axes to the suppliers (<code>xSupplier</code>, <code>ySupplier</code>, <code>rotSupplier</code>). 3. Ensure alliance flipping logic matches your team's requirements. 4. Adjust deadband and scaling constants as needed.</p> <p>Topics covered: Joystick Input to Motor Command, ChassisSpeeds, Alliance Considerations</p> <pre><code>public class TeleopSwerveCommand extends CommandBase {\n    private final SwerveDrive swerve;\n    private final Supplier&lt;Double&gt; xSupplier;\n    private final Supplier&lt;Double&gt; ySupplier;\n    private final Supplier&lt;Double&gt; rotSupplier;\n\n    public TeleopSwerveCommand(\n        SwerveDrive swerve,\n        Supplier&lt;Double&gt; xSupplier,\n        Supplier&lt;Double&gt; ySupplier,\n        Supplier&lt;Double&gt; rotSupplier\n    ) {\n        this.swerve = swerve;\n        this.xSupplier = xSupplier;\n        this.ySupplier = ySupplier;\n        this.rotSupplier = rotSupplier;\n        addRequirements(swerve);\n    }\n\n    @Override\n    public void execute() {\n        // Get joystick inputs via suppliers\n        double xSpeed = -ySupplier.get();  // Forward/backward\n        double ySpeed = -xSupplier.get();  // Left/right\n        double rot = -rotSupplier.get();   // Rotation\n\n        // Alliance flip for red side\n        if (AllianceFlipUtil.shouldFlipToRed()) {\n            xSpeed = -xSpeed;\n            ySpeed = -ySpeed;\n        }\n\n        // Apply deadband and scale to max speeds\n        xSpeed = Math.abs(xSpeed) &gt; XboxInterfaceConstants.kDeadband\n            ? xSpeed * DriveConstants.MAX_SPEED_MPS\n            : 0.0;\n        ySpeed = Math.abs(ySpeed) &gt; XboxInterfaceConstants.kDeadband\n            ? ySpeed * DriveConstants.MAX_SPEED_MPS\n            : 0.0;\n        rot = Math.abs(rot) &gt; XboxInterfaceConstants.kDeadband\n            ? rot * DriveConstants.MAX_ANGULAR_SPEED_RAD_PER_SEC\n            : 0.0;\n\n        // Create chassis speeds (field-oriented)\n        ChassisSpeeds chassisSpeeds = ChassisSpeeds.fromFieldRelativeSpeeds(\n            xSpeed, ySpeed, rot, swerve.getRotation2d()\n        );\n\n        // Drive the robot\n        swerve.drive(chassisSpeeds);\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_5/swerve_control/#integrating-swerve-drive-in-robotcontainer","title":"Integrating Swerve Drive in RobotContainer","text":"<p>To connect your swerve drive subsystem and teleop command to the overall robot structure: 1. Instantiate both objects in your <code>RobotContainer</code> class.  2. Your <code>TelopSwerveCommand</code> should be instantiated as a <code>default command</code> so that it is running continously.</p> <p>Example RobotContainer Setup:</p> <pre><code>public class RobotContainer {\n    // Subsystems\n    private final SwerveDrive swerveDrive = new SwerveDrive();\n\n    // Joystick\n    private final CommandXboxController driverController = new CommandXboxController(0);\n\n    // Teleop command\n    private final TeleopSwerveCommand teleopSwerveCommand = new TeleopSwerveCommand(\n        swerveDrive,\n        () -&gt; driverController.getLeftX(),\n        () -&gt; driverController.getLeftY(),\n        () -&gt; driverController.getRightX()\n    );\n\n    public RobotContainer() {\n        configureBindings();\n    }\n\n    private void configureBindings() {\n        // Set default command for swerve drive\n        swerveDrive.setDefaultCommand(teleopSwerveCommand);\n    }\n}\n</code></pre> <p>Key Steps: - Instantiate the <code>SwerveDrive</code> subsystem. - Create the teleop command, passing joystick axis suppliers. - Set the teleop command as the default for the swerve subsystem. - Place all subsystem and command instantiations in <code>RobotContainer</code> for centralized management.</p> <p>This structure ensures your swerve drive responds to joystick input during teleop and is ready for further autonomous command integration.</p>"},{"location":"Learn/Chapter_5/swerve_control/#7-troubleshooting","title":"7. Troubleshooting","text":""},{"location":"Learn/Chapter_5/swerve_control/#common-issues-and-possible-fixes","title":"Common Issues and Possible fixes","text":"<p>Motors not reciving command input from controller - Trace back your logic and add printlns at each step to see where your code logic stops working</p> <p>Module Not Reaching Target Angle - Check encoder calibration and offset values - Verify PID tuning parameters - Ensure continuous input is enabled for angle PID</p> <p>Robot Drifting During Movement - Calibrate gyroscope properly - Check for mechanical issues (loose wheels, etc.) - Verify coordinate system consistency</p> <p>Jerky or Unstable Movement - Reduce PID gains, especially derivative - Add input filtering/deadbands - Check for sensor noise</p> <p>Modules Fighting Each Other - Ensure all modules use same coordinate system - Verify kinematics setup matches physical layout - Check for inverted motors or encoders</p>"},{"location":"Learn/Chapter_5/swerve_control/#debug-tools","title":"Debug Tools","text":"<p>Module State Monitoring: <pre><code>public void updateTelemetry() {\n    for (int i = 0; i &lt; modules.length; i++) {\n        SwerveModuleState state = modules[i].getState();\n        SmartDashboard.putNumber(\"Module \" + i + \" Speed\", state.speedMetersPerSecond);\n        SmartDashboard.putNumber(\"Module \" + i + \" Angle\", state.angle.getDegrees());\n    }\n}\n</code></pre></p> <p>Field-Relative Position Tracking: <pre><code>private final SwerveDriveOdometry odometry;\n\npublic void updateOdometry() {\n    odometry.update(gyro.getRotation2d(), getModuleStates());\n    SmartDashboard.putString(\"Robot Pose\", odometry.getPoseMeters().toString());\n}\n</code></pre></p>"},{"location":"Learn/Chapter_5/swerve_control/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with basic teleop control before adding autonomous features</li> <li>Test Incrementally: Test each module individually before running full swerve</li> <li>Monitor Performance: Always log module states and robot pose for debugging</li> <li>Consistent Units: Use meters and radians throughout for consistency with WPILIB</li> </ol>"},{"location":"Learn/Chapter_5/swerve_control/#9-reference-resources","title":"9. Reference Resources","text":"<ul> <li>WPILIB Swerve Drive Documentation</li> <li>Yagsl Library</li> <li>PhantomCatz RobotCode2025 Swerve Implementation (GitHub)</li> </ul> <p>This guide is based on Team 2637's swerve drive training presentation and includes additional implementation details for new programmers.</p>"},{"location":"Learn/Chapter_6/auto_strategy/","title":"FRC Autonomous Programming - Strategy and Planning","text":""},{"location":"Learn/Chapter_6/auto_strategy/#overview","title":"Overview","text":"<p>This guide teaches you the strategic foundation and planning principles for designing high-performance autonomous routines for FRC robots, covering game analysis, alliance coordination, and decision-making frameworks.</p> <p>Note: This guide utilizes many Command-Based Programming concepts. For a solid foundation, please review the Command-Based Theory training before proceeding.</p> <p>What you'll build: A strategic framework for autonomous planning that maximizes competitive advantage and adapts to different match scenarios.</p>"},{"location":"Learn/Chapter_6/auto_strategy/#1-why-autonomous-matters","title":"1. Why Autonomous Matters?","text":"<p>For competitive FRC teams, autonomous mode is often the difference between winning and losing matches.</p> <p>Simple Drive Straight Acroos the starting line? Most seasons, a game will award you points for simply crossing a starting line. Many teams often forget about these easy to earn points can make or break matches. Below is an example of a simple drive foward auto.</p> <p><pre><code>// Basic mobility - just drive forward\npublic Command getMobilityAuto() {\n    return new DriveForward(driveSubsystem, 3.0); // Drive 3 meters\n}\n</code></pre> Static Autonomous? Extra points are often available for scoring in autonomous. Below is an example of an autonomous that takes advantage of the extra points by adding onto their simple drive foward command.</p> <pre><code>// Static preload scoring\npublic Command getPreloadAuto() {\n    return new SequentialCommandGroup(\n        new ScorePreload(superstructure),\n        new DriveForward(driveSubsystem, 3.0)\n    );\n}\n</code></pre> <p>However, as teams are looking to increase their leverage in alliance meetings and matchups, static autonomous routines can't adapt to alliance partners, field conditions, or strategic needs that change throughout a competition.</p> <p>Strategic Autonomous Autonomous routines designed around game analysis, alliance coordination, and robust execution provide:</p> <ul> <li>Maximum point potential: Score multiple game pieces efficiently whilist not overbearing on robot capability.</li> <li>Strategic positioning: End in advantageous locations for teleop. (ie. getting into the closest position to the loading station)</li> <li>Alliance coordination: Complement rather than conflict with partners (ie. Having multiple autos that can start from different positions or take different routes)</li> <li>Tiebreaker advantage: Auto points often determine close matches</li> <li>Consistent execution: Work reliably across different field conditions.</li> </ul> <p>Real Benefits Teams who are able to checkmark the above in autonomous can have: - Significant competitive advantage in qualification matches - Higher alliance selection value during top alliance scouting meetings - Better playoff performance through coordination with alliance partners</p> <p>Note: Alliance Partner Roles in FRC: - Alliance Captain: Highest-seeded team; leads alliance strategy and usually runs the most complex/high-scoring autonomous routine. - First Pick: Chosen first by the captain; expected to execute a strong, complementary autonomous routine that aligns with the captain\u2019s plan. - Second Pick: Chosen second; often focuses on reliability, easy points (like mobility), or supporting roles to avoid conflicts and assist primary scorers.</p> <p>Tip: Plan autonomous routines with your likely alliance role in mind. Captains and first picks coordinate for maximum scoring, while second picks prioritize reliability and support. Having flexible routines for each role increases your team\u2019s strategic value.</p>"},{"location":"Learn/Chapter_6/auto_strategy/#2-autos-strategic-foundation","title":"2. Autos Strategic Foundation","text":""},{"location":"Learn/Chapter_6/auto_strategy/#game-analysis-what-should-we-accomplish","title":"Game Analysis - \"What should we accomplish?\"","text":"<p>Before writing any code, analyze what autonomous can achieve and work with the strategy subunit of your team to go over the following points:</p> <p>Key Questions: - What bonus points are available only during autonomous? - What role can our team's robot realistically play in playoff matches and qualificaitons? - How do autonomous points compare to teleop scoring rates? - What field positioning advantages exist?</p>"},{"location":"Learn/Chapter_6/auto_strategy/#alliance-considerations-who-are-we-playing-with","title":"Alliance Considerations - \"Who are we playing with?\"","text":"<p>Autonomous routines play different roles during Qualification and Playoff matches. Here are some considerations to make about both types of matches when writing your routines.</p> <p>Qualification Matches: - Always assume most alliance partners will provide minimal coordination during alliance meetings - Design routines that maximize your individual contribution - Avoid areas where robot conflicts are likely</p> <p>Playoff Matches: - Playing with skilled, coordinated alliance partners - Design complementary routines (who does what?) - Enable multiple strategic options</p> <p>Example Analysis (2023 Charged Up): <pre><code>Mobility: 3 points (auto only)\nGame pieces: 3-6 points depending on level  \nBalance: 8-12 points depending on robot count\n</code></pre> Alliance Captain + 1<sup>st</sup> Pick autos revolved around using their mobility to manuver around the \"Charge Station\" and score pieces (2-3 typically) that were not in a straight line. They aimed for scoring cone game pieces at the highest rung before moving down to gain the max potential auto points.</p> <p>Second pick autos focused on reliably balancing on the Charge Station for extra points, ensuring compatibility with alliance partners even if their maneuverability was limited.</p> <p>For official rules and scoring details, see the 2023 FRC Game Manual.</p> <p></p> <p>Example Analysis (2024 Crescendo): <pre><code>Mobility: 2 points (auto only)  \nAmp Note: 2 points (auto only, scored in Amp)  \nSpeaker Note: 5 points (auto only, scored in Speaker)  \n</code></pre></p> <p>For official rules and scoring details, see the 2024 FRC Game Manual.</p> <p>Alliance Captain + 1<sup>st</sup> Pick autos typically focused on collecting and scoring multiple Notes in the Speaker, often starting with a preload and then picking up additional gamepieces at the center of the feild that could potentially be stolen by the other alliance. Advanced routines included coordinated paths to avoid collisions and maximize scoring.</p> <p>Second pick autos prioritized reliable mobility and collecting game pieces that were closer to them. These routines emphasized consistency and compatibility, ensuring they did not interfere with primary scorers that mostly focused on the stealable outside gamepieces.</p> <p></p>"},{"location":"Learn/Chapter_6/auto_strategy/#strategic-planning-template-planning-guide","title":"Strategic Planning Template Planning Guide","text":""},{"location":"Learn/Chapter_6/auto_strategy/#phase-1-game-analysis","title":"Phase 1: Game Analysis","text":"<ol> <li>Rule Study - Analyze current game manual for autonomous opportunities</li> <li>Point Calculation - Determine theoretical maximum autonomous scores</li> <li>Timing Analysis - Break down mechanism and movement times</li> <li>Competition Research - Study what top teams accomplished in autonomous</li> </ol>"},{"location":"Learn/Chapter_6/auto_strategy/#phase-2-strategic-planning","title":"Phase 2: Strategic Planning","text":"<ol> <li>Option Development - Create 3-5 different autonomous routine concepts</li> <li>Alliance Scenarios - Plan for different partnership situations</li> <li>Risk Assessment - Evaluate reliability vs. scoring potential</li> <li>Prioritization - Rank routines by strategic value</li> </ol> <p>Success Criteria: - Clear understanding of autonomous scoring opportunities - Documented timing analysis for all major actions - Multiple autonomous options for different scenarios - Strategic reasoning for each autonomous choice</p>"},{"location":"Learn/Chapter_6/auto_strategy/#where-to-go-next","title":"Where to Go Next","text":"<p>Ready for implementation? Continue with: - FRC Autonomous Programming - Implementation \u2013 Technical implementation of your strategic plans</p>"},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/","title":"extrainfoPuttingelsewhere","text":""},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/#8-advanced-techniques","title":"8. Advanced Techniques","text":""},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/#dynamic-path-planning","title":"Dynamic Path Planning","text":"<pre><code>// Adapt paths based on field conditions\npublic Command getAdaptiveAuto() {\n    return new ConditionalCommand(\n        getAggressiveThreePiece(),\n        getConservativeTwoPiece(),\n        () -&gt; alliancePartnersHaveReliableAuto()\n    );\n}\n</code></pre>"},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/#vision-assisted-scoring","title":"Vision-Assisted Scoring","text":"<pre><code>public class VisionAssistedScore extends CommandBase {\n    @Override\n    public void execute() {\n        Optional&lt;PhotonTrackedTarget&gt; target = getTarget();\n\n        if (target.isPresent()) {\n            // Use vision for final alignment\n            Transform3d robotToTarget = target.get().getBestCameraToTarget();\n            ChassisSpeeds correction = calculateCorrectionSpeeds(robotToTarget);\n            driveSubsystem.drive(correction);\n        }\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/#failure-recovery","title":"Failure Recovery","text":"<pre><code>// Handle missed game pieces gracefully\npublic class RobustIntakeCommand extends CommandBase {\n    private final Timer timeoutTimer = new Timer();\n\n    @Override\n    public void initialize() {\n        timeoutTimer.restart();\n    }\n\n    @Override\n    public boolean isFinished() {\n        return gamePieceAcquired() || timeoutTimer.hasElapsed(2.0);\n    }\n\n    @Override\n    public void end(boolean interrupted) {\n        if (!gamePieceAcquired()) {\n            // Switch to backup autonomous routine\n            CommandScheduler.getInstance().schedule(getBackupAuto());\n        }\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/#9-practice-project","title":"9. Practice Project","text":"<p>Build this step-by-step:</p>"},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/#phase-1-foundation-week-1-2","title":"Phase 1: Foundation (Week 1-2)","text":"<ol> <li>Swerve Drive Setup - Get basic driving working with odometry</li> <li>Path Following - Implement and tune PID controllers for trajectory following  </li> <li>Vision Pipeline - Set up AprilTag detection and pose estimation</li> </ol>"},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/#phase-2-mechanisms-week-3-4","title":"Phase 2: Mechanisms (Week 3-4)","text":"<ol> <li>State Machine - Implement superstructure state management</li> <li>Mechanism Control - Tune all mechanism PID controllers</li> <li>Sensor Integration - Add game piece detection, limit switches</li> </ol>"},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/#phase-3-integration-week-5-6","title":"Phase 3: Integration (Week 5-6)","text":"<ol> <li>Simple Autonomous - One game piece + mobility</li> <li>Complex Routines - Multi-piece autonomous with path planning</li> <li>Alliance Coordination - Multiple autonomous options</li> </ol>"},{"location":"Learn/Chapter_6/extrainfoPuttingelsewhere/#phase-4-refinement-week-7-8","title":"Phase 4: Refinement (Week 7-8)","text":"<ol> <li>Competition Testing - Test on official field elements</li> <li>Failure Handling - Add robust error recovery</li> <li>Performance Optimization - Fine-tune for consistency</li> </ol> <p>Success criteria: - Consistently scores planned number of game pieces - Reliable execution across different field conditions - Smooth, professional-looking movement - Provides strategic advantage in matches</p> <p>Feedforward Tuning (kS, kV, kA): <pre><code>// 1. Find static friction (kS)\n// Increase voltage until mechanism just starts moving\ndouble kS = findMinimumVoltageToMove();\n\n// 2. Find velocity feedforward (kV)  \n// Run at constant velocity, measure voltage needed\ndouble kV = testVoltage / testVelocity;\n\n// 3. Tune visually - setpoint vs actual velocity should be parallel\n</code></pre></p> <p>PID Tuning: <pre><code>// 4. Add proportional gain until oscillation\n// 5. Back off by 25-50%  \n// 6. Add derivative to reduce overshoot\n// 7. Add integral only if steady-state error exists\n</code></pre></p> <p>Camera Calibration: <pre><code>// Lens calibration first - use checkerboard patterns\npublic void calibrateLens() {\n    // Follow PhotonVision calibration process\n    // Bad lens calibration makes position calibration impossible\n}\n\n// Position calibration second\npublic void calibratePosition() {\n    // Start with measured physical offsets\n    // Test close and far from AprilTags\n    // Verify detected position matches actual position\n}\n</code></pre></p> <p>Sensor Fusion Tuning: <pre><code>// Balance accuracy vs stability\npublic void tuneSensorFusion() {\n    // Increase vision trust until jitter becomes unacceptable\n    // Decrease until path following performance degrades\n    // Find optimal balance point\n}\n</code></pre> <pre><code>// Start with aggressive P terms\nxController.setP(2.0);\nyController.setP(2.0);\nrotationController.setP(1.0);\n\n// Increase until jitter occurs during path following\n// Back off to stable values\n// Tune BEFORE adding vision - vision can introduce oscillations\n</code></pre></p>"},{"location":"Learn/Chapter_6/writing_autos/","title":"FRC Autonomous Programming - Technical Implementation","text":""},{"location":"Learn/Chapter_6/writing_autos/#overview","title":"Overview","text":"<p>This guide covers the technical implementation of autonomous routines for FRC robots, including control systems, tuning processes, state machines, and testing methodologies.</p> <p>Note: This guide assumes you have completed the strategic planning process covered in FRC Autonomous Programming - Strategy and Planning.</p> <p>What you'll build: A complete technical autonomous system with precise control, robust state management, and reliable execution.</p>"},{"location":"Learn/Chapter_6/writing_autos/#1-architecture-and-hardware-setup","title":"1. Architecture and Hardware Setup","text":""},{"location":"Learn/Chapter_6/writing_autos/#robot-designconstruction-for-autonomous-success","title":"Robot Design/Construction for Autonomous Success","text":"<p>Certain design/fabrication choices significantly impact autonomous capability. Teams have different philosophies concerning whether certain issues or capabilities fall under a software, electrical or mechanical jurisdiction. That being said, not everything can be \"fixed in software\" and it's up to the software developer at hand to make that determination.</p> <p>Below are general guidelines to help you ensure robust autonomous performance on everything \"non-software\" related. Ensure you work with your mechanical, electrical and CAD subunit of your team to work out the following:</p> <p>Enabling Design Features:</p> <p>Design mechanisms to support key autonomous tasks. For example, in 2025's \"Reefscape,\" teams chose between ground or station intakes. Ground intakes improved autonomous performance but added design complexity. Teams had to choose which was more important to reaching their competition goals.</p> <pre><code>// Tank intake example - allows pickup while driving\npublic class IntakeSubsystem extends SubsystemBase {\n    public void deployAndRun() {\n        deploy();\n        startRollers();\n        // Robot can drive while this runs\n    }\n}\n</code></pre> <p>Vision-Friendly Camera Placement:</p> <p>Vision supplemented pose estimation will ensure accurate robot movements, especially as teams look to score more game pieces with faster drivebases. Ensure:</p> <ul> <li>Clear sight lines to commonly used AprilTags (ie. At scoring and loading zones)</li> <li>Place cameras at different corners or heights on the robot to maximize AprilTag visibility and minimize occlusion from mechanisms or game pieces.</li> <li>Protected from game piece interference and damage</li> <li>Multiple cameras for more points of triangulation to estimate your robot's position.</li> </ul> <p>Fast, Consistent Mechanisms: - Minimize/optimize areas of handoff and degrees of freedom - Repeatable positioning without mechanism backlash and robust sensor feedback. - Extending mechanisms past the frame increases torque and risk of damage. Ensure mechanisms (elevators, pivot arms, etc.) are well-supported for accuracy and durability.</p> <p>Reliable Wiring for Autonomous Consistency:</p> <p>Autonomous performance also depends on robust electrical connections. Poor wiring can cause intermittent failures, sensor dropouts, or unexpected resets than can make troubleshooting electrical vs programming errors difficult.</p> <p>Best Practices: - Securing all connectors with cable ties or heat shrink to prevent vibration-induced disconnects. - Label all connectors for quick troubleshooting. - Perform a \"wiggle test\"\u2014gently move wires while the robot is powered to check for intermittent faults.</p> <p>Example: CAN Bus Reliability Checklist: - Terminate CAN bus at both ends with 120\u03a9 resistors. - Keep CAN wiring as short and direct as possible. - Use twisted pair wire for CAN to reduce noise. - Monitor CAN utilization and error counts in telemetry.</p>"},{"location":"Learn/Chapter_6/writing_autos/#2-control-system-implementation","title":"2. Control System Implementation","text":"<p>Effective autonomous routines depend on a well-tuned software foundation, with the \"first layer\" starting with reliable motor control. </p> <p>Case in point, many teams have struggled with trajectory control for swerve because they only focused on tuning their pathplanning tool's pid rather than building their control system from the ground up ensuring the motors themselves were tuned properly.</p> <p>Here are the layers that build up the foundation of a well constructed autonomous control system.</p>"},{"location":"Learn/Chapter_6/writing_autos/#layer-1-motor-control","title":"Layer 1: Motor Control","text":"<p>The foundation of autonomous control starts with precise motor control. For a deeper dive into PID control fundamentals, see Basic PID Control. However, here are some motor configurations to take note of that teams have used to ensure consistent auto performance.</p> <p>TalonFX (Phoenix 6) Voltage Compensation: Ensure consistent motor output regardless of battery voltage. <pre><code>// Set voltage compensation saturation to 12V and enable it\ntalonFX.getConfigurator().apply(new VoltageCompensationConfigs().withSaturation(12.0));\ntalonFX.setControl(new VoltageOut(0)); // Use voltage control mode if needed\n</code></pre></p> <p>Motion Profiling Setup: For more on motion profiling, see Simple Motion Profiling. <pre><code>// Configure slot gains for velocity/position control\nSlot0Configs slot0 = new Slot0Configs();\nslot0.kP = 0.1;\nslot0.kI = 0.001;\nslot0.kD = 0.01;\nslot0.kV = 0.05; // Feedforward for velocity\ntalonFX.getConfigurator().apply(slot0);\n\n// Motion Magic (Motion Profile) configuration\nMotionMagicConfigs mm = new MotionMagicConfigs();\nmm.MotionMagicCruiseVelocity = 15000; // sensor units per second\nmm.MotionMagicAcceleration = 6000;    // sensor units per second^2\ntalonFX.getConfigurator().apply(mm);\n</code></pre></p> <p>Current Limiting: Protect against brownouts and ensure consistent acceleration. <pre><code>CurrentLimitsConfigs currentLimits = new CurrentLimitsConfigs();\ncurrentLimits.SupplyCurrentLimitEnable = true;\ncurrentLimits.SupplyCurrentLimit = 35.0;\ncurrentLimits.SupplyCurrentThreshold = 40.0;\ncurrentLimits.SupplyTimeThreshold = 0.5;\ntalonFX.getConfigurator().apply(currentLimits);\n</code></pre></p> <p>Neutral Deadband Adjustment: Reduce deadband for more precise low-speed control. <pre><code>talonFX.getConfigurator().apply(new MotorOutputConfigs().withNeutralDeadband(0.01));\n</code></pre></p> <p>Brake/Coast Mode Selection: Use brake mode for precise stopping in autonomous. <pre><code>talonFX.setNeutralMode(NeutralModeValue.Brake);\n</code></pre></p> <p>Open Loop Ramp Rate: Smooth acceleration to prevent wheel slip. <pre><code>talonFX.getConfigurator().apply(new OpenLoopRampsConfigs().withOpenLoopRampPeriod(0.2));\n</code></pre></p> <p>Closed Loop Ramp Rate: Prevents sudden jumps in velocity setpoints. <pre><code>talonFX.getConfigurator().apply(new ClosedLoopRampsConfigs().withClosedLoopRampPeriod(0.1));\n</code></pre></p>"},{"location":"Learn/Chapter_6/writing_autos/#layer-2-localization-and-sensors","title":"Layer 2: Localization and Sensors","text":""},{"location":"Learn/Chapter_6/writing_autos/#wheel-odometry-calibration","title":"Wheel Odometry Calibration","text":"<p>Wheel odometry calibration ensures your robot's position estimates from encoders are accurate. It corrects for errors like wheel size differences, wheelbase mismeasurements, and encoder inaccuracies. Proper calibration reduces drift, improves path following, and should be repeated after wheel changes or significant use. Below is a simple calculation that determines the \"odometry wheel radius\" for driving.</p> <pre><code>// Critical: Determine actual wheel radius on carpet\npublic void calibrateWheelRadius() {\n    // Drive known distance at low acceleration\n    double commandedDistance = 3.0; // meters\n    double actualDistance = measureActualDistance();\n    double reportedDistance = getOdometryDistance();\n\n    double correctionFactor = actualDistance / reportedDistance;\n    wheelRadius *= correctionFactor;\n}\n</code></pre> <p>Vision System Integration:</p> <p>Wheel odometry alone can drift over time due to wheel slip, uneven surfaces, or mechanical inaccuracies, leading to increasing errors in the robot's estimated position. </p> <p>Integrating vision allows the robot to periodically correct its pose estimate using absolute field references. This fusion of sensor data combines the short-term accuracy and responsiveness of wheel odometry with the long-term reliability of vision-based localization, resulting in more precise autonomous movement, better path following, and improved scoring consistency, especially in complex or multi-piece autonomous routines.</p> <pre><code>public class VisionSubsystem extends SubsystemBase {\n    private final PhotonCamera camera;\n    private final PhotonPoseEstimator poseEstimator;\n\n    @Override\n    public void periodic() {\n        PhotonPipelineResult result = camera.getLatestResult();\n\n        if (result.hasTargets()) {\n            Optional&lt;EstimatedRobotPose&gt; pose = poseEstimator.update(result);\n            if (pose.isPresent()) {\n                // Add vision measurement to pose estimator\n                driveSubsystem.addVisionMeasurement(\n                    pose.get().estimatedPose.toPose2d(),\n                    pose.get().timestampSeconds\n                );\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_6/writing_autos/#layer-3-path-following","title":"Layer 3: Path Following","text":"<pre><code>public class AutoDriveCommand extends CommandBase {\n    private final DriveSubsystem driveSubsystem;\n    private final PIDController xController;\n    private final PIDController yController; \n    private final PIDController rotationController;\n    private final Trajectory trajectory;\n\n    public AutoDriveCommand(DriveSubsystem drive, Trajectory trajectory) {\n        this.driveSubsystem = drive;\n        this.trajectory = trajectory;\n\n        // Tuned PID controllers for path following\n        xController = new PIDController(1.0, 0.0, 0.0);\n        yController = new PIDController(1.0, 0.0, 0.0);\n        rotationController = new PIDController(0.5, 0.0, 0.0);\n\n        addRequirements(driveSubsystem);\n    }\n\n    @Override\n    public void execute() {\n        Trajectory.State goal = trajectory.sample(timer.get());\n        Pose2d currentPose = driveSubsystem.getPose();\n\n        // Calculate velocity commands\n        double xVel = xController.calculate(currentPose.getX(), goal.poseMeters.getX());\n        double yVel = yController.calculate(currentPose.getY(), goal.poseMeters.getY());\n        double rotVel = rotationController.calculate(\n            currentPose.getRotation().getRadians(), \n            goal.poseMeters.getRotation().getRadians()\n        );\n\n        driveSubsystem.drive(new ChassisSpeeds(xVel, yVel, rotVel));\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_6/writing_autos/#3-systematic-tuning-process","title":"3. Systematic Tuning Process","text":"<p>Tune each layer sequentially to ensure a solid foundation\u2014if earlier layers are not properly tuned, later layers will struggle to compensate, leading to unreliable autonomous performance.</p>"},{"location":"Learn/Chapter_6/writing_autos/#step-1-motor-control-tuning","title":"Step 1: Motor Control Tuning","text":"<p>Using visual feedback for optimal performance:</p>"},{"location":"Learn/Chapter_6/writing_autos/#step-2-localization-tuning","title":"Step 2: Localization Tuning","text":""},{"location":"Learn/Chapter_6/writing_autos/#step-3-path-following-tuning","title":"Step 3: Path Following Tuning","text":""},{"location":"Learn/Chapter_6/writing_autos/#4-state-machine-implementation","title":"4. State Machine Implementation","text":""},{"location":"Learn/Chapter_6/writing_autos/#robust-robot-state-management","title":"Robust Robot State Management","text":"<pre><code>public enum SuperstructureState {\n    IDLE,\n    INTAKE_DEPLOY,\n    INTAKE_RUNNING,\n    PREPARE_SCORE,\n    SCORING,\n    STOWED\n}\n\npublic class SuperstructureSubsystem extends SubsystemBase {\n    private SuperstructureState currentState = SuperstructureState.IDLE;\n    private SuperstructureState nextState = SuperstructureState.IDLE;\n\n    @Override\n    public void periodic() {\n        switch (currentState) {\n            case IDLE:\n                // Safe positions, ready for commands\n                setElevatorPosition(0);\n                setArmPosition(STOWED_ANGLE);\n\n                if (requestIntake) {\n                    nextState = SuperstructureState.INTAKE_DEPLOY;\n                }\n                break;\n\n            case INTAKE_DEPLOY:\n                // Deploy intake mechanism\n                setIntakePosition(DEPLOYED);\n\n                if (intakeDeployed()) {\n                    nextState = SuperstructureState.INTAKE_RUNNING;\n                }\n                break;\n\n            case INTAKE_RUNNING:\n                // Run intake rollers, wait for game piece\n                runIntakeRollers();\n\n                if (gamePieceDetected()) {\n                    nextState = SuperstructureState.PREPARE_SCORE;\n                } else if (!requestIntake) {\n                    nextState = SuperstructureState.IDLE;\n                }\n                break;\n\n            case PREPARE_SCORE:\n                // Move to scoring position\n                setElevatorPosition(scoreHeight);\n                setArmPosition(SCORE_ANGLE);\n\n                if (readyToScore()) {\n                    nextState = SuperstructureState.SCORING;\n                }\n                break;\n\n            case SCORING:\n                // Eject game piece\n                ejectGamePiece();\n\n                if (scoreComplete()) {\n                    nextState = SuperstructureState.STOWED;\n                }\n                break;\n        }\n\n        currentState = nextState;\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_6/writing_autos/#5-autonomous-routine-scripting","title":"5. Autonomous Routine Scripting","text":""},{"location":"Learn/Chapter_6/writing_autos/#command-based-autonomous-structure-inline-factory-implementation","title":"Command-Based Autonomous Structure (Inline Factory Implementation)","text":"<p>While there are many different command based formats for constructing autonomous commands based on each team's individual robot code structure preferences, this is the simplest implementation.</p> <p>Store these autonomous command and others like it within your \"RobotContainer\" Class. This makes it easy for you to pass in the subsystems as parameters needed to run the autonomous program. <pre><code>// Inline SequentialCommandGroup factory for Three Piece Auto\npublic Command getThreePieceAuto(\n    DriveSubsystem drive, \n    SuperstructureSubsystem superstructure,\n    IntakeSubsystem intake\n) {\n    return new SequentialCommandGroup(\n        // Score preloaded game piece\n        new ParallelCommandGroup(\n            new ScoreHigh(superstructure),\n            new PrepareForIntake(intake)\n        ),\n\n        // Drive to first game piece while deploying intake\n        new ParallelDeadlineGroup(\n            new FollowTrajectory(drive, \"ToFirstPiece\"),\n            new IntakeGamePiece(intake)\n        ),\n\n        // Return and score second piece  \n        new ParallelCommandGroup(\n            new FollowTrajectory(drive, \"FirstPieceReturn\"),\n            new PrepareToScore(superstructure)\n        ),\n        new ScoreMid(superstructure),\n\n        // Get third piece\n        new ParallelDeadlineGroup(\n            new FollowTrajectory(drive, \"ToSecondPiece\"), \n            new IntakeGamePiece(intake)\n        ),\n\n        // Final score and positioning\n        new ParallelCommandGroup(\n            new FollowTrajectory(drive, \"FinalScore\"),\n            new PrepareToScore(superstructure)\n        ),\n        new ScoreHigh(superstructure),\n\n        // End in good position for teleop\n        new FollowTrajectory(drive, \"FinalPosition\")\n    );\n}\n</code></pre></p>"},{"location":"Learn/Chapter_6/writing_autos/#robot-container-integration","title":"Robot Container Integration","text":"<pre><code>// In your RobotContainer class\npublic class RobotContainer {\n    private final DriveSubsystem drive = new DriveSubsystem();\n    private final SuperstructureSubsystem superstructure = new SuperstructureSubsystem();\n    private final IntakeSubsystem intake = new IntakeSubsystem();\n\n    // Inline SequentialCommandGroup factory for PathPlanner-based auto\n    public Command getPathPlannerAuto() {\n        return new SequentialCommandGroup(\n            new InstantCommand(() -&gt; drive.resetOdometry()),\n            new ParallelCommandGroup(\n                new StartIntake(intake),\n                new PrepareScore(superstructure)\n            ),\n            // Follow the generated path using PathPlanner\n            AutoBuilder.followPath(\n                PathPlannerPath.fromPathFile(\"ThreePiecePath\")\n            ),\n            new StopIntake(intake),\n            new ScoreHigh(superstructure)\n        );\n    }\n\n    // Auto Factories -&gt; See above for more info\n}\n</code></pre>"},{"location":"Learn/Chapter_6/writing_autos/#6-testing-and-validation","title":"6. Testing and Validation","text":""},{"location":"Learn/Chapter_6/writing_autos/#layer-by-layer-testing","title":"Layer-by-Layer Testing","text":"<p>Test each system independently before integration:</p> <p>Motor Control Validation: <pre><code>// Verify setpoint tracking\npublic void testMotorControl() {\n    // Plot commanded vs actual position/velocity\n    // Check for voltage compensation effectiveness\n    // Validate across different battery levels\n}\n</code></pre></p> <p>Localization Testing: <pre><code>// Odometry accuracy test\npublic void testOdometry() {\n    // Drive known distance, measure error\n    // Should be &lt;2% error over 5+ meters\n}\n\n// Vision system test  \npublic void testVision() {\n    // Verify pose updates at various distances\n    // Check for consistency across lighting conditions\n}\n</code></pre></p> <p>Integration Testing: <pre><code>// Full autonomous testing\npublic void testAutonomous() {\n    // Run complete routines repeatedly\n    // Test with different starting positions\n    // Validate timing assumptions\n    // Test failure recovery (missed game pieces, etc.)\n}\n</code></pre></p>"},{"location":"Learn/Chapter_6/writing_autos/#field-testing-protocol","title":"Field Testing Protocol","text":"<p>When possible, test on competition-like conditions:</p> <pre><code>// Competition simulation\npublic class AutonomousTest extends CommandBase {\n    private final List&lt;Command&gt; testRoutines;\n    private int currentTest = 0;\n\n    @Override\n    public void execute() {\n        if (currentTest &lt; testRoutines.size()) {\n            Command routine = testRoutines.get(currentTest);\n            routine.schedule();\n\n            // Log performance metrics\n            logTestResults(routine, currentTest);\n            currentTest++;\n        }\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_6/writing_autos/#7-essential-debugging-tips","title":"7. Essential Debugging Tips","text":""},{"location":"Learn/Chapter_6/writing_autos/#comprehensive-telemetry","title":"Comprehensive Telemetry","text":"<pre><code>@Override\npublic void periodic() {\n    // Pose and localization\n    SmartDashboard.putString(\"Robot Pose\", getPose().toString());\n    SmartDashboard.putNumber(\"Vision Targets\", getVisibleTargets());\n\n    // Path following\n    SmartDashboard.putNumber(\"Path Error X\", getPathErrorX());\n    SmartDashboard.putNumber(\"Path Error Y\", getPathErrorY()); \n    SmartDashboard.putNumber(\"Path Error Rot\", getPathErrorRotation());\n\n    // State machine\n    SmartDashboard.putString(\"Superstructure State\", getCurrentState().toString());\n    SmartDashboard.putBoolean(\"At Target\", atTarget());\n\n    // Performance monitoring\n    SmartDashboard.putNumber(\"Loop Time\", getLoopTime());\n    SmartDashboard.putNumber(\"Battery Voltage\", getBatteryVoltage());\n}\n</code></pre>"},{"location":"Learn/Chapter_6/writing_autos/#using-advantagekit-for-analysis","title":"Using AdvantageKit for Analysis","text":""},{"location":"Learn/Chapter_6/writing_autos/#import-orglittletonroboticsjunctionlogger-override-public-void-periodic-loggerrecordoutputdrivepose-getpose-loggerrecordoutputdrivevelocity-getchassisspeeds-loggerrecordoutputdrivepathtarget-getpathtarget-loggerrecordoutputvisiontargets-getvisiontargets-loggerrecordoutputautostate-getcurrentstatename","title":"<pre><code>import org.littletonrobotics.junction.Logger;\n\n@Override  \npublic void periodic() {\n    Logger.recordOutput(\"Drive/Pose\", getPose());\n    Logger.recordOutput(\"Drive/Velocity\", getChassisSpeeds());\n    Logger.recordOutput(\"Drive/PathTarget\", getPathTarget());\n    Logger.recordOutput(\"Vision/Targets\", getVisionTargets());\n    Logger.recordOutput(\"Auto/State\", getCurrentState().name());\n}\n</code></pre>","text":""},{"location":"Learn/Chapter_6/writing_autos/#where-to-go-next","title":"Where to Go Next","text":"<p>Linked below are extra resources and disscussions for autonomous performance</p> <p>Competition Strategies</p> <p>TODO: The links in this section are not working properly and need to be redone.</p>"},{"location":"Learn/Chapter_8/apriltag_processing/","title":"AprilTag Processing: Getting Started","text":""},{"location":"Learn/Chapter_8/apriltag_processing/#overview","title":"Overview","text":"<p>AprilTags are like QR codes for robots - fiducial markers that provide precise position and orientation data. Unlike basic vision targets, AprilTags tell your robot exactly where it is on the field and can enable centimeter-accurate autonomous navigation and positioning.</p> <p>What you'll build: A robot that uses AprilTags for precise field localization and automated alignment.</p> <p>Next steps: Multi-tag pose estimation, camera calibration, and integration with advanced odometry using PhotonVision and WPILib Pose Estimation.</p>"},{"location":"Learn/Chapter_8/apriltag_processing/#1-what-are-apriltags","title":"1. What Are AprilTags?","text":""},{"location":"Learn/Chapter_8/apriltag_processing/#the-smart-fiducial-marker","title":"The Smart Fiducial Marker","text":"<p>AprilTag vs Regular Vision Target: <pre><code>// Regular target: Only tells you angle/distance\nVisionTarget reflectiveTape = findGreenRectangle();\ndouble angle = calculateAngle(reflectiveTape);\n\n// AprilTag: Tells you exact position and orientation\nAprilTagDetection tag = detector.detect(image);\nPose3d robotPose = tag.getPose();  // Full 6DOF position!\n</code></pre></p> <p>Key Features: - Unique IDs - Each tag has a distinct pattern (587 possible in 36h11 family) - 6DOF Pose - X, Y, Z position + roll, pitch, yaw rotation - Robust Detection - Works with partial occlusion and poor lighting - No Calibration Required - For basic centering applications</p>"},{"location":"Learn/Chapter_8/apriltag_processing/#frc-2023-implementation","title":"FRC 2023+ Implementation","text":"<p>Field Layout: - Multiple tags placed at known positions throughout the field - Each tag is 6.5\" \u00d7 6.5\" with unique ID - Tags mounted flat on field elements - Standard 36h11 family used</p> <p>Applications: - Auto-alignment - Drive until tag is centered - Pose estimation - Know exact robot position - Autonomous navigation - Navigate using field landmarks - Backup odometry - Correct wheel/gyro drift</p>"},{"location":"Learn/Chapter_8/apriltag_processing/#2-how-apriltag-detection-works","title":"2. How AprilTag Detection Works","text":""},{"location":"Learn/Chapter_8/apriltag_processing/#the-8-step-detection-pipeline","title":"The 8-Step Detection Pipeline","text":"<p>Step 1: Grayscale Conversion <pre><code>Color Image \u2192 Grayscale Image\n(Remove color, keep brightness only)\n</code></pre></p> <p>Step 2: Image Downsampling <pre><code>High Resolution \u2192 Lower Resolution\n(Faster processing, less precision)\n</code></pre></p> <p>Step 3: Adaptive Thresholding <pre><code>Grayscale \u2192 Binary (Black/White)\n(Each pixel classified as light/dark/unsure)\n</code></pre></p> <p>Step 4: Connected Components <pre><code>Binary Image \u2192 Pixel Clusters\n(Group connected pixels, discard small clusters)\n</code></pre></p> <p>Step 5: Quadrilateral Fitting <pre><code>Pixel Clusters \u2192 4-Sided Shapes\n(Find best-fit rectangles for each cluster)\n</code></pre></p> <p>Step 6: Tag Candidate Selection <pre><code>All Quadrilaterals \u2192 Potential Tags\n(Look for nested quad pattern: outer border + inner grid)\n</code></pre></p> <p>Step 7: Bit Pattern Decoding <pre><code>Tag Candidate \u2192 Tag ID\n(Read 6\u00d76 grid pattern, match to known tag)\n</code></pre></p> <p>Step 8: Sub-pixel Refinement <pre><code>Low-res Detection \u2192 High-precision Corners\n(Re-analyze at full resolution for accuracy)\n</code></pre></p>"},{"location":"Learn/Chapter_8/apriltag_processing/#simplified-pipeline-visualization","title":"Simplified Pipeline Visualization","text":"<pre><code>Camera Image\n     \u2193\n[Grayscale + Downsample]\n     \u2193\n[Find Black/White Regions]\n     \u2193\n[Fit Rectangles to Regions]\n     \u2193\n[Decode Interior Pattern]\n     \u2193\n[Refine Corner Positions]\n     \u2193\nAprilTag Detection (ID + Pose)\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#3-basic-implementation","title":"3. Basic Implementation","text":""},{"location":"Learn/Chapter_8/apriltag_processing/#step-1-simple-tag-detection","title":"Step 1: Simple Tag Detection","text":"<pre><code>public class AprilTagVision extends SubsystemBase {\n    private final UsbCamera camera;\n    private final AprilTagDetector detector;\n    private final CvSink cvSink;\n    private final Mat frame = new Mat();\n\n    private AprilTagDetection[] lastDetections = new AprilTagDetection[0];\n\n    public AprilTagVision() {\n        // Camera setup\n        camera = CameraServer.startAutomaticCapture();\n        camera.setResolution(640, 480);\n        camera.setFPS(30);\n\n        cvSink = CameraServer.getVideo();\n\n        // AprilTag detector setup\n        detector = new AprilTagDetector();\n        detector.addFamily(AprilTagFamily.TAG_36h11); // FRC standard\n\n        // Configure detection parameters\n        AprilTagDetector.Config config = detector.getConfig();\n        config.decimationFactor = 2;  // Downsample by factor of 2\n        config.numThreads = 2;        // Use 2 CPU cores\n        detector.setConfig(config);\n    }\n\n    @Override\n    public void periodic() {\n        // Grab frame\n        if (cvSink.grabFrame(frame) == 0) {\n            return; // No new frame\n        }\n\n        // Detect AprilTags\n        lastDetections = detector.detect(frame);\n\n        // Send results to dashboard\n        SmartDashboard.putNumber(\"Tags Detected\", lastDetections.length);\n\n        for (int i = 0; i &lt; lastDetections.length; i++) {\n            AprilTagDetection detection = lastDetections[i];\n            SmartDashboard.putNumber(\"Tag \" + i + \" ID\", detection.getId());\n            SmartDashboard.putNumber(\"Tag \" + i + \" Center X\", detection.getCenterX());\n            SmartDashboard.putNumber(\"Tag \" + i + \" Center Y\", detection.getCenterY());\n        }\n    }\n\n    public AprilTagDetection[] getDetections() {\n        return lastDetections;\n    }\n\n    public boolean hasTag(int targetID) {\n        for (AprilTagDetection detection : lastDetections) {\n            if (detection.getId() == targetID) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    public AprilTagDetection getTag(int targetID) {\n        for (AprilTagDetection detection : lastDetections) {\n            if (detection.getId() == targetID) {\n                return detection;\n            }\n        }\n        return null;\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#step-2-auto-alignment-simple","title":"Step 2: Auto-Alignment (Simple)","text":"<pre><code>public class AlignToTagCommand extends CommandBase {\n    private final SwerveDrive drive;\n    private final AprilTagVision vision;\n    private final int targetTagID;\n    private final PIDController rotationController;\n\n    public AlignToTagCommand(SwerveDrive drive, AprilTagVision vision, int tagID) {\n        this.drive = drive;\n        this.vision = vision;\n        this.targetTagID = tagID;\n        this.rotationController = new PIDController(0.1, 0.0, 0.01);\n\n        addRequirements(drive);\n    }\n\n    @Override\n    public void execute() {\n        AprilTagDetection tag = vision.getTag(targetTagID);\n\n        if (tag != null) {\n            // Calculate error from center of image\n            double imageWidth = 640;\n            double centerX = imageWidth / 2.0;\n            double pixelError = tag.getCenterX() - centerX;\n\n            // Convert to angular error (rough approximation)\n            double angularError = pixelError / centerX; // Normalized error\n\n            // Use PID to control rotation\n            double rotationSpeed = rotationController.calculate(0, angularError);\n\n            // Drive with rotation correction\n            ChassisSpeeds speeds = new ChassisSpeeds(0, 0, rotationSpeed);\n            drive.drive(speeds);\n        } else {\n            // No tag visible, stop\n            drive.drive(new ChassisSpeeds());\n        }\n    }\n\n    @Override\n    public boolean isFinished() {\n        AprilTagDetection tag = vision.getTag(targetTagID);\n        if (tag == null) return false;\n\n        double imageWidth = 640;\n        double centerX = imageWidth / 2.0;\n        double pixelError = Math.abs(tag.getCenterX() - centerX);\n\n        return pixelError &lt; 10; // Within 10 pixels of center\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#4-advanced-pose-estimation","title":"4. Advanced Pose Estimation","text":""},{"location":"Learn/Chapter_8/apriltag_processing/#camera-calibration-and-3d-pose","title":"Camera Calibration and 3D Pose","text":"<pre><code>public class AdvancedAprilTagVision extends SubsystemBase {\n    private final AprilTagDetector detector;\n    private final AprilTagFieldLayout fieldLayout;\n    private final PhotonPoseEstimator poseEstimator;\n\n    // Camera calibration parameters (get these from camera calibration)\n    private final Matrix&lt;N3, N3&gt; cameraMatrix = MatBuilder.fill(Nat.N3(), Nat.N3(),\n        500.0, 0.0, 320.0,     // fx, 0, cx\n        0.0, 500.0, 240.0,     // 0, fy, cy  \n        0.0, 0.0, 1.0          // 0, 0, 1\n    );\n\n    private final Matrix&lt;N5, N1&gt; distortionCoefficients = MatBuilder.fill(Nat.N5(), Nat.N1(),\n        0.1, -0.2, 0.0, 0.0, 0.0  // k1, k2, p1, p2, k3\n    );\n\n    public AdvancedAprilTagVision() {\n        detector = new AprilTagDetector();\n        detector.addFamily(AprilTagFamily.TAG_36h11);\n\n        // Load field layout (tag positions)\n        try {\n            fieldLayout = AprilTagFieldLayout.loadFromResource(\n                AprilTagFields.k2024Crescendo.m_resourceFile\n            );\n        } catch (IOException e) {\n            throw new RuntimeException(\"Failed to load AprilTag field layout\", e);\n        }\n\n        // Create pose estimator\n        Transform3d robotToCamera = new Transform3d(\n            new Translation3d(0.0, 0.0, 0.5), // Camera 0.5m above robot center\n            new Rotation3d(0, 0, 0)            // Camera facing forward\n        );\n\n        poseEstimator = new PhotonPoseEstimator(\n            fieldLayout,\n            PoseStrategy.MULTI_TAG_PNP_ON_COPROCESSOR,\n            robotToCamera\n        );\n    }\n\n    public Optional&lt;EstimatedRobotPose&gt; getEstimatedGlobalPose() {\n        // Convert AprilTag detections to PhotonVision format\n        List&lt;PhotonTrackedTarget&gt; targets = new ArrayList&lt;&gt;();\n\n        for (AprilTagDetection detection : getDetections()) {\n            // Calculate 3D pose of tag relative to camera\n            Pose3d tagPose = calculateTagPose(detection);\n\n            PhotonTrackedTarget target = new PhotonTrackedTarget(\n                Math.atan2(detection.getCenterY() - 240, 500), // yaw\n                Math.atan2(detection.getCenterX() - 320, 500), // pitch  \n                0.0,                                           // area (not used)\n                0.0,                                           // skew (not used)\n                detection.getId(),                             // fiducial ID\n                tagPose,                                       // best camera to target\n                tagPose,                                       // alternate camera to target\n                0.0,                                           // pose ambiguity\n                List.of(),                                     // corners (empty for now)\n                List.of()                                      // detected corners (empty)\n            );\n\n            targets.add(target);\n        }\n\n        // Create PhotonPipelineResult\n        PhotonPipelineResult result = new PhotonPipelineResult(\n            Timer.getFPGATimestamp(),\n            targets\n        );\n\n        return poseEstimator.update(result);\n    }\n\n    private Pose3d calculateTagPose(AprilTagDetection detection) {\n        // Use PnP (Perspective-n-Point) algorithm to get 3D pose\n        // This requires camera calibration for accuracy\n\n        // Tag corners in 3D (6.5\" square tag)\n        double tagSize = 0.1651; // 6.5 inches in meters\n        double half = tagSize / 2.0;\n\n        List&lt;Translation3d&gt; objectPoints = List.of(\n            new Translation3d(-half, -half, 0),\n            new Translation3d(half, -half, 0),\n            new Translation3d(half, half, 0),\n            new Translation3d(-half, half, 0)\n        );\n\n        // Detected corners in image\n        CornerArray corners = detection.getCorners();\n        List&lt;Translation2d&gt; imagePoints = List.of(\n            new Translation2d(corners.get(0).x, corners.get(0).y),\n            new Translation2d(corners.get(1).x, corners.get(1).y),\n            new Translation2d(corners.get(2).x, corners.get(2).y),\n            new Translation2d(corners.get(3).x, corners.get(3).y)\n        );\n\n        // Solve PnP to get pose (simplified - real implementation more complex)\n        return OpenCVHelp.solvePNP_SQUARE(\n            cameraMatrix, distortionCoefficients, \n            objectPoints, imagePoints\n        );\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#integration-with-odometry","title":"Integration with Odometry","text":"<pre><code>public class DriveSubsystem extends SubsystemBase {\n    private final SwerveDriveOdometry odometry;\n    private final SwerveDrivePoseEstimator poseEstimator;\n    private final AdvancedAprilTagVision vision;\n\n    public DriveSubsystem(AdvancedAprilTagVision vision) {\n        this.vision = vision;\n\n        // Create pose estimator that fuses wheel odometry with vision\n        poseEstimator = new SwerveDrivePoseEstimator(\n            kinematics,\n            getGyroRotation(),\n            getModulePositions(),\n            new Pose2d(),\n            VecBuilder.fill(0.1, 0.1, 0.1),    // Odometry standard deviations\n            VecBuilder.fill(0.5, 0.5, 0.5)     // Vision standard deviations\n        );\n    }\n\n    @Override\n    public void periodic() {\n        // Update odometry with wheel and gyro data\n        poseEstimator.update(getGyroRotation(), getModulePositions());\n\n        // Add vision measurements\n        Optional&lt;EstimatedRobotPose&gt; visionPose = vision.getEstimatedGlobalPose();\n        if (visionPose.isPresent()) {\n            EstimatedRobotPose estimate = visionPose.get();\n            poseEstimator.addVisionMeasurement(\n                estimate.estimatedPose.toPose2d(),\n                estimate.timestampSeconds\n            );\n        }\n\n        // Send pose to dashboard\n        Pose2d currentPose = poseEstimator.getEstimatedPosition();\n        SmartDashboard.putNumber(\"Robot X\", currentPose.getX());\n        SmartDashboard.putNumber(\"Robot Y\", currentPose.getY());\n        SmartDashboard.putNumber(\"Robot Rotation\", currentPose.getRotation().getDegrees());\n    }\n\n    public Pose2d getPose() {\n        return poseEstimator.getEstimatedPosition();\n    }\n\n    public void resetPose(Pose2d pose) {\n        poseEstimator.resetPosition(getGyroRotation(), getModulePositions(), pose);\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#5-real-world-applications","title":"5. Real-World Applications","text":""},{"location":"Learn/Chapter_8/apriltag_processing/#auto-scoring-alignment","title":"Auto-Scoring Alignment","text":"<pre><code>public class AutoScoreCommand extends CommandBase {\n    private final SwerveDrive drive;\n    private final AprilTagVision vision;\n    private final int scoringTagID;\n    private final double targetDistance; // meters\n\n    private final PIDController xController = new PIDController(1.0, 0.0, 0.1);\n    private final PIDController yController = new PIDController(1.0, 0.0, 0.1);\n    private final PIDController rotController = new PIDController(2.0, 0.0, 0.2);\n\n    public AutoScoreCommand(SwerveDrive drive, AprilTagVision vision, int tagID) {\n        this.drive = drive;\n        this.vision = vision;\n        this.scoringTagID = tagID;\n        this.targetDistance = 1.0; // 1 meter from tag\n\n        addRequirements(drive);\n    }\n\n    @Override\n    public void execute() {\n        Optional&lt;EstimatedRobotPose&gt; visionPose = vision.getEstimatedGlobalPose();\n\n        if (visionPose.isEmpty()) {\n            drive.drive(new ChassisSpeeds());\n            return;\n        }\n\n        // Get current robot pose\n        Pose2d robotPose = visionPose.get().estimatedPose.toPose2d();\n\n        // Get target tag pose from field layout\n        Optional&lt;Pose3d&gt; tagPose = vision.getFieldLayout().getTagPose(scoringTagID);\n        if (tagPose.isEmpty()) return;\n\n        // Calculate desired robot position (in front of tag)\n        Pose2d tagPose2d = tagPose.get().toPose2d();\n        Translation2d offsetFromTag = new Translation2d(-targetDistance, 0)\n            .rotateBy(tagPose2d.getRotation());\n        Pose2d targetRobotPose = new Pose2d(\n            tagPose2d.getTranslation().plus(offsetFromTag),\n            tagPose2d.getRotation().rotateBy(Rotation2d.fromDegrees(180))\n        );\n\n        // Calculate control outputs\n        double xSpeed = xController.calculate(\n            robotPose.getX(), \n            targetRobotPose.getX()\n        );\n        double ySpeed = yController.calculate(\n            robotPose.getY(), \n            targetRobotPose.getY()\n        );\n        double rotSpeed = rotController.calculate(\n            robotPose.getRotation().getRadians(),\n            targetRobotPose.getRotation().getRadians()\n        );\n\n        // Drive robot\n        ChassisSpeeds speeds = new ChassisSpeeds(xSpeed, ySpeed, rotSpeed);\n        drive.drive(speeds);\n    }\n\n    @Override\n    public boolean isFinished() {\n        return xController.atSetpoint() &amp;&amp; \n               yController.atSetpoint() &amp;&amp; \n               rotController.atSetpoint();\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#multi-tag-localization","title":"Multi-Tag Localization","text":"<pre><code>public class MultiTagLocalization {\n    public static Optional&lt;Pose2d&gt; estimatePoseFromMultipleTags(\n            AprilTagDetection[] detections, \n            AprilTagFieldLayout fieldLayout) {\n\n        if (detections.length &lt; 2) {\n            return Optional.empty(); // Need at least 2 tags for good estimate\n        }\n\n        List&lt;Pose2d&gt; poseEstimates = new ArrayList&lt;&gt;();\n\n        for (AprilTagDetection detection : detections) {\n            Optional&lt;Pose3d&gt; tagFieldPose = fieldLayout.getTagPose(detection.getId());\n            if (tagFieldPose.isEmpty()) continue;\n\n            // Calculate robot pose from this tag\n            Pose3d cameraToTag = calculateTagPose(detection);\n            Pose3d robotToCamera = new Pose3d(); // Camera at robot center for simplicity\n\n            Pose3d fieldToTag = tagFieldPose.get();\n            Pose3d fieldToCamera = fieldToTag.transformBy(cameraToTag.inverse());\n            Pose3d fieldToRobot = fieldToCamera.transformBy(robotToCamera.inverse());\n\n            poseEstimates.add(fieldToRobot.toPose2d());\n        }\n\n        if (poseEstimates.isEmpty()) {\n            return Optional.empty();\n        }\n\n        // Average all pose estimates (simple fusion)\n        double avgX = poseEstimates.stream().mapToDouble(p -&gt; p.getX()).average().orElse(0);\n        double avgY = poseEstimates.stream().mapToDouble(p -&gt; p.getY()).average().orElse(0);\n\n        // Circular mean for rotation\n        double sumSin = poseEstimates.stream().mapToDouble(p -&gt; p.getRotation().getSin()).sum();\n        double sumCos = poseEstimates.stream().mapToDouble(p -&gt; p.getRotation().getCos()).sum();\n        Rotation2d avgRotation = new Rotation2d(sumCos, sumSin);\n\n        return Optional.of(new Pose2d(avgX, avgY, avgRotation));\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#6-performance-optimization","title":"6. Performance Optimization","text":""},{"location":"Learn/Chapter_8/apriltag_processing/#detection-parameters","title":"Detection Parameters","text":"<pre><code>public void configureForPerformance() {\n    AprilTagDetector.Config config = detector.getConfig();\n\n    // Fast detection (lower accuracy)\n    config.decimationFactor = 4;    // Downsample more aggressively\n    config.sigma = 0.8;             // More blur (faster but less precise)\n    config.numThreads = 4;          // Use more CPU cores\n    config.refineEdges = false;     // Skip sub-pixel refinement\n\n    detector.setConfig(config);\n}\n\npublic void configureForAccuracy() {\n    AprilTagDetector.Config config = detector.getConfig();\n\n    // Accurate detection (slower)\n    config.decimationFactor = 1;    // No downsampling\n    config.sigma = 0.0;             // No blur\n    config.numThreads = 2;          // Fewer threads to avoid conflicts\n    config.refineEdges = true;      // Enable sub-pixel refinement\n\n    detector.setConfig(config);\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#camera-settings-for-apriltags","title":"Camera Settings for AprilTags","text":"<pre><code>public void optimizeCameraForTags() {\n    // Lower resolution for speed (tags don't need high resolution)\n    camera.setResolution(320, 240);\n    camera.setFPS(30);\n\n    // Fixed exposure prevents auto-exposure lag\n    camera.setExposure(50);  // Adjust based on field lighting\n\n    // Higher contrast helps with black/white detection\n    camera.setBrightness(30);\n    camera.setContrast(80);\n\n    // Disable auto white balance for consistent colors\n    camera.setWhiteBalance(3000); // Typical field lighting temperature\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#7-debugging-and-troubleshooting","title":"7. Debugging and Troubleshooting","text":""},{"location":"Learn/Chapter_8/apriltag_processing/#essential-telemetry","title":"Essential Telemetry","text":"<pre><code>@Override\npublic void periodic() {\n    AprilTagDetection[] detections = getDetections();\n\n    // Detection info\n    SmartDashboard.putNumber(\"AprilTags Detected\", detections.length);\n    SmartDashboard.putNumber(\"Processing Time (ms)\", processingTimeMs);\n\n    for (int i = 0; i &lt; Math.min(detections.length, 3); i++) {\n        AprilTagDetection detection = detections[i];\n        String prefix = \"Tag\" + i + \"_\";\n\n        SmartDashboard.putNumber(prefix + \"ID\", detection.getId());\n        SmartDashboard.putNumber(prefix + \"CenterX\", detection.getCenterX());\n        SmartDashboard.putNumber(prefix + \"CenterY\", detection.getCenterY());\n        SmartDashboard.putNumber(prefix + \"Decision_Margin\", detection.getDecisionMargin());\n        SmartDashboard.putNumber(prefix + \"Hamming_Distance\", detection.getHammingDistance());\n    }\n\n    // Pose estimation info\n    Optional&lt;EstimatedRobotPose&gt; pose = getEstimatedGlobalPose();\n    if (pose.isPresent()) {\n        Pose2d robotPose = pose.get().estimatedPose.toPose2d();\n        SmartDashboard.putNumber(\"Vision_X\", robotPose.getX());\n        SmartDashboard.putNumber(\"Vision_Y\", robotPose.getY());\n        SmartDashboard.putNumber(\"Vision_Rotation\", robotPose.getRotation().getDegrees());\n        SmartDashboard.putNumber(\"Vision_Timestamp\", pose.get().timestampSeconds);\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"<p>Problem: No tags detected - Solution: Check lighting, camera exposure, tag visibility and distance</p> <p>Problem: Inconsistent detection - Solution: Improve lighting consistency, check for tag damage/flatness</p> <p>Problem: Poor pose accuracy - Solution: Calibrate camera properly, use multiple tags, check tag mounting</p> <p>Problem: High CPU usage - Solution: Increase decimation factor, reduce threads, lower camera resolution</p> <p>Problem: Pose estimates jumping around - Solution: Filter poses, reject outliers, tune pose estimator standard deviations</p>"},{"location":"Learn/Chapter_8/apriltag_processing/#detection-quality-metrics","title":"Detection Quality Metrics","text":"<pre><code>public boolean isGoodDetection(AprilTagDetection detection) {\n    // Decision margin - how confident the detector is\n    // Higher is better (typically &gt; 100 is good)\n    if (detection.getDecisionMargin() &lt; 80) {\n        return false;\n    }\n\n    // Hamming distance - number of bit errors corrected\n    // Lower is better (0-2 is good, &gt;3 is suspicious)\n    if (detection.getHammingDistance() &gt; 2) {\n        return false;\n    }\n\n    // Tag size in image - too small = too far away\n    CornerArray corners = detection.getCorners();\n    double area = calculatePolygonArea(corners);\n    if (area &lt; 100) { // Minimum pixel area\n        return false;\n    }\n\n    return true;\n}\n</code></pre>"},{"location":"Learn/Chapter_8/apriltag_processing/#8-practice-project","title":"8. Practice Project","text":"<p>Build this step-by-step:</p> <ol> <li>Basic Detection - Detect and display AprilTag IDs</li> <li>Simple Alignment - Center robot on specific tag</li> <li>Distance Control - Stop at specific distance from tag</li> <li>Pose Estimation - Calculate robot field position</li> <li>Multi-Tag Fusion - Use multiple tags for better accuracy</li> <li>Autonomous Navigation - Navigate to scoring positions</li> </ol> <p>Success Criteria: - Detect tags consistently at 3+ meters distance - Align to tags within 2 degrees accuracy - Pose estimation within 10cm of actual position - Smooth autonomous navigation to targets</p>"},{"location":"Learn/Chapter_8/apriltag_processing/#where-to-go-next","title":"Where to Go Next","text":"<p>Ready for production AprilTag systems? Explore these:</p> <p>\ud83c\udfaf Professional Solutions - PhotonVision - Complete AprilTag processing pipeline - LimelightLib - Hardware-accelerated tag detection - WPILib Pose Estimation - Sensor fusion</p> <p>\ud83d\udd27 Advanced Techniques - Camera Calibration - Precise 3D pose estimation - Multi-Camera Systems - 360\u00b0 field coverage - Outlier Rejection - Robust pose filtering</p> <p>\ud83d\udcca Competition Integration - PathPlanner Integration - Vision-corrected autonomous - Real-Time Localization - Live pose tracking - Auto-Scoring Systems - Competition-ready alignment</p> <p>\u26a1 Performance Optimization - FPGA Processing - Ultra-low latency detection - Edge Computing - High-performance coprocessors</p> <p>\ud83d\ude80 Ready to give your robot perfect vision? Start with basic tag detection, master pose estimation, then integrate with professional vision systems for championship-level autonomous performance!</p>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/","title":"Kalman Filters &amp; Pose Estimation: Getting Started","text":""},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#overview","title":"Overview","text":"<p>Kalman filters are \"smart\" sensors that combine multiple imperfect measurements to create a better estimate than any single sensor alone. Think of them as a mathematical way to blend wheel odometry, gyroscopes, and vision data to know exactly where your robot is on the field - even when individual sensors drift or have noise.</p> <p>What you'll build: A robot that maintains accurate position tracking by fusing encoder, gyro, and vision measurements.</p> <p>Next steps: Advanced sensor fusion, custom state-space models, and competition-ready localization using WPILib's Advanced Controls.</p>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#1-why-kalman-filters","title":"1. Why Kalman Filters?","text":""},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#the-sensor-fusion-problem","title":"The Sensor Fusion Problem","text":"<p>Individual sensors have problems: <pre><code>// Wheel odometry: Accurate short-term, drifts over time\nPose2d wheelPose = odometry.getPoseMeters(); // Drifts due to wheel slip\n\n// Vision: Accurate but noisy and intermittent  \nPose2d visionPose = camera.getRobotPose(); // Jumpy, sometimes wrong\n\n// Gyro: Good for rotation, but can drift\nRotation2d heading = gyro.getRotation2d(); // Slowly drifts over time\n</code></pre></p> <p>Kalman filter solution: <pre><code>// Fuses all sensors for best estimate\nposeEstimator.update(gyro.getRotation2d(), leftDistance, rightDistance);\nposeEstimator.addVisionMeasurement(visionPose, timestamp);\nPose2d bestEstimate = poseEstimator.getEstimatedPosition(); // Smooth and accurate!\n</code></pre></p> <p>Real Benefits: - Smooth tracking - No jumpy position updates - Drift correction - Vision fixes wheel/gyro drift - Noise rejection - Filters out sensor noise automatically - Confidence weighting - Trusts reliable sensors more - Latency compensation - Handles delayed vision measurements</p>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#2-how-kalman-filters-work","title":"2. How Kalman Filters Work","text":""},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#the-two-step-dance","title":"The Two-Step Dance","text":"<p>Step 1: Predict (What we think happened) <pre><code>Previous State + Model + Time = Predicted State\n</code></pre></p> <p>Step 2: Correct (Update with measurements) <pre><code>Predicted State + New Measurement + Confidence = Updated State\n</code></pre></p>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#simple-example-robot-position","title":"Simple Example: Robot Position","text":"<pre><code>// Step 1: Predict where robot moved based on wheels\npredictedX = lastX + (leftWheel + rightWheel) / 2 * cos(heading);\npredictedY = lastY + (leftWheel + rightWheel) / 2 * sin(heading);\n\n// Step 2: Correct with vision (if available)\nif (hasVisionMeasurement) {\n    // Blend prediction with vision based on confidence\n    finalX = blendWithConfidence(predictedX, visionX, confidence);\n    finalY = blendWithConfidence(predictedY, visionY, confidence);\n}\n</code></pre>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#key-concepts","title":"Key Concepts","text":"<p>State - What we're tracking (robot X, Y, rotation) Model - How we predict state changes (wheel movement) Measurement - What sensors tell us (vision pose) Uncertainty - How much we trust each source Covariance - Mathematical measure of uncertainty</p>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#3-basic-kalman-filter-implementation","title":"3. Basic Kalman Filter Implementation","text":""},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#simple-flywheel-example","title":"Simple Flywheel Example","text":"<pre><code>public class SmartFlywheelSubsystem extends SubsystemBase {\n    private final CANSparkMax motor;\n    private final RelativeEncoder encoder;\n\n    // The system model (how flywheel behaves)\n    private final LinearSystem&lt;N1, N1, N1&gt; flywheelPlant;\n\n    // The Kalman filter\n    private final KalmanFilter&lt;N1, N1, N1&gt; kalmanFilter;\n\n    public SmartFlywheelSubsystem() {\n        motor = new CANSparkMax(1, MotorType.kBrushless);\n        encoder = motor.getEncoder();\n\n        // Create system model from characterization\n        double kV = 0.023; // Volts per (rad/s)  \n        double kA = 0.001; // Volts per (rad/s\u00b2)\n        flywheelPlant = LinearSystemId.identifyVelocitySystem(kV, kA);\n\n        // Create Kalman filter\n        kalmanFilter = new KalmanFilter&lt;&gt;(\n            Nat.N1(), Nat.N1(),\n            flywheelPlant,\n            VecBuilder.fill(3.0),    // Model uncertainty (trust model 3 rad/s)\n            VecBuilder.fill(0.01),   // Measurement uncertainty (trust encoder 0.01 rad/s)\n            0.020                    // Loop time (20ms)\n        );\n    }\n\n    public void setVoltage(double volts) {\n        // Update the filter with our control input\n        kalmanFilter.predict(VecBuilder.fill(volts), 0.020);\n        motor.setVoltage(volts);\n    }\n\n    @Override\n    public void periodic() {\n        // Update filter with encoder measurement\n        double encoderVelocity = encoder.getVelocity() * 2 * Math.PI / 60; // Convert RPM to rad/s\n        kalmanFilter.correct(VecBuilder.fill(volts), VecBuilder.fill(encoderVelocity));\n\n        // Get the filtered estimate\n        double filteredVelocity = kalmanFilter.getXhat(0);\n        SmartDashboard.putNumber(\"Raw Encoder\", encoderVelocity);\n        SmartDashboard.putNumber(\"Filtered Velocity\", filteredVelocity);\n    }\n\n    public double getFilteredVelocity() {\n        return kalmanFilter.getXhat(0);\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#understanding-the-parameters","title":"Understanding the Parameters","text":"<pre><code>// Model uncertainty: How much we trust our physics model\nVecBuilder.fill(3.0)   // Larger = trust model less, trust measurements more\n\n// Measurement uncertainty: How much we trust our sensors  \nVecBuilder.fill(0.01)  // Larger = trust measurements less, trust model more\n</code></pre> <p>Tuning Rules: - Noisy encoder? Increase measurement uncertainty (0.01 \u2192 0.1) - Model inaccurate? Increase model uncertainty (3.0 \u2192 10.0) - Want faster response? Decrease measurement uncertainty - Want smoother output? Increase measurement uncertainty</p>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#4-wpilib-pose-estimators","title":"4. WPILib Pose Estimators","text":""},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#swerve-drive-pose-estimator","title":"Swerve Drive Pose Estimator","text":"<pre><code>public class SwerveDriveSubsystem extends SubsystemBase {\n    private final SwerveDriveKinematics kinematics;\n    private final Gyro gyro;\n    private final SwerveModule[] modules;\n\n    // The pose estimator (Kalman filter for robot position)\n    private final SwerveDrivePoseEstimator poseEstimator;\n\n    public SwerveDriveSubsystem() {\n        gyro = new Pigeon2(1);\n        modules = new SwerveModule[4]; // Initialize your modules\n\n        // Create kinematics\n        kinematics = new SwerveDriveKinematics(\n            new Translation2d(0.3, 0.3),   // Front left\n            new Translation2d(0.3, -0.3),  // Front right  \n            new Translation2d(-0.3, 0.3),  // Back left\n            new Translation2d(-0.3, -0.3)  // Back right\n        );\n\n        // Create pose estimator\n        poseEstimator = new SwerveDrivePoseEstimator(\n            kinematics,\n            gyro.getRotation2d(),\n            getModulePositions(),\n            new Pose2d(),\n\n            // State standard deviations [x, y, rotation]\n            VecBuilder.fill(0.1, 0.1, Math.toRadians(5)),  // Trust odometry\n\n            // Vision standard deviations [x, y, rotation]  \n            VecBuilder.fill(0.5, 0.5, Math.toRadians(30))  // Less trust in vision\n        );\n    }\n\n    @Override\n    public void periodic() {\n        // Update with odometry every loop (fast, reliable)\n        poseEstimator.update(\n            gyro.getRotation2d(),\n            getModulePositions()\n        );\n\n        // Add vision measurements when available (slower, accurate)\n        Optional&lt;Pose2d&gt; visionPose = getVisionPose();\n        if (visionPose.isPresent()) {\n            poseEstimator.addVisionMeasurement(\n                visionPose.get(),\n                Timer.getFPGATimestamp() - 0.1  // Account for camera latency\n            );\n        }\n\n        // Send to dashboard\n        Pose2d currentPose = poseEstimator.getEstimatedPosition();\n        SmartDashboard.putNumber(\"Robot X\", currentPose.getX());\n        SmartDashboard.putNumber(\"Robot Y\", currentPose.getY());\n        SmartDashboard.putNumber(\"Robot Rotation\", currentPose.getRotation().getDegrees());\n    }\n\n    public Pose2d getPose() {\n        return poseEstimator.getEstimatedPosition();\n    }\n\n    public void resetPose(Pose2d pose) {\n        poseEstimator.resetPosition(\n            gyro.getRotation2d(),\n            getModulePositions(),\n            pose\n        );\n    }\n\n    private SwerveModulePosition[] getModulePositions() {\n        SwerveModulePosition[] positions = new SwerveModulePosition[4];\n        for (int i = 0; i &lt; 4; i++) {\n            positions[i] = modules[i].getPosition();\n        }\n        return positions;\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#vision-integration","title":"Vision Integration","text":"<pre><code>public class VisionSubsystem extends SubsystemBase {\n    private final PhotonCamera camera;\n    private final AprilTagFieldLayout fieldLayout;\n\n    public VisionSubsystem() {\n        camera = new PhotonCamera(\"main_camera\");\n\n        // Load field layout\n        try {\n            fieldLayout = AprilTagFieldLayout.loadFromResource(\n                AprilTagFields.k2024Crescendo.m_resourceFile\n            );\n        } catch (IOException e) {\n            throw new RuntimeException(\"Failed to load AprilTag layout\", e);\n        }\n    }\n\n    public Optional&lt;EstimatedRobotPose&gt; getEstimatedPose(Pose2d prevPose) {\n        PhotonPipelineResult result = camera.getLatestResult();\n\n        if (!result.hasTargets()) {\n            return Optional.empty();\n        }\n\n        // Use PhotonPoseEstimator for 3D pose estimation\n        PhotonPoseEstimator poseEstimator = new PhotonPoseEstimator(\n            fieldLayout,\n            PoseStrategy.MULTI_TAG_PNP_ON_COPROCESSOR,\n            camera,\n            robotToCamera // Transform from robot center to camera\n        );\n\n        return poseEstimator.update();\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#5-tuning-your-pose-estimator","title":"5. Tuning Your Pose Estimator","text":""},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#understanding-standard-deviations","title":"Understanding Standard Deviations","text":"<pre><code>// State standard deviations - how much we trust odometry\nVecBuilder.fill(\n    0.1,                     // X uncertainty: \u00b110cm\n    0.1,                     // Y uncertainty: \u00b110cm  \n    Math.toRadians(5)        // Rotation uncertainty: \u00b15\u00b0\n);\n\n// Vision standard deviations - how much we trust vision\nVecBuilder.fill(\n    0.5,                     // X uncertainty: \u00b150cm\n    0.5,                     // Y uncertainty: \u00b150cm\n    Math.toRadians(30)       // Rotation uncertainty: \u00b130\u00b0\n);\n</code></pre>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#tuning-guidelines","title":"Tuning Guidelines","text":"<p>Good Odometry (new wheels, accurate gyro): <pre><code>// Trust odometry more\nVecBuilder.fill(0.05, 0.05, Math.toRadians(2))  // State std devs\nVecBuilder.fill(0.8, 0.8, Math.toRadians(45))   // Vision std devs\n</code></pre></p> <p>Poor Odometry (worn wheels, drifting gyro): <pre><code>// Trust vision more\nVecBuilder.fill(0.2, 0.2, Math.toRadians(10))   // State std devs\nVecBuilder.fill(0.3, 0.3, Math.toRadians(15))   // Vision std devs\n</code></pre></p> <p>Distance-Based Vision Tuning: <pre><code>public void addVisionMeasurement(Pose2d visionPose, double timestamp) {\n    // Calculate distance to nearest AprilTag\n    double distance = calculateDistanceToTag(visionPose);\n\n    // Scale uncertainty by distance (farther = less reliable)\n    double baseStdDev = 0.1;\n    double scaledStdDev = baseStdDev * (1.0 + distance / 5.0);\n\n    poseEstimator.addVisionMeasurement(\n        visionPose,\n        timestamp,\n        VecBuilder.fill(scaledStdDev, scaledStdDev, Math.toRadians(30))\n    );\n}\n</code></pre></p>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#6-real-world-applications","title":"6. Real-World Applications","text":""},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#auto-alignment-with-pose-estimation","title":"Auto-Alignment with Pose Estimation","text":"<pre><code>public class DriveToPositionCommand extends CommandBase {\n    private final SwerveDriveSubsystem drive;\n    private final Pose2d targetPose;\n    private final PIDController xController = new PIDController(2.0, 0.0, 0.1);\n    private final PIDController yController = new PIDController(2.0, 0.0, 0.1);\n    private final PIDController rotController = new PIDController(3.0, 0.0, 0.1);\n\n    public DriveToPositionCommand(SwerveDriveSubsystem drive, Pose2d target) {\n        this.drive = drive;\n        this.targetPose = target;\n        addRequirements(drive);\n\n        rotController.enableContinuousInput(-Math.PI, Math.PI);\n    }\n\n    @Override\n    public void execute() {\n        // Get current pose from estimator (fused odometry + vision)\n        Pose2d currentPose = drive.getPose();\n\n        // Calculate control outputs\n        double xSpeed = xController.calculate(currentPose.getX(), targetPose.getX());\n        double ySpeed = yController.calculate(currentPose.getY(), targetPose.getY());\n        double rotSpeed = rotController.calculate(\n            currentPose.getRotation().getRadians(),\n            targetPose.getRotation().getRadians()\n        );\n\n        // Drive robot\n        ChassisSpeeds speeds = new ChassisSpeeds(xSpeed, ySpeed, rotSpeed);\n        drive.drive(speeds);\n    }\n\n    @Override\n    public boolean isFinished() {\n        return xController.atSetpoint() &amp;&amp; \n               yController.atSetpoint() &amp;&amp; \n               rotController.atSetpoint();\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#field-relative-autonomous","title":"Field-Relative Autonomous","text":"<pre><code>public class FieldRelativeAuto extends SequentialCommandGroup {\n    public FieldRelativeAuto(SwerveDriveSubsystem drive) {\n        addCommands(\n            // Drive to specific field positions using pose estimation\n            new DriveToPositionCommand(drive, new Pose2d(2, 1, Rotation2d.fromDegrees(0))),\n            new DriveToPositionCommand(drive, new Pose2d(4, 3, Rotation2d.fromDegrees(180))),\n            new DriveToPositionCommand(drive, new Pose2d(1, 5, Rotation2d.fromDegrees(90)))\n        );\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#7-debugging-and-monitoring","title":"7. Debugging and Monitoring","text":""},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#essential-telemetry","title":"Essential Telemetry","text":"<pre><code>@Override\npublic void periodic() {\n    // Pose estimator state\n    Pose2d pose = poseEstimator.getEstimatedPosition();\n    SmartDashboard.putNumber(\"Pose X\", pose.getX());\n    SmartDashboard.putNumber(\"Pose Y\", pose.getY());\n    SmartDashboard.putNumber(\"Pose Rotation\", pose.getRotation().getDegrees());\n\n    // Individual sensor readings\n    SmartDashboard.putNumber(\"Gyro Angle\", gyro.getRotation2d().getDegrees());\n    SmartDashboard.putNumber(\"Left Distance\", leftEncoder.getDistance());\n    SmartDashboard.putNumber(\"Right Distance\", rightEncoder.getDistance());\n\n    // Vision data\n    Optional&lt;Pose2d&gt; visionPose = getVisionPose();\n    SmartDashboard.putBoolean(\"Has Vision\", visionPose.isPresent());\n    if (visionPose.isPresent()) {\n        SmartDashboard.putNumber(\"Vision X\", visionPose.get().getX());\n        SmartDashboard.putNumber(\"Vision Y\", visionPose.get().getY());\n    }\n\n    // Uncertainty monitoring (advanced)\n    Matrix&lt;N3, N3&gt; covariance = poseEstimator.getCovariance();\n    SmartDashboard.putNumber(\"X Uncertainty\", Math.sqrt(covariance.get(0, 0)));\n    SmartDashboard.putNumber(\"Y Uncertainty\", Math.sqrt(covariance.get(1, 1)));\n}\n</code></pre>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"<p>Problem: Position jumps when vision appears - Solution: Increase vision standard deviations, check camera calibration</p> <p>Problem: Pose drifts even with vision - Solution: Decrease vision standard deviations, check AprilTag positions</p> <p>Problem: Jerky movement during auto-alignment - Solution: Increase state standard deviations, tune PID controllers</p> <p>Problem: Vision measurements rejected - Solution: Check timestamp calculation, verify field layout loading</p> <p>Problem: Poor performance/lag - Solution: Reduce vision frequency, optimize pose calculation</p>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#quality-metrics","title":"Quality Metrics","text":"<pre><code>public class PoseEstimatorDiagnostics {\n    private final List&lt;Double&gt; visionErrors = new ArrayList&lt;&gt;();\n\n    public void checkVisionQuality(Pose2d odometryPose, Pose2d visionPose) {\n        double error = odometryPose.getTranslation()\n                                  .getDistance(visionPose.getTranslation());\n        visionErrors.add(error);\n\n        // Keep last 10 measurements\n        if (visionErrors.size() &gt; 10) {\n            visionErrors.remove(0);\n        }\n\n        // Calculate average error\n        double avgError = visionErrors.stream().mapToDouble(Double::doubleValue).average().orElse(0);\n        SmartDashboard.putNumber(\"Avg Vision Error\", avgError);\n\n        // Warn if vision quality is poor\n        if (avgError &gt; 0.5) { // 50cm average error\n            DriverStation.reportWarning(\"Poor vision quality detected\", false);\n        }\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#8-practice-project","title":"8. Practice Project","text":"<p>Build this step-by-step:</p> <ol> <li>Basic Kalman Filter - Implement simple flywheel velocity filtering</li> <li>Pose Estimator Setup - Create swerve drive pose estimator</li> <li>Vision Integration - Add AprilTag pose measurements</li> <li>Parameter Tuning - Optimize standard deviations for your robot</li> <li>Auto-Alignment - Use fused pose for precise positioning</li> <li>Competition Integration - Build reliable autonomous routines</li> </ol> <p>Success Criteria: - Smooth position tracking without jumps - Vision corrects odometry drift over time - Accurate positioning within 5cm after vision correction - Reliable autonomous navigation</p>"},{"location":"Learn/Chapter_8/pose_estimators_and_kalman_filters/#where-to-go-next","title":"Where to Go Next","text":"<p>Ready for advanced estimation? Explore these:</p> <p>\ud83c\udfaf Advanced State-Space Control - Linear Quadratic Regulator - Optimal feedback control - Custom State-Space Models - Model complex mechanisms</p> <p>\ud83d\udd27 Sensor Fusion Techniques - Unscented Kalman Filters - Handle nonlinear systems - Multi-Camera Fusion - Combine multiple vision sources</p> <p>\ud83d\udcca Performance Optimization - Extended Kalman Filters - Advanced nonlinear estimation - Particle Filters - Handle non-Gaussian noise</p> <p>\u26a1 Competition Applications - PathPlanner Integration - Pose-corrected path following - Real-Time Trajectory Correction - Adaptive autonomous - Match Replay Analysis - Post-match pose tracking</p> <p>\ud83d\ude80 Ready to master robot localization? Start with basic filtering, understand the fundamentals, then build competition-ready pose estimation systems that rival GPS accuracy!</p>"},{"location":"Learn/Chapter_8/vision_explanation/","title":"Vision Processing: Getting Started","text":""},{"location":"Learn/Chapter_8/vision_explanation/#overview","title":"Overview","text":"<p>Vision processing turns your robot's camera into smart eyes that can recognize targets, track game pieces, and navigate autonomously. Instead of relying solely on sensors, your robot can \"see\" and react to the field like a human driver - but with perfect consistency.</p> <p>What you'll build: A robot that can detect and track targets using computer vision for autonomous aiming and navigation.</p> <p>Next steps: Advanced AprilTag tracking, neural networks, and real-time target recognition using PhotonVision and LimelightLib.</p>"},{"location":"Learn/Chapter_8/vision_explanation/#1-why-vision-processing","title":"1. Why Vision Processing?","text":""},{"location":"Learn/Chapter_8/vision_explanation/#the-power-of-robot-vision","title":"The Power of Robot Vision","text":"<p>Manual Targeting vs Vision: <pre><code>// Manual: Driver aims manually\nshooterSubsystem.setAngle(driverEstimatedAngle);\n\n// Vision: Robot automatically finds and tracks target\ndouble targetAngle = vision.getTargetAngle();\nshooterSubsystem.setAngle(targetAngle);\n</code></pre></p> <p>Real Benefits: - Autonomous scoring - Perfect aim every time - Driver assistance - Help human pilots during teleop - Game piece detection - Find balls, cones, cubes automatically - Navigation aids - Use field landmarks for positioning - Consistent performance - No human error in targeting</p> <p>FRC Applications: - Auto-aiming for shooters - Autonomous ball collection - Precise docking/charging - Field-relative navigation - Driver camera feeds</p>"},{"location":"Learn/Chapter_8/vision_explanation/#2-vision-processing-pipeline","title":"2. Vision Processing Pipeline","text":""},{"location":"Learn/Chapter_8/vision_explanation/#the-step-by-step-process","title":"The Step-by-Step Process","text":"<p>1. Image Capture <pre><code>Camera \u2192 Raw Image Frame (640x480 pixels, 30 FPS)\n</code></pre></p> <p>2. Preprocessing <pre><code>Raw Image \u2192 Color Space Conversion \u2192 Filtered Image\n</code></pre></p> <p>3. Target Detection <pre><code>Filtered Image \u2192 Contour Detection \u2192 Target Candidates\n</code></pre></p> <p>4. Target Analysis <pre><code>Candidates \u2192 Filtering \u2192 Distance/Angle Calculations\n</code></pre></p> <p>5. Robot Action <pre><code>Target Data \u2192 Robot Code \u2192 Motor Commands\n</code></pre></p>"},{"location":"Learn/Chapter_8/vision_explanation/#basic-vision-workflow","title":"Basic Vision Workflow","text":"<pre><code>// Simplified pipeline\nwhile (robot.isEnabled()) {\n    // 1. Capture frame\n    Mat frame = camera.getFrame();\n\n    // 2. Filter for target color (e.g., green retroreflective tape)\n    Mat filtered = filterForGreen(frame);\n\n    // 3. Find contours (shapes)\n    List&lt;Contour&gt; contours = findContours(filtered);\n\n    // 4. Filter for target-shaped contours\n    List&lt;Target&gt; targets = filterTargets(contours);\n\n    // 5. Use best target for robot control\n    if (!targets.isEmpty()) {\n        Target best = getBestTarget(targets);\n        double angle = calculateAngle(best);\n        robotCode.setTargetAngle(angle);\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#3-vision-architecture-options","title":"3. Vision Architecture Options","text":""},{"location":"Learn/Chapter_8/vision_explanation/#option-1-roborio-vision-simplest","title":"Option 1: RoboRIO Vision (Simplest)","text":"<p>Setup: Camera \u2192 RoboRIO \u2192 Robot Code</p> <pre><code>public class VisionSubsystem extends SubsystemBase {\n    private final UsbCamera camera;\n    private final CvSink cvSink;\n    private final Mat frame = new Mat();\n\n    public VisionSubsystem() {\n        // Camera setup on roboRIO\n        camera = CameraServer.startAutomaticCapture();\n        camera.setResolution(320, 240);  // Lower resolution for performance\n        camera.setFPS(15);               // Lower FPS for roboRIO\n\n        cvSink = CameraServer.getVideo();\n    }\n\n    @Override\n    public void periodic() {\n        // Grab frame (non-blocking)\n        if (cvSink.grabFrame(frame) == 0) {\n            return; // No new frame\n        }\n\n        // Process frame\n        processFrame(frame);\n    }\n\n    private void processFrame(Mat frame) {\n        // Your vision pipeline here\n        List&lt;Target&gt; targets = visionPipeline.process(frame);\n\n        // Send results to robot\n        if (!targets.isEmpty()) {\n            SmartDashboard.putNumber(\"Target Angle\", targets.get(0).angle);\n        }\n    }\n}\n</code></pre> <p>Pros: Simple, everything in one place Cons: Limited processing power, can slow robot code</p>"},{"location":"Learn/Chapter_8/vision_explanation/#option-2-coprocessor-vision-recommended","title":"Option 2: Coprocessor Vision (Recommended)","text":"<p>Setup: Camera \u2192 Raspberry Pi/Jetson \u2192 NetworkTables \u2192 RoboRIO</p> <pre><code># Python vision code on Raspberry Pi\nimport cv2\nimport numpy as np\nfrom networktables import NetworkTables\n\n# Initialize NetworkTables\nNetworkTables.initialize(server='10.TE.AM.2')  # Your robot IP\nvision_table = NetworkTables.getTable('vision')\n\n# Camera setup\ncamera = cv2.VideoCapture(0)\ncamera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\ncamera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n\nwhile True:\n    ret, frame = camera.read()\n    if not ret:\n        continue\n\n    # Vision pipeline\n    targets = process_frame(frame)\n\n    # Send to robot\n    if targets:\n        best_target = targets[0]\n        vision_table.putNumber('target_angle', best_target.angle)\n        vision_table.putNumber('target_distance', best_target.distance)\n        vision_table.putBoolean('target_found', True)\n    else:\n        vision_table.putBoolean('target_found', False)\n</code></pre> <pre><code>// Robot code receives vision data\npublic class ShooterSubsystem extends SubsystemBase {\n    private final NetworkTableEntry targetAngle;\n    private final NetworkTableEntry targetFound;\n\n    public ShooterSubsystem() {\n        NetworkTable visionTable = NetworkTableInstance.getDefault().getTable(\"vision\");\n        targetAngle = visionTable.getEntry(\"target_angle\");\n        targetFound = visionTable.getEntry(\"target_found\");\n    }\n\n    public boolean hasTarget() {\n        return targetFound.getBoolean(false);\n    }\n\n    public double getTargetAngle() {\n        return targetAngle.getDouble(0.0);\n    }\n}\n</code></pre> <p>Pros: Fast processing, doesn't slow robot code Cons: More complex setup, additional hardware</p>"},{"location":"Learn/Chapter_8/vision_explanation/#option-3-driver-station-vision","title":"Option 3: Driver Station Vision","text":"<p>Setup: Camera \u2192 RoboRIO \u2192 Driver Station \u2192 NetworkTables \u2192 Robot</p> <p>Pros: Powerful laptop processing Cons: High latency, bandwidth limitations</p>"},{"location":"Learn/Chapter_8/vision_explanation/#4-basic-vision-pipeline-implementation","title":"4. Basic Vision Pipeline Implementation","text":""},{"location":"Learn/Chapter_8/vision_explanation/#step-1-color-filtering","title":"Step 1: Color Filtering","text":"<pre><code>public class VisionPipeline {\n    // HSV ranges for green retroreflective tape\n    private final Scalar lowerGreen = new Scalar(40, 50, 50);\n    private final Scalar upperGreen = new Scalar(80, 255, 255);\n\n    public Mat filterForTarget(Mat input) {\n        Mat hsv = new Mat();\n        Mat mask = new Mat();\n\n        // Convert to HSV color space\n        Imgproc.cvtColor(input, hsv, Imgproc.COLOR_BGR2HSV);\n\n        // Filter for green color\n        Core.inRange(hsv, lowerGreen, upperGreen, mask);\n\n        // Clean up noise\n        Mat kernel = Imgproc.getStructuringElement(Imgproc.MORPH_RECT, new Size(3, 3));\n        Imgproc.morphologyEx(mask, mask, Imgproc.MORPH_OPEN, kernel);\n        Imgproc.morphologyEx(mask, mask, Imgproc.MORPH_CLOSE, kernel);\n\n        return mask;\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#step-2-contour-detection","title":"Step 2: Contour Detection","text":"<pre><code>public List&lt;MatOfPoint&gt; findContours(Mat mask) {\n    List&lt;MatOfPoint&gt; contours = new ArrayList&lt;&gt;();\n    Mat hierarchy = new Mat();\n\n    // Find contours\n    Imgproc.findContours(mask, contours, hierarchy, \n                        Imgproc.RETR_EXTERNAL, Imgproc.CHAIN_APPROX_SIMPLE);\n\n    return contours;\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#step-3-target-filtering","title":"Step 3: Target Filtering","text":"<pre><code>public class Target {\n    public double centerX;\n    public double centerY;\n    public double area;\n    public double angle;\n    public double distance;\n}\n\npublic List&lt;Target&gt; filterTargets(List&lt;MatOfPoint&gt; contours) {\n    List&lt;Target&gt; targets = new ArrayList&lt;&gt;();\n\n    for (MatOfPoint contour : contours) {\n        double area = Imgproc.contourArea(contour);\n\n        // Filter by size\n        if (area &lt; 100 || area &gt; 10000) {\n            continue;\n        }\n\n        // Get bounding rectangle\n        Rect boundingRect = Imgproc.boundingRect(contour);\n        double aspectRatio = (double) boundingRect.width / boundingRect.height;\n\n        // Filter by aspect ratio (targets are typically wider than tall)\n        if (aspectRatio &lt; 1.2 || aspectRatio &gt; 3.0) {\n            continue;\n        }\n\n        // Create target object\n        Target target = new Target();\n        target.centerX = boundingRect.x + boundingRect.width / 2.0;\n        target.centerY = boundingRect.y + boundingRect.height / 2.0;\n        target.area = area;\n\n        targets.add(target);\n    }\n\n    // Sort by area (biggest first)\n    targets.sort((a, b) -&gt; Double.compare(b.area, a.area));\n\n    return targets;\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#step-4-angle-and-distance-calculation","title":"Step 4: Angle and Distance Calculation","text":"<pre><code>public void calculateTargetInfo(Target target, int imageWidth, int imageHeight) {\n    // Camera parameters (calibrate these for your camera)\n    double focalLength = 300;  // pixels\n    double targetWidth = 14.5; // inches (actual target width)\n\n    // Calculate horizontal angle\n    double centerX = imageWidth / 2.0;\n    double pixelError = target.centerX - centerX;\n    target.angle = Math.atan(pixelError / focalLength);\n\n    // Estimate distance (rough approximation)\n    double targetPixelWidth = Math.sqrt(target.area * 2); // Approximate width\n    target.distance = (targetWidth * focalLength) / targetPixelWidth;\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#5-integration-with-robot-code","title":"5. Integration with Robot Code","text":""},{"location":"Learn/Chapter_8/vision_explanation/#auto-aiming-shooter","title":"Auto-Aiming Shooter","text":"<pre><code>public class AutoAimCommand extends CommandBase {\n    private final ShooterSubsystem shooter;\n    private final VisionSubsystem vision;\n    private final PIDController turnController;\n\n    public AutoAimCommand(ShooterSubsystem shooter, VisionSubsystem vision) {\n        this.shooter = shooter;\n        this.vision = vision;\n        this.turnController = new PIDController(0.1, 0.0, 0.01);\n\n        addRequirements(shooter);\n    }\n\n    @Override\n    public void execute() {\n        if (vision.hasTarget()) {\n            double targetAngle = vision.getTargetAngle();\n            double output = turnController.calculate(0, targetAngle); // Want angle error to be 0\n            shooter.setTurretSpeed(output);\n        } else {\n            shooter.setTurretSpeed(0);\n        }\n    }\n\n    @Override\n    public boolean isFinished() {\n        return vision.hasTarget() &amp;&amp; Math.abs(vision.getTargetAngle()) &lt; 0.05; // Within tolerance\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#vision-assisted-driving","title":"Vision-Assisted Driving","text":"<pre><code>public class DriveWithVisionAssist extends CommandBase {\n    private final SwerveDrive drive;\n    private final VisionSubsystem vision;\n    private final XboxController controller;\n\n    @Override\n    public void execute() {\n        // Get driver input\n        double xSpeed = -controller.getLeftY();\n        double ySpeed = -controller.getLeftX();\n        double rotation = -controller.getRightX();\n\n        // Add vision assist for rotation\n        if (controller.getAButton() &amp;&amp; vision.hasTarget()) {\n            double visionCorrection = vision.getTargetAngle() * 0.1; // Gentle assist\n            rotation += visionCorrection;\n        }\n\n        ChassisSpeeds speeds = new ChassisSpeeds(xSpeed, ySpeed, rotation);\n        drive.drive(speeds);\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#6-real-world-examples","title":"6. Real-World Examples","text":""},{"location":"Learn/Chapter_8/vision_explanation/#apriltag-detection-simple","title":"AprilTag Detection (Simple)","text":"<pre><code>// Using WPILib's AprilTag detector\nAprilTagDetector detector = new AprilTagDetector();\ndetector.addFamily(AprilTagFamily.TAG_16h5);\n\npublic void processAprilTags(Mat frame) {\n    AprilTagDetection[] detections = detector.detect(frame);\n\n    for (AprilTagDetection detection : detections) {\n        int tagID = detection.getId();\n        Pose3d tagPose = detection.getPose();\n\n        SmartDashboard.putNumber(\"Tag \" + tagID + \" X\", tagPose.getX());\n        SmartDashboard.putNumber(\"Tag \" + tagID + \" Y\", tagPose.getY());\n    }\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#game-piece-detection","title":"Game Piece Detection","text":"<pre><code># Detect orange balls/cones\ndef detect_game_pieces(frame):\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n    # Orange color range\n    lower_orange = np.array([10, 100, 100])\n    upper_orange = np.array([20, 255, 255])\n\n    mask = cv2.inRange(hsv, lower_orange, upper_orange)\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    game_pieces = []\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area &gt; 500:  # Filter small detections\n            x, y, w, h = cv2.boundingRect(contour)\n            center_x = x + w // 2\n            center_y = y + h // 2\n            game_pieces.append({'x': center_x, 'y': center_y, 'area': area})\n\n    return game_pieces\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#7-performance-optimization","title":"7. Performance Optimization","text":""},{"location":"Learn/Chapter_8/vision_explanation/#camera-settings","title":"Camera Settings","text":"<pre><code>// Optimize for performance\ncamera.setResolution(320, 240);  // Lower resolution = faster processing\ncamera.setFPS(15);               // 15 FPS often sufficient\ncamera.setExposure(0);           // Auto exposure off for consistent lighting\ncamera.setBrightness(30);        // Adjust for your field conditions\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#processing-optimizations","title":"Processing Optimizations","text":"<pre><code>// Process every other frame for performance\nprivate int frameCount = 0;\n\n@Override\npublic void periodic() {\n    frameCount++;\n    if (frameCount % 2 == 0) {  // Process every other frame\n        processFrame();\n    }\n}\n\n// Region of Interest (ROI) processing\npublic Mat getROI(Mat frame) {\n    // Only process center portion where targets likely appear\n    int x = frame.width() / 4;\n    int y = frame.height() / 4;\n    int width = frame.width() / 2;\n    int height = frame.height() / 2;\n\n    Rect roi = new Rect(x, y, width, height);\n    return new Mat(frame, roi);\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#8-debugging-and-tuning","title":"8. Debugging and Tuning","text":""},{"location":"Learn/Chapter_8/vision_explanation/#essential-telemetry","title":"Essential Telemetry","text":"<pre><code>@Override\npublic void periodic() {\n    // Camera info\n    SmartDashboard.putNumber(\"Camera FPS\", camera.getActualFPS());\n    SmartDashboard.putBoolean(\"Camera Connected\", camera.isConnected());\n\n    // Vision results\n    SmartDashboard.putBoolean(\"Target Found\", hasTarget());\n    SmartDashboard.putNumber(\"Target Count\", targets.size());\n\n    if (hasTarget()) {\n        Target best = getBestTarget();\n        SmartDashboard.putNumber(\"Target Angle\", best.angle);\n        SmartDashboard.putNumber(\"Target Distance\", best.distance);\n        SmartDashboard.putNumber(\"Target Area\", best.area);\n    }\n\n    // Performance monitoring\n    SmartDashboard.putNumber(\"Processing Time\", processingTimeMs);\n}\n</code></pre>"},{"location":"Learn/Chapter_8/vision_explanation/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"<p>Problem: No targets detected - Solution: Check color ranges, lighting conditions, camera exposure</p> <p>Problem: False positive detections - Solution: Tighten filtering criteria (area, aspect ratio, etc.)</p> <p>Problem: Intermittent target loss - Solution: Add target tracking/prediction, improve filtering</p> <p>Problem: Poor performance/lag - Solution: Lower resolution/FPS, optimize pipeline, use coprocessor</p> <p>Problem: Inaccurate distance measurements - Solution: Calibrate camera, use stereo vision, add known target size</p>"},{"location":"Learn/Chapter_8/vision_explanation/#9-practice-project","title":"9. Practice Project","text":"<p>Build this step-by-step:</p> <ol> <li>Camera Setup - Get camera streaming to dashboard</li> <li>Basic Detection - Detect simple colored objects</li> <li>Target Filtering - Filter by size and shape</li> <li>Angle Calculation - Calculate target angle from center</li> <li>Robot Integration - Use vision data to control robot</li> <li>Auto-Aiming - Create automatic targeting system</li> </ol> <p>Success Criteria: - Consistent target detection under field lighting - Accurate angle calculations within 2-3 degrees - Smooth auto-aiming without oscillation - Good performance (&gt;10 FPS processing)</p>"},{"location":"Learn/Chapter_8/vision_explanation/#where-to-go-next","title":"Where to Go Next","text":"<p>Ready for professional vision? Explore these:</p> <p>\ud83c\udfaf Pre-Built Solutions - PhotonVision - Complete vision processing suite - LimelightLib - Plug-and-play vision computer - Chameleon Vision - Open-source vision processing</p> <p>\ud83d\udd27 Advanced Techniques - Camera Calibration - Accurate 3D positioning - Pose Estimation - 6DOF target tracking - Machine Learning - Neural network object detection</p> <p>\ud83d\udcca Multi-Camera Systems - Stereo Vision - Depth perception - 360\u00b0 Coverage - Multiple camera coordination</p> <p>\u26a1 Real-Time Performance - GPU Acceleration - CUDA-accelerated processing - FPGA Vision - Ultra-low latency processing</p> <p>\ud83d\ude80 Ready to give your robot eyes? Start with basic color detection, master the fundamentals, then explore professional vision solutions for competition-winning autonomous performance!</p>"}]}